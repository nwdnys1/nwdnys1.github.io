---
title: 在SJTU-REINS的打工日记
date: 2024-10-05 21:19:00
categories: REINS
tags:
  - 实习
  - 开发
  - 架构
  - 运维
  - 测试
index_img:
banner_img:
excerpt: "大三顺利进入REINS打工 记录一下项目经历和经验"
---

# PHAROS

灯塔项目是一个类似于海上管理船只位置的云系统的项目 只不过空间由海域变为上海市区 对象由船只变为无人机等移动物联网设备

简单来说 灯塔系统会收集处于三维空间内各个个体的数据 并存储在一个云边融合系统（CEDS）内 之后将数据整理封装为态势数据 将态势数据分发回给每个个体 让个体通过实时的态势数据进行任务的规划和调度 态势数据会分为两部分 一部分是以个体为中心 在单位时间内可以安全独占的空间范围 另一部分是以个体为中心 依据任务需要而划分的一块空间范围内所有其它个体的数据

灯塔的核心思想是流控分离 即中心会分发数据流给个体设备 但是不会干涉个体设备的任务规划和调度 只是给出了安全空间的限制 个体设备会根据态势数据自行调整任务的路径 和现有的大部分统筹任务路线规划是不同的

## CEDS

CEDS 即黄子昂学长的硕士毕业论文中提出的云边融合存储系统 利用了边缘节点本身少量的数据存储与计算能力以及其分布于移动物联网设备附近的特点 实现了基于数据的就近存储、查询任务的划分下推以及负载感知的数据迁移功能的存储系统 做到了传输带宽的减少以及查询的加速 具体实现可以见其论文 大概包含以下几个部分：

### 边缘数据就近存储机制

大部分云边融合系统会将数据存储于资源富足可扩展的云端数据库中 但这会带来很多不必要的传输开销 利用边缘节点本身的存储能力 可以将数据就近存储 大大减少传输延迟

然而数据的就近存储会带来其他问题 比如原本全都位于云端中心的数据 由于就近存储就会分布在不同的边缘节点上 针对此问题 CEDS 提出一种基于 Rosetta 过滤器的全局索引机制 简单来讲 这个过滤器能够实现一个数据表内任一字段的范围过滤 即给定一个字段的某一范围 在常数时间内返回这个表是否存在这个范围内的数据 基于此种过滤器 加上基于时间划分的数据分表 可以做到在很短的时间内定位到某一个时间范围内的数据分表 并快速判断哪些分表内存在本次范围查询相关的数据 这就是 CEDS 的全局索引机制 基于此种索引 CEDS 在云端仅需存储数据表的元信息摘要（包括过滤器的 bitarray） 同时这也将为后文的查询下推提供便利

### 查询任务拆分下推

大部分云边存储系统基于边缘节点的存储数据 每次查询会将相关的数据表全部聚合到云端后进行筛选 产生了很多不必要的传输开销 CEDS 基于任务划分 将查询和聚合任务下推给各个相关的边缘子节点 减少带宽开销

具体而言 一次查询会先向云端中心索要本次查询可能涉及到的所有数据分表 以及这些分表存储在哪些边缘节点上 其方法可以简单的由之前提到的过滤器实现

获取到节点列表后 只需要把子查询任务分发给这些节点即可 每个子节点会进行相关的范围查询（节点内存在常规的索引来加快查询）并把查询结果返回 最终在查询节点进行数据聚合任务 把结果返回给用户

### 负载感知数据迁移

数据就近存储定会存在不均衡现象 由于查询任务划分给子节点并发执行 查询的延迟显然由查询时间最大的子查询决定 如果某些节点上的数据查询次数特别多 导致超出边缘节点的承受范围 导致子查询阻塞 就会导致所有涉及的查询延迟显著变高 为了防止此种情况出现 CEDS 会监测并记录每段时间内边缘节点的查询资源开销 比如记录前 3 分钟内某一节点的内存占用 一旦超过 1G 就判定存在热点数据 并将热点数据迁移到云端来减缓此节点的压力

此种策略是折中的策略 理论上可以达到最均衡的方案肯定是为每一个边缘节点根据查询开销动态分配性能资源 但这在现实中肯定不可行 因此使用较为折中的策略来解决数据不均衡分布的问题

## 技术栈

### Redisearch

- 项目使用了 redisearch 建立设备数据的地理索引 实现实时数据查询的加速 即把经纬度作为 GEO 类型的元素编写索引
- 具体实现
  - 文档参考：https://redis.io/docs/latest/develop/interact/search-and-query/indexing/（redis不同类型的索引如何使用）https://redis.readthedocs.io/en/stable/examples/search_json_examples.html#Projecting-using-JSON-Path-expressions（将 JSON 数据添加到索引的示例代码）
  - redis 实例需要使用 redis-stack 项目中使用 python 因此安装 pip 库的 redis 即可
  - 具体代码见项目中的`realtime_map.py`文件 只需要先创建 schema 描述索引的字段类型 然后添加索引即可

### Apache Avro

- 项目使用了 apache avro 进行数据的压缩 包括：
  - 设备传输数据给 realtime_map 与 data_receiver
  - 节点计算态势数据时向其他节点发送`/query`请求查询 reatime_map
  - 节点通过消息队列传输态势数据给中心
- 具体实现
  - 文档参考：https://avro.apache.org/docs/
  - python 需要安装 avro 库 java 则通过 maven 导入 avro 依赖
  - 需要预先编写好数据的 schema 语法见文档 编写好后即可使用 avro 的序列化与反序列化方法进行数据的压缩与解压
  - python 中数据类型比较自由 使用 dict 类型即可操作 但是 java 的数据类型比较严格 avro 本身提供一个 record 类封装数据 但是我使用了 fastjson 库的 json 类型来操作数据 这就导致必须要编写一个转换函数进行转换 并且这个转换需要递归分类进行 一旦类型对应不上 序列化时就会报错

### GeoMesa

- 我的理解 GM 本身是一个类似于索引引擎的工具 首先需要选取合适的存储系统 要么 Redis 要么 HBase 等数据库 或者也可以直接通过 FS 存储 但是性能较差
- 题外话 现有的 redisearch 只支持 2d 数据的复合索引 要引入高度的话 可能只能遍历筛选 不知道这部分的性能有没有要求
- 假如我们使用 GM 的目的是为了加速大量历史数据的查询 进行对历史数据的分析 而对于数据的写入与实时性并无要求 那么有两种方案
  - 一种是保留现有的 mongodb 数据库 也就是近期数据仍向 mongodb 写入 然后定期（比如 1 天）把数据迁移到适合存储大量时空数据的数据库中（比如 HBase） 并且这个迁移过程通过 GM 实现（比如把 mongo 数据批量读到内存里 在通过 GM 写入 HBase）也就自动创建了时空索引 后续查询也就提高了性能 （代码量会小一点）
  - 另一种则是抛弃 mongodb 直接通过 GM 持续写入数据 大致流程就是通过 mqtt 接受消息 把消息里的数据解析为 GM 的 simplefeature 类 然后通过 writer 写入 HBase 里 自动建立索引 省去了 mongodb 的部分 但是代码量会大一点
- 方便切换其他数据库是指？拿第一种方案举例 是指切换 mongodb 还是切换 GM 的存储系统？
- GM 现有的 API 都是大型分布式数据库 我没啥使用经验 如果单机部署不知道合不合适 要考虑分布部署到不同节点上吗？
- GM 支持空间索引和时空索引 其中时空索引支持 2d point+时间 或是 2d 非 point（polygon line 等）+时间 不直接支持 3d+时间索引 
- 原理部分
  - GM 如何写入数据？（以 HBase 为例）
    - 参考：https://zhuanlan.zhihu.com/p/164645879
    - 插入一个 feature 时 进行以下几步操作
      1. 预处理 假如数据没有 id 生成 uuid 然后为数据添加合适的属性 方便插入 HBase 表 为了防止数据倾斜 最终对于 id 进行哈希 放入对应分区
      2. 计算索引值 GM 会找到数据的 geo 属性和 date 属性 提取出数据 计算索引值 其中时间数据为了适应 GeoHash 机制 采用了 BinnedTime 机制 即相对于 1970 年 1 月 1 号形成有限的 chunk 接下来开始计算索引 geo 数据为 double 类型 dtg 则为 long 类型 先进行标准化等处理后 最终通过编码得到索引值
      3. 索引值会被编码为 byte 数组 写入 HBase 表中
      4. 最后数据被序列化 写入 HBase 表中 默认使用 kryo 序列化 也可以使用 avro
  - GM 如何进行数据的序列化？
    - 参考：https://zhuanlan.zhihu.com/p/164647326
    - 分两步 分别是序列化 feature 和 type 为什么需要序列化 type？因为插入数据时需要判断是否共用连接 GM 会把 type 序列化后存放在缓存里 方便后续反序列化出来进行判断 相当于一个全局的 schema
    - ![](https://image.blog.nwdnysl.site/20250224205422-99fe17067b124be578cba66601944766.png)
    - 具体序列化过程略 可以视为和 avro 一样根据 schema 压缩为二进制数据
  - GM 如何进行数据的索引？
    - 参考：https://zhuanlan.zhihu.com/p/164748160
    - 简单来讲就是把三维数据（经纬度+时间）通过 Z 曲线映射到一维整数上 作为索引的 key
    - Z 曲线索引：以经纬度数据为例 通过如图的红色曲线决定了数据的唯一顺序 也就映射到了一维整数上 从而支持 1d 的键值索引 当需要进行 2d 的范围查询时 虽然 1d 的索引无法完全映射回 2d 但是其上下限范围内的数据由于 Z 曲线的空间局部性（空间相近的空间在转换后的整数上也尽可能地相近）基本上都互相邻接 因此筛选出来的数据不会多出很多 进而通过回溯或遍历的方式删去这些数据 实现了高性能的 2d 范围查询
    - 在索引之前需要对数据进行预处理 首先需要标准化数据 包括数据区间和单位统一 GM 中使用的是极差标准化
    - GM 索引的前身是 GeoHash 类似于八叉树划分空间的编码 将经纬度区域映射为一个字符串 字符串的长度越大 区域越小 精度越高 编码过程就是在不断二分区域并填充 0 或 1 然后经纬度交替填充 最后 base32 编码得到字符串
  - GM 的 Z3 索引
    - 参考：https://zhuanlan.zhihu.com/p/164751055
    - Z3 索引采用类似于 GeoHash 的方法 把时空交替组合 形成具有局部性的索引
    - 时间数据本身是没法二分的 因此采用了 EpochWeek 机制 把时间数据距离 1970 年 1 月 1 号过去了几周提取出来 剩下的数据就是一个最大为一周的数据 可以进行二分 这个机制的影响是 查询不同周的数据会比查询同一周内的数据慢一些 前者往往会涉及到 HBase 的多个 shard
    - 此时经纬度和时间数据都可以二分 采用八叉树空间即可编码为 bytearray

## 工作内容

### 组会汇报

#### 02.19

### TODO

#### 优先

- geomesa 实现 3d+时间的索引和查询 把接口写通用一点 方便切换其他数据库 找一找 3d 带时间的数据集 辅助 zhd 把 CEDS 迁移到 GeoMesa3d 上
- 这两天需要试一下 3d+时间的数据可不可以查询
- 另外 时延比较差 可能需要优化

#### 一些可以 fix 的小细节

- realtime_map 中接收消息时的 time 和消息中的 time 有延迟 差不多是 2s 不知道为什么
- 节点刚好处于区域边界时可能会有点问题 暂时没做测试
- 断开连接时如何告知节点 目前是心跳计时实现的 考虑是否要更加实时
- 如果只是发给每个单独的设备 似乎没有必要汇聚到中心？以后如果 push 给应用 一个应用可能需要多个设备的数据 比如一个应用商只负责 taxi 的数据
- 目前数据缺少方向 因此先以设备为中心划分矩形范围 后续加入方向后再进行修改
- jedis 中取出的 Document 类中的 properties 不知道为什么是$={}的格式 导致必须要解析 string
- B 与 A 的通信需要发送独占空间 数据比较大 除了让 zhd 进一步压缩以外也可以使用 avro 进行压缩
- java 部署太慢了 看一下如何改仓库源
- 合并大圈小圈数据很 sb 看看能不能把 B 的小圈数据格式改一下
- java 这边的 avro 由于我使用了 fastjson 导致需要先解析为 jsonobject 再进行使用 需要手动分类序列化 未来可以考虑直接使用 avro 的 record 类
- 当数据较多时 会超时（正常） 当数据很多时 space-manage 会挂掉 报错如下：![alt text](image.png) 看起来是 mqtt 的问题 但是 mqtt 没报错
- avro 的压缩 如何自动生成 schema 文件？如何定义统一的 realtime_map 的 schema？（环境变量）

#### 性能需求

- 现在是中心主动 push 瓶颈可能在两个地方 一个是每 1s 计算态势数据来不来得及（主要是节点的强化学习） 另一个是中心需要并发发送态势数据给各个设备
- 一种是加快计算频率 看看到多少会出现问题
- 另一种是增加订阅设备的数量 看看到多少会出现问题

#### 代码整合

- 三个部分的代码 CEDS 八叉树空间 小圈计算 称为 A B C
- 节点收到请求开始计算：
  - 大圈
    1. 在 A 的 situ_cal 中 取出 redis 所有的设备
    2. 对于每个设备 使用 redisearch 的范围查询得到范围内的设备 即大圈数据
  - 小圈
    1. 在 A 的 situ_cal 中 直接给 B 通过 mqtt 发送异步请求 topic 为 `situation/send/{timestamp}` 传入时间戳作为唯一标识
    2. B 从 redis 里取出所有的实时数据 遍历设备 将设备以及需要的数据（目前是最多 7 个设备与 6 个人）发送给 C
    3. B 得到 C 返回的小圈后 将小圈编码为八叉树格式 然后根据存储的每个设备的特性（优先级等）以及形状进行冲突的解决 最后返回以八叉树格式表示的小圈数据给 A topic 为 situation/receive/{timestamp}
- 只需要改为使用 redis 然后使用 mqtt 即可 计算独占空间的小圈只需要每个设备的位置数据 然后使用元数据计算 独占空间的大圈则需要发给 C 需要找到最近的 7 个设备和 6 个人的数据
- 目前 B C 之间的通信还没有做

### 项目架构

#### 并发处理

- 节点计算态势数据用了线程池进行并发
- 向邻居节点发送请求也可以并发处理 不过由于 http 有长连接机制 请求应该不会重新建立连接 且一个设备的计算最多也就发送 3 个请求（大多数情况是 1 个） 所以并发处理的意义不大
- 中心对于消息的处理使用线程池并发
  - 本来的思路是使用 TPE（ThreadPoolExecutor）类初始化线程池 然后用一个 ConcurrentHashMap 来存储每个请求的线程对象 请求的第一个消息会被线程池随机分配一个线程对象 后续的消息取出 map 里的线程对象即可保证同一个请求的消息被同一个线程处理 但是我不知道如何使用线程池指定某一个线程处理...
  - 现在的实现比较简单 就是一个 List<ExecutorService> 里面存了 n 个单线程池 每次请求根据时间戳取模分配一个线程池处理 不知道可不可以保证均匀
  - 中心并发的意义在于高并发请求 对于串行请求无提升
  - **改为 push 后 这部分就没什么意义了**
- **中心发送态势数据给各个设备**：现在是 for 循环+publish 不知道 mqtt 的 publish 是不是异步的 可能需要多线程或者至少保证异步发送

#### 总体架构

##### 物联网设备

- sender.py: 模拟设备发送数据 通过 mqtt 向边缘节点实时发送数据

##### 边缘节点

- realtime_map.py: 在内存中维护两个结构 `realtime_map`（设备的实时数据）和 `online_clients`（在线设备列表）其中 realtime_map 存储在 redis 中 方便使用 `redisearch` 进行地理空间查询
  - 节点收到设备的数据后更新 `realtime_map` 与 `online_clients` 同时上传 realtime_index 给中心
  - 另起一个子线程判断设备是否离线 标准是每隔 2s 进行检查 如果上一次收到数据的时间距离现在超过 2s 就认为设备离线
  - 使用 flask 提供接口`/query` 用于其他边缘节点进行态势数据计算时 来查询范围内的设备数据
- situation_calculation.py: 计算态势数据的模块 连接到中心节点的 mqtt 并订阅`/situation/{node_id}`的 topic 接受到中心发来的消息（payload 为 timestamp）后开始计算态势数据 步骤为
  1. 从本节点的 redis 和邻近节点的 `/query` 接口中获取需要的实时数据 其中使用了 `shapely` 库来判断大圈与邻居是否相交
  2. 节点把每个设备 realtimeData 中的时间戳作为态势数据的时间戳放入 用于中心节点的合并去重
  3. 用这些数据计算每个设备的态势数据 使用线程池进行多线程并发提高性能
  4. 封装好最终的态势数据 放入中心消息队列的`/situation/center/{timestamp}`的 topic 中 消息格式如下：
     ```
     {
         "nodeId": 1,
         "taxi/0001": {
             "TIMESTAMP": 1372662000, # 用于合并去重
             "BIGCIRCLE": {...},      # 其它设备的态势数据
             "SMALLCIRCLE": {         # 独占空间
                  "smallArea": ... ,
                  "largeArea": ... ,
              },
          },
         ...
     }
     ```
     ```
      {
          "taxi/0001": {
              "TIMESTAMP": 1372662000, # 用于合并去重
              "BIGCIRCLE": {...},      # 其它设备的态势数据
          },
          ...
      }
     ```
- process_manager.py：进程管理器 负责管理数据采集进程的生命周期
- data_handler.py: 数据采集进程 定期存储数据到 mongodb

##### 中心

- 中心会在内存中维护一个名为 `situationData` 的结构 其包含两部分数据 一个是 map 存储态势数据 其结构类似于：
  ```
  {
      "timestamp": {                            # 发起这次态势数据计算时的时间戳
          "nodes": {                            # 所有节点是否已经收到消息
              1: False,
              2: False,
              ...
              12: False,
          },
          "data": {                             # 节点计算的态势数据
              "TAXI_ID": {...},
              "DRONE_ID": {...},
          },
          "cnt": 12,                            # 还未收到消息的节点数
          "done": False,                        # 是否已经完成
      },
      ...
  }
  ```
  另一个是 `devices` 维护了哪些设备订阅态势数据 记录他们的设备 id 以及这个设备的 region 参数
- 中心每隔 1s 进行一次态势数据的计算 先向 situationData 中插入新的键值对 随后向中心的消息队列的 `situation/{nodeID}` topic 中放入 payload 为`timestamp`的消息 告知各个节点开始计算
- 中心收到节点返回的消息后立马进行合并 根据时间戳判断最新态势数据 如果 `nodes` 中已经收到过此节点的数据 则说明消息重复了 不处理 否则置为 TRUE 然后 cnt--
- 当发现所有 node 都已合并（即 cnt==0）则将 `done` 字段置为 TRUE 并遍历 devices 将态势数据发送到`situation/app/{deviceID}` topic 中 等待设备自取
- scheduler 类中还设置了两个定时任务
  - checktimeout 任务会定期查看每个请求的时间戳是否已经过时 2s 如果过时 2s 就删除 这个主要是为了防止因为某一个节点挂掉而导致一个未完成的态势数据一直存在内存里的情况
  - persistData 任务定期将已经完成的请求的态势数据持久化到数据库中 以便后续进行数据分析 这里不做处理直接作为文档存入 MongoDB 中（测试时怕数据溢出 所以每 1 分钟就把这个表 drop 掉）
