---
title: 在SJTU-REINS的打工日记
date: 2024-10-05 21:19:00
categories: 实验室
tags:
  - 打工
index_img:
banner_img:
excerpt: "大三开学顺利和 chp 面谈并进组开始打工 这个博客用来记录一下每一份工作项目的经历和经验"
---

大三开学顺利和 chp 面谈并进组开始打工 这个博客用来记录一下每一份工作项目的经历和经验

# LIGHTHOUSE

灯塔项目是一个类似于海上管理船只位置的云系统的项目 只不过空间由海域变为上海市区 对象由船只变为无人机等移动物联网设备

简单来说 灯塔系统会收集处于三维空间内各个个体的数据 并存储在一个云边融合系统（CEDS）内 之后将数据整理封装为态势数据 将态势数据分发回给每个个体 让个体通过实时的态势数据进行任务的规划和调度 态势数据会分为两部分 一部分是以个体为中心 在单位时间内可以安全独占的空间范围 另一部分是以个体为中心 依据任务需要而划分的一块空间范围内所有其它个体的数据

我负责的部分主要是从已有的 CEDS 系统中提取需要的数据 并封装为合适格式的态势数据进行分发 比较简单

## CEDS

CEDS 是黄子昂学长的硕士毕业论文中提出的云边融合存储系统 利用了边缘节点本身少量的数据存储与计算能力以及其分布于移动物联网设备附近的特点 实现了基于数据的就近存储、查询任务的划分下推以及负载感知的数据迁移功能的存储系统 做到了传输带宽的减少以及查询的加速 具体实现可以见其论文 我简单讲一下我的理解

### 边缘数据就近存储机制

大部分云边融合系统会将数据存储于资源富足可扩展的云端数据库中 但这会带来很多不必要的传输开销 利用边缘节点本身的存储能力 可以将数据就近存储 大大减少传输延迟

然而数据的就近存储会带来其他问题 比如原本全都位于云端中心的数据 由于就近存储就会分布在不同的边缘节点上 针对此问题 CEDS 提出一种基于 Rosetta 过滤器的全局索引机制 简单来讲 这个过滤器能够实现一个数据表内任一字段的范围过滤 即给定一个字段的某一范围 在常数时间内返回这个表是否存在这个范围内的数据 基于此种过滤器 加上基于时间划分的数据分表 可以做到在很短的时间内定位到某一个时间范围内的数据分表 并快速判断哪些分表内存在本次范围查询相关的数据 这就是 CEDS 的全局索引机制 基于此种索引 CEDS 在云端仅需存储数据表的元信息摘要（包括过滤器的 bitarray） 同时这也将为后文的查询下推提供便利

### 查询任务拆分下推

大部分云边存储系统基于边缘节点的存储数据 每次查询会将相关的数据表全部聚合到云端后进行筛选 产生了很多不必要的传输开销 CEDS 基于任务划分 将查询和聚合任务下推给各个相关的边缘子节点 减少带宽开销

具体而言 一次查询会先向云端中心索要本次查询可能涉及到的所有数据分表 以及这些分表存储在哪些边缘节点上 其方法可以简单的由之前提到的过滤器实现

获取到节点列表后 只需要把子查询任务分发给这些节点即可 每个子节点会进行相关的范围查询（节点内存在常规的索引来加快查询）并把查询结果返回 最终在查询节点进行数据聚合任务 把结果返回给用户

### 负载感知数据迁移

数据就近存储定会存在不均衡现象 由于查询任务划分给子节点并发执行 查询的延迟显然由查询时间最大的子查询决定 如果某些节点上的数据查询次数特别多 导致超出边缘节点的承受范围 导致子查询阻塞 就会导致所有涉及的查询延迟显著变高 为了防止此种情况出现 CEDS 会监测并记录每段时间内边缘节点的查询资源开销 比如记录前 3 分钟内某一节点的内存占用 一旦超过 1G 就判定存在热点数据 并将热点数据迁移到云端来减缓此节点的压力

此种策略是折中的策略 理论上可以达到最均衡的方案肯定是为每一个边缘节点根据查询开销动态分配性能资源 但这在现实中肯定不可行 因此使用较为折中的策略来解决数据不均衡分布的问题

### 部署

跟随仓库中的指导 在自己的电脑上部署一下 CEDS

1. 用 IDEA 跑起中心节点 需连接本地的 MySQL 和 MongoDB 服务
2. 用容器部署 12 个 MongoDB 作为边缘节点的数据库 再用容器部署 12 个边缘节点（基于 python） 手动在本地运行一个 center 节点用于查询 会监听 8000 端口
3. 本地运行 parse_csv 解析数据集 load_data 将数据导入边缘节点 最后用 register 接口把固定的元数据上传到数据中心
4. 运行各个查询进行试验即可

## 工作内容

### 思路

我的任务很简单 只需要进行查询后封装为相应格式的态势数据即可 即学会如何查询与如何使用正确的函数进行封装

### 如何查询所需数据

- 首先需要注册元信息 告诉数据中心这个元数据组的字段都是什么样的

  CEDS 提供了注册接口`/metaData/register` 请求体形如下：

  ```
  {
        "id": "6701140529b28d4cd90959d0",
        "topicPrefix": "taxi",
        "collectionName": "taxi",
        "payloadType": "json",
        "timeField": "TIMESTAMP",
        "fields": {
            "TAXI_ID": {"name": "TAXI_ID", "type": "str"},
            "TRIP_ID": {"name": "TRIP_ID", "type": "str"},
            "LONGITUDE": {
                "name": "LONGITUDE",
                "type": "float",
                "min": "-180",
                "max": "180",
                "granularity": "0.001",
            },
            "SPEED": {
                "name": "SPEED",
                "type": "float",
                "min": "0",
                "max": "120",
                "granularity": "1",
            },
            "LATITUDE": {
                "name": "LATITUDE",
                "type": "float",
                "min": "-180",
                "max": "180",
                "granularity": "0.001",
            },
        },
  }
  ```

  后续我们应该要先定好数据的元信息 然后进行测试

- CEDS 提供了查询接口`/statistic/searchTables`来查询数据相关的分片信息 接口的请求体格式类似如下：

  ```
      {
          "topic": topic,
          "realtime": False,
          "startTime": t_start * 60,
          "endTime": t_end * 60,
          "filters": [
              {"field": "LONGITUDE", "op": "bet", "val1": lon_start, "val2": lon_end},
              {"field": "LATITUDE", "op": "bet", "val1": lat_start, "val2": lat_end},
          ],
      }
  ```

  topic 即资源路径 realtime 我查看了代码 是注释掉的 用不着 startTime 和 endTime 就是时间范围 filter 内是各个字段的范围 比如这里有经度和纬度 经度对应的范围 op 是运算符 目前的实现中字符串只支持 equal 数值只支持 bet

  响应的数据形如下：shards 是数据包含在哪些节点的哪些分表里 比如这里是节点 1 的 05 到 11 分表 parseConfig 就是之前此元数据组注册的摘要

  ```
      {
          "shards": {"node1": [118605, 118606, 118607, 118608, 118609, 118610, 118611]},
          "parseConfig": {
              "id": "6701140529b28d4cd90959d0",
              "topicPrefix": "taxi",
              "collectionName": "taxi",
              "payloadType": "json",
              "timeField": "TIMESTAMP",
              "fields": {
                  "TAXI_ID": {"name": "TAXI_ID", "type": "str"},
                  "TRIP_ID": {"name": "TRIP_ID", "type": "str"},
                  "LONGITUDE": {
                      "name": "LONGITUDE",
                      "type": "float",
                      "min": "-180",
                      "max": "180",
                      "granularity": "0.001",
                  },
                  "SPEED": {
                      "name": "SPEED",
                      "type": "float",
                      "min": "0",
                      "max": "120",
                      "granularity": "1",
                  },
                  "LATITUDE": {
                      "name": "LATITUDE",
                      "type": "float",
                      "min": "-180",
                      "max": "180",
                      "granularity": "0.001",
                  },
              },
          },
      }
  ```

  当你向 8000 端口的查询中心发送请求后 其会先向云端数据中心发送上述请求并得到子节点列表 然后进行查询下推 返回结果

- 向 8000 的查询中心发送请求可以进行查询 这一步就是用户做的了 或者说我要做的 进行查询后封装

  请求体如下：

  ```
  {
    "function": "chadingdan",
    "module": "centerTasks",
    "filter": {
        "topic": "taxi",
        "realtime": False,
        "startTime": 1372662000,
        "endTime": 1372669200,
        "filters": [{"field": "TRIP_ID", "op": "str_eq", "val1": trip_id}],
    },
    "args": {"no": "args"},
  }
  ```

  这个请求会查询出 TRIP_ID 等于某一个值的所有数据 即某一个订单的轨迹数据 可以看到 function 是写死的类型 只针对了 taxi 一种数据组 不方便解耦 以后可以在应用层级上拓展

  论文内还测试了另外两种查询 一个是指定地区内经过的所有车辆 id 一个是时间范围内所有超速 60 的车辆 id 后者没有找到测试代码 不过查询的逻辑应该同理很简单

- 针对态势数据需要的查询 我们可以仔细分析一下：
  1. 第一部分态势数据需要确定一个 robot 在接下来某一段时间内可以安全独占的空间范围 重点是如何计算这个范围 比如查询附近某一小范围的所有 robot 然后根据速度信息进行计算？
  2. 第二部分态势数据需要确定一个 robot 在当前时间某一个大范围内所存在的其他 robot 的数据 重点是如何确定范围以及如何获取最新数据

### 汇报

- 目前假设：设备向边缘节点实时发送数据 边缘节点每隔 6s 向中心发送新的索引数据 中心每隔 1s 向各个边缘节点查询所有设备的实时数据 根据这个数据计算出所有设备的态势数据 发送给各个边缘节点 边缘节点再发送给设备

  - 几个问题 是中心来计算吧？是计算所有设备吧？是实时性（比如 1s）吧？态势数据的下发是设备发送请求还是中心主动下推？另外 边缘节点和设备通信是通过什么？中心和边缘节点呢？
  - 为什么选择中心来计算态势数据：中心有很高的计算资源 理论上不该让边缘节点来计算态势数据
  - 中心需要所有设备的实时数据 而中心如果不直接和所有设备进行通信 就必须通过边缘节点来间接获取所有设备的实时数据
  - 中心获取实时数据可以有两种方法 都把实时数据维护在内存里 维护在数据库里再查询的话非常慢
    1. 边缘节点获取到的所有数据都需要发送给中心 中心在内存里维护一个 map 存储形如<设备 ID, 数据>的键值对 每次收到新的数据就更新这个 map 然后使用这个 map 计算态势数据
    2. 边缘节点维护一个 map 存储形如<设备 ID, 数据>的键值对 中心每隔 1s 向各个边缘节点发送请求 边缘节点返回这个 map 中的所有数据 中心合并所有边缘节点的 map 然后计算态势数据
       感觉第二个好一点 因为中心计算完就可以把 map 丢掉 1 则需要一直维护并一直和边缘节点通信 而且 1 可能会有并发问题 不过 2 的话边缘节点的内存资源可能不够
  - 下发为什么不直接下发给设备 而是下发给边缘节点：因为中心并不直接和设备连接 只和边缘节点连接
  - 下发给设备可能出现一个问题：设备可能上一秒在这个节点 下一秒在另一个节点 假如断开连接 原来的节点就无法发送态势数据了
  - 如果需要压缩 map 大小 可以只存必要字段 比如计算态势数据需要空间位置和速度信息 可以只存这两个字段

- 问题：假设中心每隔 1s 向所有设备发送态势数据 首先中心需要收集所有设备的实时位置 而不可能收集单个设备的位置 如果用 chp 的思路 设计 RealtimeIndex 只能判断设备在哪个边缘节点中 无法判断设备最新数据在哪个分表 就算加上分表字段 也需要一个一个查询 多次查询开销很大 而且还是 IO 不如维护在内存里 不用查询 而是实时发送请求 本来边缘节点就需要一直和设备通信 不妨通信时顺便花一点内存维护一下 map 本来中心就需要每 1s 向边缘节点查询 不妨现在变为单纯的接受数据 省去查询的开销

  
- 查询部分：muban 函数里的 fragments 参数即哪些数据分片需要查询 这个 fragments 参数实际上由 fromCenter 函数得到 也就是向数据中心发送 searchTables 请求得到的数据 我们在这里改一下逻辑 如果是请求最新时间戳 应该向另一个 url 发送 让中心用另一套逻辑（也就是查询实时 index 表）返回某一个数据分片

- 上传部分：理论上用 kafka 的连接和断开事件来触发这个索引的发送 我觉得可以先写一个模拟移动设备的程序 在这个程序里读取数据库的数据 每隔 1s 发送一行记录 再在数据中心和边缘节点写一个上传数据的接口 模拟程序里需要一个函数来计算发送到哪个边缘节点 现在先用简单的经纬度映射来实现 上传的接口分为两部分 上传给数据库是好实现的 上传索引数据需要仔细看看

### 10.11 组会

- 强化学习进行独占空间的计算 需要输入哪些数据 目前看来需要与人的距离以及实时速度、加速度
- 态势数据累计到一定程度后 使用这些数据进行空间使用效能的分析 并根据这些分析找到可以提升效能的点 比如算法的改进 或是空间资源的调度
- 态势数据统一格式 设备的上传数据也需要屏蔽差异性

### 10.18 组会

- 元数据更新有 6s 的延迟 因此需要额外的信息来找到实时位置 大致思路是设备经过一个边缘节点时 边缘节点向中心发送进入的时间戳索引 离开时发送离开的时间戳索引 这样就可以通过这两个时间戳知道设备实时位于哪个边缘节点中 索引形如<设备 ID, 边缘节点 ID, 进入时间戳, 离开时间戳> 通过这个索引可以找到设备在哪个边缘节点中的数据分表 可以通过 kafka 的连接和断开事件来触发这个索引的发送
