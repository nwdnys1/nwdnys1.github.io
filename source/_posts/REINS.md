---
title: 在SJTU-REINS的打工日记
date: 2024-10-05 21:19:00
categories: 实验室
tags:
  - 打工
index_img:
banner_img:
excerpt: "大三开学顺利和 chp 面谈并进组开始打工 这个博客用来记录一下每一份工作项目的经历和经验"
---

大三开学顺利和 chp 面谈并进组开始打工 这个博客用来记录一下每一份工作项目的经历和经验

# LIGHTHOUSE

灯塔项目是一个类似于海上管理船只位置的云系统的项目 只不过空间由海域变为上海市区 对象由船只变为无人机等移动物联网设备

简单来说 灯塔系统会收集处于三维空间内各个个体的数据 并存储在一个云边融合系统（CEDS）内 之后将数据整理封装为态势数据 将态势数据分发回给每个个体 让个体通过实时的态势数据进行任务的规划和调度 态势数据会分为两部分 一部分是以个体为中心 在单位时间内可以安全独占的空间范围 另一部分是以个体为中心 依据任务需要而划分的一块空间范围内所有其它个体的数据

我负责的部分主要是从已有的 CEDS 系统中提取需要的数据 并封装为合适格式的态势数据进行分发 比较简单

## CEDS

CEDS 是黄子昂学长的硕士毕业论文中提出的云边融合存储系统 利用了边缘节点本身少量的数据存储与计算能力以及其分布于移动物联网设备附近的特点 实现了基于数据的就近存储、查询任务的划分下推以及负载感知的数据迁移功能的存储系统 做到了传输带宽的减少以及查询的加速 具体实现可以见其论文 我简单讲一下我的理解

### 边缘数据就近存储机制

大部分云边融合系统会将数据存储于资源富足可扩展的云端数据库中 但这会带来很多不必要的传输开销 利用边缘节点本身的存储能力 可以将数据就近存储 大大减少传输延迟

然而数据的就近存储会带来其他问题 比如原本全都位于云端中心的数据 由于就近存储就会分布在不同的边缘节点上 针对此问题 CEDS 提出一种基于 Rosetta 过滤器的全局索引机制 简单来讲 这个过滤器能够实现一个数据表内任一字段的范围过滤 即给定一个字段的某一范围 在常数时间内返回这个表是否存在这个范围内的数据 基于此种过滤器 加上基于时间划分的数据分表 可以做到在很短的时间内定位到某一个时间范围内的数据分表 并快速判断哪些分表内存在本次范围查询相关的数据 这就是 CEDS 的全局索引机制 基于此种索引 CEDS 在云端仅需存储数据表的元信息摘要（包括过滤器的 bitarray） 同时这也将为后文的查询下推提供便利

### 查询任务拆分下推

大部分云边存储系统基于边缘节点的存储数据 每次查询会将相关的数据表全部聚合到云端后进行筛选 产生了很多不必要的传输开销 CEDS 基于任务划分 将查询和聚合任务下推给各个相关的边缘子节点 减少带宽开销

具体而言 一次查询会先向云端中心索要本次查询可能涉及到的所有数据分表 以及这些分表存储在哪些边缘节点上 其方法可以简单的由之前提到的过滤器实现

获取到节点列表后 只需要把子查询任务分发给这些节点即可 每个子节点会进行相关的范围查询（节点内存在常规的索引来加快查询）并把查询结果返回 最终在查询节点进行数据聚合任务 把结果返回给用户

### 负载感知数据迁移

数据就近存储定会存在不均衡现象 由于查询任务划分给子节点并发执行 查询的延迟显然由查询时间最大的子查询决定 如果某些节点上的数据查询次数特别多 导致超出边缘节点的承受范围 导致子查询阻塞 就会导致所有涉及的查询延迟显著变高 为了防止此种情况出现 CEDS 会监测并记录每段时间内边缘节点的查询资源开销 比如记录前 3 分钟内某一节点的内存占用 一旦超过 1G 就判定存在热点数据 并将热点数据迁移到云端来减缓此节点的压力

此种策略是折中的策略 理论上可以达到最均衡的方案肯定是为每一个边缘节点根据查询开销动态分配性能资源 但这在现实中肯定不可行 因此使用较为折中的策略来解决数据不均衡分布的问题

### 部署

跟随仓库中的指导 在自己的电脑上部署一下 CEDS

1. 用 IDEA 跑起中心节点 需连接本地的 MySQL 和 MongoDB 服务
2. 用容器部署 12 个 MongoDB 作为边缘节点的数据库 再用容器部署 12 个边缘节点（基于 python） 手动在本地运行一个 center 节点用于查询 会监听 8000 端口
3. 本地运行 parse_csv 解析数据集 load_data 将数据导入边缘节点 最后用 register 接口把固定的元数据上传到数据中心
4. 运行各个查询进行试验即可

## 工作内容

### 思路

我的任务很简单 只需要进行查询后封装为相应格式的态势数据即可 即学会如何查询与如何使用正确的函数进行封装

### 如何查询所需数据

- 首先需要注册元信息 告诉数据中心这个元数据组的字段都是什么样的

  CEDS 提供了注册接口`/metaData/register` 请求体形如下：

  ```
  {
        "id": "6701140529b28d4cd90959d0",
        "topicPrefix": "taxi",
        "collectionName": "taxi",
        "payloadType": "json",
        "timeField": "TIMESTAMP",
        "fields": {
            "TAXI_ID": {"name": "TAXI_ID", "type": "str"},
            "TRIP_ID": {"name": "TRIP_ID", "type": "str"},
            "LONGITUDE": {
                "name": "LONGITUDE",
                "type": "float",
                "min": "-180",
                "max": "180",
                "granularity": "0.001",
            },
            "SPEED": {
                "name": "SPEED",
                "type": "float",
                "min": "0",
                "max": "120",
                "granularity": "1",
            },
            "LATITUDE": {
                "name": "LATITUDE",
                "type": "float",
                "min": "-180",
                "max": "180",
                "granularity": "0.001",
            },
        },
  }
  ```

  后续我们应该要先定好数据的元信息 然后进行测试

- CEDS 提供了查询接口`/statistic/searchTables`来查询数据相关的分片信息 接口的请求体格式类似如下：

  ```
      {
          "topic": topic,
          "realtime": False,
          "startTime": t_start * 60,
          "endTime": t_end * 60,
          "filters": [
              {"field": "LONGITUDE", "op": "bet", "val1": lon_start, "val2": lon_end},
              {"field": "LATITUDE", "op": "bet", "val1": lat_start, "val2": lat_end},
          ],
      }
  ```

  topic 即资源路径 realtime 我查看了代码 是注释掉的 用不着 startTime 和 endTime 就是时间范围 filter 内是各个字段的范围 比如这里有经度和纬度 经度对应的范围 op 是运算符 目前的实现中字符串只支持 equal 数值只支持 bet

  响应的数据形如下：shards 是数据包含在哪些节点的哪些分表里 比如这里是节点 1 的 05 到 11 分表 parseConfig 就是之前此元数据组注册的摘要

  ```
      {
          "shards": {"node1": [118605, 118606, 118607, 118608, 118609, 118610, 118611]},
          "parseConfig": {
              "id": "6701140529b28d4cd90959d0",
              "topicPrefix": "taxi",
              "collectionName": "taxi",
              "payloadType": "json",
              "timeField": "TIMESTAMP",
              "fields": {
                  "TAXI_ID": {"name": "TAXI_ID", "type": "str"},
                  "TRIP_ID": {"name": "TRIP_ID", "type": "str"},
                  "LONGITUDE": {
                      "name": "LONGITUDE",
                      "type": "float",
                      "min": "-180",
                      "max": "180",
                      "granularity": "0.001",
                  },
                  "SPEED": {
                      "name": "SPEED",
                      "type": "float",
                      "min": "0",
                      "max": "120",
                      "granularity": "1",
                  },
                  "LATITUDE": {
                      "name": "LATITUDE",
                      "type": "float",
                      "min": "-180",
                      "max": "180",
                      "granularity": "0.001",
                  },
              },
          },
      }
  ```

  当你向 8000 端口的查询中心发送请求后 其会先向云端数据中心发送上述请求并得到子节点列表 然后进行查询下推 返回结果

- 向 8000 的查询中心发送请求可以进行查询 这一步就是用户做的了 或者说我要做的 进行查询后封装

  请求体如下：

  ```
  {
    "function": "chadingdan",
    "module": "centerTasks",
    "filter": {
        "topic": "taxi",
        "realtime": False,
        "startTime": 1372662000,
        "endTime": 1372669200,
        "filters": [{"field": "TRIP_ID", "op": "str_eq", "val1": trip_id}],
    },
    "args": {"no": "args"},
  }
  ```

  这个请求会查询出 TRIP_ID 等于某一个值的所有数据 即某一个订单的轨迹数据 可以看到 function 是写死的类型 只针对了 taxi 一种数据组 不方便解耦 以后可以在应用层级上拓展

  论文内还测试了另外两种查询 一个是指定地区内经过的所有车辆 id 一个是时间范围内所有超速 60 的车辆 id 后者没有找到测试代码 不过查询的逻辑应该同理很简单

### 汇报

- realtime_map 中接收消息时的 time 和消息中的 time 有延迟 差不多是 2s 不知道为什么 难道消息处理不是实时的？
- 节点刚好处于大边界时可能会有点问题 暂时没做处理
- 断开连接时如何告知节点
- 并发处理
  - 节点计算态势数据用了线程池 向邻居节点发送请求也可以并发处理 不过由于 http 有长连接机制 请求不会重新建立连接 且一个设备的计算最多也就发送 3 个请求（大多数情况是 1 个） 所以并发处理的意义不大
  - 中心的去重和消息的处理**可以并发** 注意同一个 requestId 的消息不能并发处理 否则没法去重
- 中心提供一个接口/situation body 包含 region 参数和应用的 token 应用 A 发送请求后 中心立马生成一个唯一标识（应用 ID通过参数获取 类似于 token）返回给应用 中心后台继续根据 region 涉及的节点 向消息队列中放入 topic 为 situation/{nodeID} 的消息 消息中要包含请求 ID 告知节点开始计算 节点计算完后把态势数据放入中心的消息队列 situation/center/{requestId} 的 topic 中 消息中要包含请求 ID、节点 ID、态势数据内容 中心在内存中维护一个 map 键为请求的 ID 值需要包含哪些 node 收到了消息、合并去重后的态势数据、请求的时间戳 收到消息时就立马进行合并 当发现所有 node 都已合并后 放入内存 然后发回到消息队列的 situation/app/{requestId} 等应用来取 如果发生异常 中心的消息队列会一直缺少某个节点的态势数据 考虑使用定时器进行定期检查 超时 2s 后返回 error 已经 done 的数据持久化到数据库里
- 在 flask 里写一个接口 会发送态势数据的查询请求 然后等待消息队列里的态势数据 计算 RTT
  - 经测验空转时 RTT 基本上是 0.01s 左右
  - 10 个设备 基本上 RTT 在 0.05s 以下
  - 查询 9 个节点 共 294 个设备 在 0.2-0.3s 之间 这还是只涉及查询 并未涉及强化学习
  - 如果 error 则在 2s 左右（因为 timeout 设置的是 2s）
  - 每隔几秒 RTT 会突然变大 猜测是定时落盘导致的？
  - 上述测试包含网络时延大概在几十 ms 左右
  - 并且这还只是串行请求 高并发的请求可能更离谱
  - 而且我看 CPU 占用已经很高了 后续压测估计要减点数据量
- 使用 apache avro 进行数据的压缩
  - 对于不同的设备有不同的 schema 解决方法有二
    1. schema 中采用 union 类型 也就是一个数组 avro 会自动匹配合适的 type 这种会浪费性能
    2. 建立多个 schema 文件 预读取为 schemas 解析 topic 中的设备类型来选择合适的 schema
  - situ_cal 查询的数据中包含所有类型的设备 不过好在计算态势数据需要的字段可以维持一致 不需要用 union
  - 以下数据传输可以被压缩
    - 设备传输数据给 realtime_map 与 data_receiver
    - situ_cal 通过 http 请求查询 reatime_map
    - 节点传输态势数据给中心
  - 还需要考虑 如何自动生成 schema 文件？如何定义统一的 realtime_map 的 schema？（环境变量）
- RTT 的因素有很多 查询态势数据的请求数 节点的计算资源

### 12.6 组会

- 根据不同的 target（任务） bigcircle 的大小可能会不同 只需要加一个参数即可
- 另外 甲方有性能要求 比如能做到 1s 内返回结果的话 独占空间至少得是 1s\*速度的大小 才能保证设备在这段时间内不会碰到别的设备 因此要么给出更大的独占空间 要么做到更快的计算速度 计算的瓶颈主要在于独占空间的计算 我这边主要是节点内部做多线程处理 以及 http 请求的加速 还有 range_query 的优化 中心的并发去重
