---
title: SJTU-REINS
date: 2024-10-05 21:19:00
categories: REINS
tags:
  - 实习
  - 开发
  - 架构
  - 运维
  - 测试
index_img:
banner_img:
excerpt: "大三顺利进入REINS打工 记录一下项目经历和经验"
---

# PHAROS

灯塔项目是一个类似于海上管理船只位置的云系统的项目 只不过空间由海域变为上海市区 对象由船只变为无人机等移动物联网设备

简单来说 灯塔系统会收集处于三维空间内各个个体的数据 并存储在一个云边融合系统（CEDS）内 之后将数据整理封装为态势数据 将态势数据分发回给每个个体 让个体通过实时的态势数据进行任务的规划和调度 态势数据会分为两部分 一部分是以个体为中心 在单位时间内可以安全独占的空间范围 另一部分是以个体为中心 依据任务需要而划分的一块空间范围内所有其它个体的数据

灯塔的核心思想是流控分离 即中心会分发数据流给个体设备 但是不会干涉个体设备的任务规划和调度 只是给出了安全空间的限制 个体设备会根据态势数据自行调整任务的路径 和现有的大部分统筹任务路线规划是不同的

## CEDS

CEDS 即黄子昂学长的硕士毕业论文中提出的云边融合存储系统 利用了边缘节点本身少量的数据存储与计算能力以及其分布于移动物联网设备附近的特点 实现了基于数据的就近存储、查询任务的划分下推以及负载感知的数据迁移功能的存储系统 做到了传输带宽的减少以及查询的加速 具体实现可以见其论文 大概包含以下几个部分：

### 边缘数据就近存储机制

大部分云边融合系统会将数据存储于资源富足可扩展的云端数据库中 但这会带来很多不必要的传输开销 利用边缘节点本身的存储能力 可以将数据就近存储 大大减少传输延迟

然而数据的就近存储会带来其他问题 比如原本全都位于云端中心的数据 由于就近存储就会分布在不同的边缘节点上 针对此问题 CEDS 提出一种基于 Rosetta 过滤器的全局索引机制 简单来讲 这个过滤器能够实现一个数据表内任一字段的范围过滤 即给定一个字段的某一范围 在常数时间内返回这个表是否存在这个范围内的数据 基于此种过滤器 加上基于时间划分的数据分表 可以做到在很短的时间内定位到某一个时间范围内的数据分表 并快速判断哪些分表内存在本次范围查询相关的数据 这就是 CEDS 的全局索引机制 基于此种索引 CEDS 在云端仅需存储数据表的元信息摘要（包括过滤器的 bitarray） 同时这也将为后文的查询下推提供便利

### 查询任务拆分下推

大部分云边存储系统基于边缘节点的存储数据 每次查询会将相关的数据表全部聚合到云端后进行筛选 产生了很多不必要的传输开销 CEDS 基于任务划分 将查询和聚合任务下推给各个相关的边缘子节点 减少带宽开销

具体而言 一次查询会先向云端中心索要本次查询可能涉及到的所有数据分表 以及这些分表存储在哪些边缘节点上 其方法可以简单的由之前提到的过滤器实现

获取到节点列表后 只需要把子查询任务分发给这些节点即可 每个子节点会进行相关的范围查询（节点内存在常规的索引来加快查询）并把查询结果返回 最终在查询节点进行数据聚合任务 把结果返回给用户

### 负载感知数据迁移

数据就近存储定会存在不均衡现象 由于查询任务划分给子节点并发执行 查询的延迟显然由查询时间最大的子查询决定 如果某些节点上的数据查询次数特别多 导致超出边缘节点的承受范围 导致子查询阻塞 就会导致所有涉及的查询延迟显著变高 为了防止此种情况出现 CEDS 会监测并记录每段时间内边缘节点的查询资源开销 比如记录前 3 分钟内某一节点的内存占用 一旦超过 1G 就判定存在热点数据 并将热点数据迁移到云端来减缓此节点的压力

此种策略是折中的策略 理论上可以达到最均衡的方案肯定是为每一个边缘节点根据查询开销动态分配性能资源 但这在现实中肯定不可行 因此使用较为折中的策略来解决数据不均衡分布的问题

## 技术调研

### Redisearch

- 项目使用了 redisearch 建立设备数据的地理索引 实现实时数据查询的加速 即把经纬度作为 GEO 类型的元素编写索引
- 具体实现
  - 文档参考：https://redis.io/docs/latest/develop/interact/search-and-query/indexing/（redis不同类型的索引如何使用）https://redis.readthedocs.io/en/stable/examples/search_json_examples.html#Projecting-using-JSON-Path-expressions（将 JSON 数据添加到索引的示例代码）
  - redis 实例需要使用 redis-stack 项目中使用 python 因此安装 pip 库的 redis 即可
  - 具体代码见项目中的`realtime_map.py`文件 只需要先创建 schema 描述索引的字段类型 然后添加索引即可

### Apache Avro

- 项目使用了 apache avro 进行数据的压缩 包括：
  - 设备传输数据给 realtime_map 与 data_receiver
  - 节点计算态势数据时向其他节点发送`/query`请求查询 reatime_map
  - 节点通过消息队列传输态势数据给中心
- 具体实现
  - 文档参考：https://avro.apache.org/docs/
  - python 需要安装 avro 库 java 则通过 maven 导入 avro 依赖
  - 需要预先编写好数据的 schema 语法见文档 编写好后即可使用 avro 的序列化与反序列化方法进行数据的压缩与解压
  - python 中数据类型比较自由 使用 dict 类型即可操作 但是 java 的数据类型比较严格 avro 本身提供一个 record 类封装数据 但是我使用了 fastjson 库的 json 类型来操作数据 这就导致必须要编写一个转换函数进行转换 并且这个转换需要递归分类进行 一旦类型对应不上 序列化时就会报错

### 时空索引

- 时空索引是一种将时空数据进行编码以便于快速查询的技术 我们可以先简单认为时空索引需要解决这样的查询：查询某一个时间段内某个空间范围内所有的数据
- 先从空间索引开始说起 目前主流的空间索引可以分为三种：哈希、树以及空间填充曲线
  - 哈希思想主要是通过网格哈希索引来实现的 也就是把已有的地理空间划分为网格 每个网格相当于一个哈希桶 哈希索引里就会存储每一个网格内有哪些数据 假如需要查询某一个范围内的所有数据 只需要先根据范围找到对应网格 然后再在哈希桶内找到对应数据即可 这个方案偏理论 实际工程不太用 对于灯塔项目 一个网格可能会有很多数据 尤其是不同时间段的数据 会导致哈希桶内数据量过大 失去意义
  - 基于树的索引包括四叉树、R 树等
    - 四叉树仍然是一种网格索引 但是它是一种递归的网格索引 不断四划分空间直到触发阈值为止 对于范围查询 四叉树需要递归的找到所有被包含的节点
    - R 树是 B 树在高维空间的扩展 其结构很简单 适合静态空间对象的索引
      - 先引入几个概念 BB 即 Bounding Box 是一个包含了空间对象的矩形 MBB 则是最小的 BB
      - R 树的非叶子节点存放(BB,ptr)的键值对 叶子节点则存放(MBB,ptr)的键值对 叶子节点的 ptr 直接指向了空间对象数据的位置 其可以是质点 也可以是几何对象
      - 需要进行范围查询时 R 树和四叉树一样 需要递归寻找被范围包含的节点
  - 基于降维的索引通常使用空间填充曲线来实现 GM 部分已经详细讲过了 简单概括就是使用递归二分把高维数据映射到 bytearray 然后 Geohash 会通过 base32 编码得到字符串
- 加入时间维度后 有三种主流的实现方式 即基于时间分片、基于空间分片以及时空维度同时进行索引
  - 基于时间分片是目前最主流的时空索引方式 比如 ES 使用空间填充曲线、PG 使用 R 树作为空间索引等 然后数据按照时间进行分片 个人认为如果灯塔的历史数据分析不需要跨很大的时间段 是可以使用这种方式的
  - 基于空间分片即把空间网格化后分表 由于查询需求 很少被实际应用
  - 时空同时进行索引 在 GM 中的 Z3 索引中有实现 大致可以理解为把时间也进行二分后和空间数据一起编码为 bytearray 在时空尺寸不匹配时会有严重的空间放大问题 详见京东 JUST 的论文
  - 另外 轨迹数据比较适合空间填充曲线 R 树等基本可以不考虑 空间填充曲线若使用 B+树 可能仍存在写入性能差的情况 但是很多非关系型数据库采用的是 LSM 树结构 写入性能会好很多
- 综上 要么基于时间分片 要么时空一起编码
  - 实际上 时空一起编码的索引 还是需要按时间分片 否则会有严重的空间放大问题
  - 既然如此 就变成决定时间参不参与编码了
  - 从感性上讲 我觉得时间参不参与不太影响查询性能 对于一个时空查询 其先会按照时间被分到不同分片上 中间的分片都是占满了时间段的 因此时间编码失去意义 只有两端的分片是可以用时间编码剪枝的 如果我们的查询时间范围总是大于一个分片的时间范围 那么时间参与编码意义是不大的
  - 另外还有一个查询是直接 scan 还是先递归到基本分辨率后再 scan 的问题 我认为直接 scan 即可 因为磁盘上的随机访问仍然需要扫过未查询的数据 只不过省去了判断数据是否符合条件的过程 而递归查询范围这个过程可能本身就比较慢 JUST 是直接 scan 的
- 参考：https://zhuanlan.zhihu.com/p/663029637

#### GeoMesa

- GM 本身是一个类似于索引引擎的工具 首先需要选取合适的存储系统 要么 Redis 要么 HBase 等数据库 或者也可以直接通过 FS 存储 但是性能较差
- 现有的 redisearch 只支持 2d 数据的复合索引 要引入高度的话 可能只能遍历筛选 取决于独占空间部分需不需要 3d point
- 我们使用 GM 的目的是为了加速大量历史数据的查询 进行对历史数据的分析 而对于数据的写入与实时性并无要求 那么有两种方案
  - 先考虑如何使用 mongodb 存储数据 然后通过 GM 进行查询
  - 一种是保留现有的 mongodb 数据库 也就是近期数据仍向 mongodb 写入 然后定期（比如 1 天）把数据迁移到适合存储大量时空数据的数据库中（比如 HBase） 并且这个迁移过程通过 GM 实现（比如把 mongo 数据批量读到内存里 在通过 GM 写入 HBase）也就自动创建了时空索引 后续查询也就提高了性能 （代码量会小一点）
  - 另一种则是抛弃 mongodb 直接通过 GM 持续写入数据 大致流程就是通过 mqtt 接受消息 把消息里的数据解析为 GM 的 simplefeature 类 然后通过 writer 写入 HBase 里 自动建立索引 省去了 mongodb 的部分 但是代码量会大一点
- 方便切换其他数据库指的是最后存储历史数据的数据库 索引层可以固定
- GM 现有的 API 都是大型分布式数据库 如果要测试 可以先部署两个容器试试
- GM 支持空间索引和时空索引 其中时空索引支持 2d point+时间 或是 2d 非 point（polygon line 等）+时间 其中 POINT 类型不知道是否支持 3d redis 由于本身不支持 3d 数据类型 所以没法存储 HBase 还没研究
- 原理部分
  - GM 如何写入数据？（以 HBase 为例）
    - 参考：https://zhuanlan.zhihu.com/p/164645879
    - 插入一个 feature 时 进行以下几步操作
      1. 预处理 假如数据没有 id 生成 uuid 然后为数据添加合适的属性 方便插入 HBase 表 为了防止数据倾斜 最终对于 id 进行哈希 放入对应分区
      2. 计算索引值 GM 会找到数据的 geo 属性和 date 属性 提取出数据 计算索引值 其中时间数据为了适应 GeoHash 机制 采用了 BinnedTime 机制 即相对于 1970 年 1 月 1 号形成有限的 chunk 接下来开始计算索引 geo 数据为 double 类型 dtg 则为 long 类型 先进行标准化等处理后 最终通过编码得到索引值
      3. 索引值会被编码为 byte 数组 写入 HBase 表中
      4. 最后数据被序列化 写入 HBase 表中 默认使用 kryo 序列化 也可以使用 avro
  - GM 如何进行数据的序列化？
    - 参考：https://zhuanlan.zhihu.com/p/164647326
    - 分两步 分别是序列化 feature 和 type 为什么需要序列化 type？因为插入数据时需要判断是否共用连接 GM 会把 type 序列化后存放在缓存里 方便后续反序列化出来进行判断 相当于一个全局的 schema
    - ![](https://image.blog.nwdnysl.site/20250224205422-99fe17067b124be578cba66601944766.png)
    - 具体序列化过程略 可以视为和 avro 一样根据 schema 压缩为二进制数据
  - GM 如何进行数据的索引？
    - 参考：https://zhuanlan.zhihu.com/p/164748160
    - Z2 索引：将二维空间编码为一维 以经纬度数据为例 通过空间填充曲线决定了数据的唯一顺序 也就映射到了一维整数上（这个过程相当于分别不断二分经纬度） 从而支持 1d 的键值索引 当需要进行 2d 的范围查询时 递归的寻找所有被区域包含的字符串 然后通过二分查找快速找到所有数据 或者对编码作主键建立 B+树索引
    - Z3 索引：将二维空间和时间编码到一维 时间按照 time period 切分 取每一段内的 offset 作为二分的依据 然后和空间数据一起编码为 bytearray 这种方法具有显著的空间放大问题 后续会讲到
    - XZ2 索引：将二维非点对象编码为一维 与 Z2 类似 通过找到空间对象的 mbb 来进行编码 相当于找到 Z2 编码中 mbb 的左下角方块的编码
    - XZ3 索引：将二维非点对象和时间编码为一维 类似

#### TrajMesa

- JUST 的前身 基于 GM 实现的分布式 NoSQL 轨迹查询引擎
- 工作可以分为三部分：预处理、建立索引以及查询
- 预处理
- 建立索引与存储
  - 论文提到 传统的垂直存储（也就是一个点存一行）对于轨迹数据来说不适合 包括查询一个轨迹比较慢、IO 次数多等问题 因此 TM 采用了水平存储 一行存一个轨迹 包括轨迹的元数据（mbr、时间范围等）、点 list（经压缩）、签名（一个 4\*4 的编码来描述轨迹的形状）、其他属性
  - TM 存在两份轨迹数据的副本表 分别是 IDTI 和 SRI 即 IDTQ 的索引表和 SRQ 的索引表
    - IDTI 表的 Key 为 `shard（随机数）+ 设备 id+BinNum（距离 RefTime 的第几个 Bin）+ EleCode（Bin 内的时间戳）+ 轨迹 id`
    - SRI 表基于 GM 的 XZ2 索引实现 Key 为 `shard + PosCode（与签名类似的 2\*2 编码 细化轨迹形状）+ XZ2（XZ2 产生的编码）+ 轨迹 id` 注意签名是基于 mbr 的 而 PosCode 是基于 XZ2 空间的
- 查询
  - 支持 4 种查询：某一个设备在一段时间内所有轨迹的查询（IDTQ）、空间范围查询（SRQ）、相似性查询（SQ）以及 knn 查询（KNNQ）
  - IDTQ：查询窗口=`可能的shard+设备 id+与时间范围相交的BinNum+每个Bin的偏移范围` 然后通过并发扫描 并删去不符合条件的轨迹
  - SRQ：查询窗口=`可能的shard+与空间范围相交的XZ2编码+每一个XZ2子空间对应的PosCode`
  - SQ 和 KNNQ 讲的比较晦涩 暂时没看懂

#### JUST

- 参考：https://zhuanlan.zhihu.com/p/300606530
- 基于 TM 作为底层存储的完整数据引擎系统 我们主要看其更新的时空索引部分
- Z2T 索引
  - Z3 索引用于时空数据的编码 然而存在空间放大问题 即索引中时间的粒度过大 会涉及到很多不必要的数据 为了解决这个问题 JUST 提出了 Z2T 索引
  - Z2T 索引很简单 就是把时间分片 索引变为`Num(T)+Z2` 其中 Num(T) 是时间分片的编号 查询时相当于先按时间范围剪枝 然后扫描 Z2 索引的 min-max 范围
- XZ2T 索引
  - XZ2T 索引用于非点对象的编码 与 Z2T 类似 只是把 Z2 换成了 XZ2
  - 我其实不懂上述索引和按时间分片有什么区别
- 除此之外 JUST 还实现了 SQL 查询和存储引擎 包括 Plugin Table（预定义 schema 的表）、View Table（缓存查询中间结果为 DF）

#### JUST-Traj

- 结合了 JUST 和 TrajMesa 的优点 实现了一个轨迹数据管理系统
- 索引
  - XZ2+索引：即 TM 里 SRI 的索引 XZ2 再加上一个 PosCode
  - XZT 索引：即 TM 里的 IDTI 索引 BinNum+EleCode
  - XZ2+T 索引：按时间分片后的 XZ2+索引

#### 其他时空引擎

- GeoMesa：支持各种分布式数据库以及本地 fs
  - 基本原理如上 是把高维数据使用 Z 曲线编码为字符串 然后查询时使用递归 支持 2d+时间的索引
  - 尝试过 cassandra 和 redis 查过文档 没找到支持 3d POINT 的方法 其 srid 也只支持 4326 也许默认只能用于经纬度
  - 文档：https://www.osgeo.cn/geomesa/index.html
- PostGIS：是 PostgreSQL 的一个插件 支持空间索引 存储 3d 空间数据 甚至支持 GeoServer 接入
  - 原理：有三种索引 GIST BRIN 和 SP-GiST 如果后续决定使用 可以做性能测试
    - GiST 即通用搜索树 支持各种数据类型 PG 实际上是在这个基础上实现了 R 树
    - BRIN 是一种轻量级索引类型 专为处理非常大的表而设计 它通过存储数据块范围（block range）的摘要信息 而不是每个数据行的索引值 从而显著减少索引的存储空间和维护成本 简单来说 有点像 LSM 树 存储了每个分块的最值 从而可以快速定位到某个范围内的数据
    - SP-GiST 是一种支持分区搜索树的通用索引方法 意为空间分区的 GIST 适用多维空间数据 通过四叉树、kd 树等递归分割数据
  - 文档中写到：坐标可以包含可选的 Z 和 M 坐标值。 Z 坐标通常用于表示高程。 M 坐标包含一个度量值，该值可以表示时间或距离。 如果几何图形值中存在 Z 或 M 值，则必须为几何图形中的每个点定义这些值。 如果几何图形具有 Z 或 M 坐标，则坐标尺寸为 3D; 如果它同时具有 Z 和 M，则坐标尺寸为 4D。
  - 也就是说可以存储 4d 的 POINT（x,y,z,m） 并支持上述三种索引 我简单试了一下 是可以存储 4d 的 如果要使用 就考虑把时间数据进行分片 肯定不能存整个时间戳
  - 文档：http://postgis.net/docs/manual-3.5/
- Big Query：是 Google 的一个云数据仓库 支持 SQL 查询 支持空间数据类型
  - 和 redisearch 有点像 支持存储 GEO 类型数据 并且进行空间查询 但是不支持时空索引
  - 另外 google 的数据库区域基本都在欧美 而且要付费 甚至 api 可能还需要代理 基本排除这个方案
  - 文档：https://cloud.google.com/bigquery/docs?hl=zh-cn
- Snowflake：是一个云数据仓库 支持 SQL 查询 和 Big Query 类似 支持 GEO 类型数据 以及一些空间查询 不支持时空索引 并且也需要付费 也可以排除
  - 文档：https://docs.snowflake.com/
- RedShift：亚马逊的云数据仓库 亚马逊云账户需要绑卡 因此我没有做测试
  - 看了下文档 和 PG 一样支持 ZM 的 POINT 类型
  - 文档：https://docs.aws.amazon.com/redshift/latest/dg/welcome.html
- GeoLake：基于湖仓的空间数据层 没找到相关文档 看 github 介绍 应该是支持 ZM 的空间类型数据 但是没有提及时间数据
- GeoWave：和 GM 类似的索引软件 支持分布式数据库和 fs 等 支持 3 维空间数据和时间 并且可以选择多种索引 基本上 GW 对于分布式键值数据库就像是 PG 对于 PostgreSQL
  - 试着用了一下 bbox 查询语法不支持 3 维 大概率不支持三维空间查询 不知道文档里说的支持 3 维存储和索引是什么意思
  - 文档中介绍空间索引时也举了 3 维+时间的例子 原理和 GM 一样 递归分解 代码语法基本也和 GM 一样
  - 文档：https://locationtech.github.io/geowave/overview.html
  - python 接口：https://locationtech.github.io/geowave/latest/pydocs/
- Elastic Search：分布式搜索引擎 查看了文档 支持 geo 类型的空间数据 但是仅限于 2d 经纬度数据 可以通过复合条件查询实现 3d+时间的范围查询 不知性能如何
  - 文档：https://www.elastic.co/docs
- 总结
  - GM 和 GW 的索引原理差不多 GW 声称支持 3d 空间数据 但是还未找到用法 如果找到了则可以直接使用 另外 Z3 索引存在空间放大问题 可能需要测试性能
  - PostGIS 支持 4d 空间数据 可以把时间数据分片后作为 M 维存储 但是需要进一步测试性能 GL 应该和 PG 差不多 但是前者显然更完善成熟
  - 几个云数据库基本排除 都只支持空间索引
  - ES 支持时空数据的查询 但是不清楚具体查询用到的索引如何
  - 如果都不支持 自己实现 无论时间参不参与编码 都需要先把时间分片 我觉得时间参与编码意义不大 因为时间字段是可以自然有序的

### PostGIS

- 主要任务是研究一下基于八叉树的 SP-GiST 索引

#### GiST

- 参考：https://habr.com/en/companies/postgrespro/articles/444742/
- GiST 和 B+树类似 区别在于 GiST 的可扩展性 B 树只支持大于小于等于的比较操作 而 G 可以支持相对位置运算符（R 树的左侧右侧等） 亦或是 RD 树的交集等运算符 在某种意义上 我们可以认为 G 是一种接口 是各种索引实现的一个基础框架
  - 结构：平衡树 所有叶节点的深度相等 每个节点代表一个集合区间（也就是一个谓词条件）且节点之间可以有交集 根节点是全集合区间 越往下集合区间越小 在搜索数据时使用 consistent 函数逐级作 dfs 搜索 将所有满足搜索条件的节点返回
  - 基于 R 树：R 树将平面拆分为多个矩形 一个节点代表一个矩形 叶节点代表空间对象 一致函数则是判断矩形是否相交 如果对空间数据进行 G 索引 则采用 R 树作为基础
  - 基于 RD 树：这是 PG 对于全文搜索采用的索引结构

#### SP-GiST

- 参考：https://habr.com/en/companies/postgrespro/articles/446624/
- 从名字可以看出 SP-GiST 是对于 G 的一种扩展 SP 指的是空间分区 SP 索引适用于任何值域空间可以递归划分为非相交区域的数据
  - 结构：非平衡树 因为每个节点的区域都是不相交的 每个内部节点存储子节点的指针 以及一个前缀值（可以被视为是所有子节点都满足的谓词） 叶节点存储实际数据的指针以及数据的值 搜索过程仍然以 consistent 函数为基础 即递归判断节点的 prefix 是否满足搜索条件
  - 基于四叉树：四叉树递归划分二维平面 内部节点的 prefix 即为四叉树的区域中心 叶节点则存储实际数据 可以是链表
  - 基于 kd 树：kd 树递归划分多维空间 prefix 是 kd 树的划分线坐标 叶节点存储实际数据
  - 基数树：基数树用于对字符串进行索引 prefix 是字符串的前缀 叶节点存储实际数据
  - SP 索引不支持排序以及唯一性约束 不去不支持在多个列上建立

#### CODE

- 空间查询：https://postgis.net/docs/manual-3.5/using_postgis_query.html#using-query-indexes
- PG 的空间数据类型：https://postgis.net/docs/manual-3.5/using_postgis_dbmanagement.html#PostGIS_Geometry
- https://postgis.net/workshops/postgis-intro/3d.html
- 学会如何创建索引并进行查询了 其中 3d 空间查询需要使用 intersects 函数等来模拟 另外 没法使用 gevel 来查看索引表数据 不知道如何验证八叉树

### 论文研读

#### 时空索引

- 结合时空关键字的轨迹范围查询混合索引结构：存储轨迹时 额外存储轨迹的文本信息 比如轨迹经过了 hospital、school 等地点 然后再关键字上做倒排索引 加快查询
- 除了使用 Z 曲线的 Geohash 以外 还有一个使用希尔伯特曲线的 Google S2 其先把球面投影到外切正方体上 然后在正方体的六个面上做希尔伯特曲线编码 另外 似乎普遍认为希尔伯特的空间聚类效果更好
- 基于 LSM-OCTree 的时空流分布式调度和存储方案：结合 LSM 和八叉树的时空索引 解决时空数据流的索引更新和查询问题 大致思路是先用八叉树预处理空间 LSM 树中一个数据块代表了一个或多个邻近的八叉树节点 合并时利用八叉树的空间性质进行合并 加快效率 查询时 由于内存中的 memtable 存放的是近期数据的八叉树 因此对于短时间的查询有较高的效率（相当于直接在内存的八叉树里进行查询）结论中提到 索引的更新效率大于普通八叉树索引（没有和 HBase 的时空索引对比 我觉得是比不过的） 查询效率比 HBase 的时空索引方案提升百分之二十左右 但是是在短时间范围内的查询 怀疑就是在内存中查询的 涉及到磁盘中 sst 的效率估计不如时空索引 总的来说没什么用 这个结合 LSM 和 OCTree 的思路
- 31

## 工作内容

### 组会汇报

- 两种索引的原理和结构
- 进行了简单的测试 POINTZ 可以存储 建立索引并查询 但是由于 gevel 插件过老 暂时没法查看索引表的数据结构 无法确定 3d 空间数据的索引就是八叉树
- 假如真的确定了八叉树索引 如何利用这个索引对冲突解决进行优化

### TODO

#### 优先

- geomesa 实现 3d+时间的索引和查询 把接口写通用一点 方便切换其他数据库
- 找一找 3d 带时间的数据集
- 我需要实现一个索引层 输入用户的 query 会生成一个编码 用于范围查询 可以再理解一下原理 这个编码是如何实现加速范围查询的 编码格式需要统一定下来 方便 zhd 进行物化视图的实现 放到 pharos 上 就是在存储历史数据的数据库上写一个较通用的索引层和查询接口（通用是因为可能需要用到自己设计的数据库）
- 查论文 谷歌学术 图书馆网站的电子数据库 VLDB ICDE SIGMOD ICDM EDBT 等会议 一是空间索引相关 二是查询优化 queryplan
- 清华李国良 ai 优化查询计划
- problem 冲突的设备处于不同区域时 需要共享数据 让节点 push 转发逻辑如何解决？首先要解决谁来计算冲突解决
- 先实现基于 PG 的索引层 测试三种索引的性能 看一下八叉树索引如何使用 大概包括：首先确定如何存储 以及数据库架构如何分布（即存哪些字段 以及数据库部署在哪里）并且考虑如何按时间分表（时间数据可以不建立索引 可以对比性能） 然后根据需求写查询接口（历史数据分析需要什么接口就写什么）
- 另外 时延比较差 可能需要优化

#### 一些可以 fix 的小细节

- realtime_map 中接收消息时的 time 和消息中的 time 有延迟 差不多是 2s 不知道为什么
- 节点刚好处于区域边界时可能会有点问题 暂时没做测试
- 断开连接时如何告知节点 目前是心跳计时实现的 考虑是否要更加实时
- 如果只是发给每个单独的设备 似乎没有必要汇聚到中心？以后如果 push 给应用 一个应用可能需要多个设备的数据 比如一个应用商只负责 taxi 的数据
- 目前数据缺少方向 因此先以设备为中心划分矩形范围 后续加入方向后再进行修改
- jedis 中取出的 Document 类中的 properties 不知道为什么是$={}的格式 导致必须要解析 string
- B 与 A 的通信需要发送独占空间 数据比较大 除了让 zhd 进一步压缩以外也可以使用 avro 进行压缩
- java 部署太慢了 看一下如何改仓库源
- 合并大圈小圈数据很 sb 看看能不能把 B 的小圈数据格式改一下
- java 这边的 avro 由于我使用了 fastjson 导致需要先解析为 jsonobject 再进行使用 需要手动分类序列化 未来可以考虑直接使用 avro 的 record 类
- 当数据较多时 会超时（正常） 当数据很多时 space-manage 会挂掉 报错如下：![alt text](image.png) 看起来是 mqtt 的问题 但是 mqtt 没报错
- avro 的压缩 如何自动生成 schema 文件？如何定义统一的 realtime_map 的 schema？（环境变量）

#### 性能需求

- 现在是中心主动 push 瓶颈可能在两个地方 一个是每 1s 计算态势数据来不来得及（主要是节点的强化学习） 另一个是中心需要并发发送态势数据给各个设备
- 一种是加快计算频率 看看到多少会出现问题
- 另一种是增加订阅设备的数量 看看到多少会出现问题

#### 代码整合

- 三个部分的代码 CEDS 八叉树空间 小圈计算 称为 A B C
- 节点收到请求开始计算：
  - 大圈
    1. 在 A 的 situ_cal 中 取出 redis 所有的设备
    2. 对于每个设备 使用 redisearch 的范围查询得到范围内的设备 即大圈数据
  - 小圈
    1. 在 A 的 situ_cal 中 直接给 B 通过 mqtt 发送异步请求 topic 为 `situation/send/{timeta 3. B 得到 C 返回的小圈后 将小圈编码为八叉树格式 然后根据存储的每个设备的特性（优先级等）以及形状进行冲突的解决 最后返回以八叉树格式表示的小圈数据给 A topic 为 situation/receive/{timestamp}on/receive/{timestamp}
- 只需要改为使用 redis 然后使用 mqtt 即可 计算独占空间的小圈只需要每个设备的位置数据 然后使用元数据计算 独占空间的大圈则需要发给 C

### 项目架构

#### 并发处理

- 节点计算态势数据用了线程池进行并发
- 向邻居节点发送请求也可以并发处理 不过由于 http 有长连接机制 请求应该不会重新建立连接 且一个设备的计算最多也就发送 3 个请求（大多数情况是 1 个） 所以并发处理的意义不大
- 中心对于消息的处理使用线程池并发
  - 本来的思路是使用 TPE（ThreadPoolExecutor）类初始化线程池 然后用一个 ConcurrentHashMap 来存储每个请求的线程对象 请求的第一个消息会被线程池随机分配一个线程对象 后续的消息取出 map 里的线程对象即可保证同一个请求的消息被同一个线程处理 但是我不知道如何使用线程池指定某一个线程处理...
  - 现在的实现比较简单 就是一个 List<ExecutorService> 里面存了 n 个单线程池 每次请求根据时间戳取模分配一个线程池处理 不知道可不可以保证均匀
  - 中心并发的意义在于高并发请求 对于串行请求无提升
  - **改为 push 后 这部分就没什么意义了**
- **中心发送态势数据给各个设备**：现在是 for 循环+publish 不知道 mqtt 的 publish 是不是异步的 可能需要多线程或者至少保证异步发送

#### 总体架构

##### 物联网设备

- sender.py: 模拟设备发送数据 通过 mqtt 向边缘节点实时发送数据

##### 边缘节点

- realtime_map.py: 在内存中维护两个结构 `realtime_map`（设备的实时数据）和 `online_clients`（在线设备列表）其中 realtime_map 存储在 redis 中 方便使用 `redisearch` 进行地理空间查询
  - 节点收到设备的数据后更新 `realtime_map` 与 `online_clients` 同时上传 realtime_index 给中心
  - 另起一个子线程判断设备是否离线 标准是每隔 2s 进行检查 如果上一次收到数据的时间距离现在超过 2s 就认为设备离线
  - 使用 flask 提供接口`/query` 用于其他边缘节点进行态势数据计算时 来查询范围内的设备数据
- situation_calculation.py: 计算态势数据的模块 连接到中心节点的 mqtt 并订阅`/situation/{node_id}`的 topic 接受到中心发来的消息（payload 为 timestamp）后开始计算态势数据 步骤为
  1. 从本节点的 redis 和邻近节点的 `/query` 接口中获取需要的实时数据 其中使用了 `shapely` 库来判断大圈与邻居是否相交
  2. 节点把每个设备 realtimeData 中的时间戳作为态势数据的时间戳放入 用于中心节点的合并去重
  3. 用这些数据计算每个设备的态势数据 使用线程池进行多线程并发提高性能
  4. 封装好最终的态势数据 放入中心消息队列的`/situation/center/{timestamp}`的 topic 中 消息格式如下：
     ```
     {
         "nodeId": 1,
         "taxi/0001": {
             "TIMESTAMP": 1372662000, # 用于合并去重
             "BIGCIRCLE": {...},      # 其它设备的态势数据
             "SMALLCIRCLE": {         # 独占空间
                  "smallArea": ... ,
                  "largeArea": ... ,
              },
          },
         ...
     }
     ```
     ```
      {
          "taxi/0001": {
              "TIMESTAMP": 1372662000, # 用于合并去重
              "BIGCIRCLE": {...},      # 其它设备的态势数据
          },
          ...
      }
     ```
- process_manager.py：进程管理器 负责管理数据采集进程的生命周期
- data_handler.py: 数据采集进程 定期存储数据到 mongodb

##### 中心

- 中心会在内存中维护一个名为 `situationData` 的结构 其包含两部分数据 一个是 map 存储态势数据 其结构类似于：
  ```
  {
      "timestamp": {                            # 发起这次态势数据计算时的时间戳
          "nodes": {                            # 所有节点是否已经收到消息
              1: False,
              2: False,
              ...
              12: False,
          },
          "data": {                             # 节点计算的态势数据
              "TAXI_ID": {...},
              "DRONE_ID": {...},
          },
          "cnt": 12,                            # 还未收到消息的节点数
          "done": False,                        # 是否已经完成
      },
      ...
  }
  ```
  另一个是 `devices` 维护了哪些设备订阅态势数据 记录他们的设备 id 以及这个设备的 region 参数
- 中心每隔 1s 进行一次态势数据的计算 先向 situationData 中插入新的键值对 随后向中心的消息队列的 `situation/{nodeID}` topic 中放入 payload 为`timestamp`的消息 告知各个节点开始计算
- 中心收到节点返回的消息后立马进行合并 根据时间戳判断最新态势数据 如果 `nodes` 中已经收到过此节点的数据 则说明消息重复了 不处理 否则置为 TRUE 然后 cnt--
- 当发现所有 node 都已合并（即 cnt==0）则将 `done` 字段置为 TRUE 并遍历 devices 将态势数据发送到`situation/app/{deviceID}` topic 中 等待设备自取
- scheduler 类中还设置了两个定时任务
  - checktimeout 任务会定期查看每个请求的时间戳是否已经过时 2s 如果过时 2s 就删除 这个主要是为了防止因为某一个节点挂掉而导致一个未完成的态势数据一直存在内存里的情况
  - persistData 任务定期将已经完成的请求的态势数据持久化到数据库中 以便后续进行数据分析 这里不做处理直接作为文档存入 MongoDB 中（测试时怕数据溢出 所以每 1 分钟就把这个表 drop 掉）
