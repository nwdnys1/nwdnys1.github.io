---
title: SJTU-REINS
date: 2024-10-05 21:19:00
categories: REINS
tags:
  - 实习
  - 开发
  - 架构
  - 运维
  - 测试
index_img:
banner_img:
excerpt: "大三顺利进入REINS打工 记录一下项目经历和经验"
---

# PHAROS

灯塔项目是一个类似于海上管理船只位置的云系统的项目 只不过空间由海域变为上海市区 对象由船只变为无人机等移动物联网设备

简单来说 灯塔系统会收集处于三维空间内各个个体的数据 并存储在一个云边融合系统（CEDS）内 之后将数据整理封装为态势数据 将态势数据分发回给每个个体 让个体通过实时的态势数据进行任务的规划和调度 态势数据会分为两部分 一部分是以个体为中心 在单位时间内可以安全独占的空间范围 另一部分是以个体为中心 依据任务需要而划分的一块空间范围内所有其它个体的数据

灯塔的核心思想是流控分离 即中心会分发数据流给个体设备 但是不会干涉个体设备的任务规划和调度 只是给出了安全空间的限制 个体设备会根据态势数据自行调整任务的路径 和现有的大部分统筹任务路线规划是不同的

## CEDS

CEDS 即黄子昂学长的硕士毕业论文中提出的云边融合存储系统 利用了边缘节点本身少量的数据存储与计算能力以及其分布于移动物联网设备附近的特点 实现了基于数据的就近存储、查询任务的划分下推以及负载感知的数据迁移功能的存储系统 做到了传输带宽的减少以及查询的加速 具体实现可以见其论文 大概包含以下几个部分：

### 边缘数据就近存储机制

大部分云边融合系统会将数据存储于资源富足可扩展的云端数据库中 但这会带来很多不必要的传输开销 利用边缘节点本身的存储能力 可以将数据就近存储 大大减少传输延迟

然而数据的就近存储会带来其他问题 比如原本全都位于云端中心的数据 由于就近存储就会分布在不同的边缘节点上 针对此问题 CEDS 提出一种基于 Rosetta 过滤器的全局索引机制 简单来讲 这个过滤器能够实现一个数据表内任一字段的范围过滤 即给定一个字段的某一范围 在常数时间内返回这个表是否存在这个范围内的数据 基于此种过滤器 加上基于时间划分的数据分表 可以做到在很短的时间内定位到某一个时间范围内的数据分表 并快速判断哪些分表内存在本次范围查询相关的数据 这就是 CEDS 的全局索引机制 基于此种索引 CEDS 在云端仅需存储数据表的元信息摘要（包括过滤器的 bitarray） 同时这也将为后文的查询下推提供便利

### 查询任务拆分下推

大部分云边存储系统基于边缘节点的存储数据 每次查询会将相关的数据表全部聚合到云端后进行筛选 产生了很多不必要的传输开销 CEDS 基于任务划分 将查询和聚合任务下推给各个相关的边缘子节点 减少带宽开销

具体而言 一次查询会先向云端中心索要本次查询可能涉及到的所有数据分表 以及这些分表存储在哪些边缘节点上 其方法可以简单的由之前提到的过滤器实现

获取到节点列表后 只需要把子查询任务分发给这些节点即可 每个子节点会进行相关的范围查询（节点内存在常规的索引来加快查询）并把查询结果返回 最终在查询节点进行数据聚合任务 把结果返回给用户

### 负载感知数据迁移

数据就近存储定会存在不均衡现象 由于查询任务划分给子节点并发执行 查询的延迟显然由查询时间最大的子查询决定 如果某些节点上的数据查询次数特别多 导致超出边缘节点的承受范围 导致子查询阻塞 就会导致所有涉及的查询延迟显著变高 为了防止此种情况出现 CEDS 会监测并记录每段时间内边缘节点的查询资源开销 比如记录前 3 分钟内某一节点的内存占用 一旦超过 1G 就判定存在热点数据 并将热点数据迁移到云端来减缓此节点的压力

此种策略是折中的策略 理论上可以达到最均衡的方案肯定是为每一个边缘节点根据查询开销动态分配性能资源 但这在现实中肯定不可行 因此使用较为折中的策略来解决数据不均衡分布的问题

## 技术调研

### Redisearch

- 项目使用了 redisearch 建立设备数据的地理索引 实现实时数据查询的加速 即把经纬度作为 GEO 类型的元素编写索引
- 具体实现
  - 文档参考：https://redis.io/docs/latest/develop/interact/search-and-query/indexing/（redis不同类型的索引如何使用）https://redis.readthedocs.io/en/stable/examples/search_json_examples.html#Projecting-using-JSON-Path-expressions（将 JSON 数据添加到索引的示例代码）
  - redis 实例需要使用 redis-stack 项目中使用 python 因此安装 pip 库的 redis 即可
  - 具体代码见项目中的`realtime_map.py`文件 只需要先创建 schema 描述索引的字段类型 然后添加索引即可

### Apache Avro

- 项目使用了 apache avro 进行数据的压缩 包括：
  - 设备传输数据给 realtime_map 与 data_receiver
  - 节点计算态势数据时向其他节点发送`/query`请求查询 reatime_map
  - 节点通过消息队列传输态势数据给中心
- 具体实现
  - 文档参考：https://avro.apache.org/docs/
  - python 需要安装 avro 库 java 则通过 maven 导入 avro 依赖
  - 需要预先编写好数据的 schema 语法见文档 编写好后即可使用 avro 的序列化与反序列化方法进行数据的压缩与解压
  - python 中数据类型比较自由 使用 dict 类型即可操作 但是 java 的数据类型比较严格 avro 本身提供一个 record 类封装数据 但是我使用了 fastjson 库的 json 类型来操作数据 这就导致必须要编写一个转换函数进行转换 并且这个转换需要递归分类进行 一旦类型对应不上 序列化时就会报错

### 时空索引

- 时空索引是一种将时空数据进行编码以便于快速查询的技术 我们可以先简单认为时空索引需要解决这样的查询：查询某一个时间段内某个空间范围内所有的数据
- 先从空间索引开始说起 目前主流的空间索引可以分为三种：哈希、树以及空间填充曲线
  - 哈希思想主要是通过网格哈希索引来实现的 也就是把已有的地理空间划分为网格 每个网格相当于一个哈希桶 哈希索引里就会存储每一个网格内有哪些数据 假如需要查询某一个范围内的所有数据 只需要先根据范围找到对应网格 然后再在哈希桶内找到对应数据即可 这个方案偏理论 实际工程不太用 对于灯塔项目 一个网格可能会有很多数据 尤其是不同时间段的数据 会导致哈希桶内数据量过大 失去意义
  - 基于树的索引包括四叉树、R 树等
    - 四叉树仍然是一种网格索引 但是它是一种递归的网格索引 不断四划分空间直到触发阈值为止 对于范围查询 四叉树需要递归的找到所有被包含的节点
    - R 树是 B 树在高维空间的扩展 其结构很简单 适合静态空间对象的索引
      - 先引入几个概念 BB 即 Bounding Box 是一个包含了空间对象的矩形 MBB 则是最小的 BB
      - R 树的非叶子节点存放(BB,ptr)的键值对 叶子节点则存放(MBB,ptr)的键值对 叶子节点的 ptr 直接指向了空间对象数据的位置 其可以是质点 也可以是几何对象
      - 需要进行范围查询时 R 树和四叉树一样 需要递归寻找被范围包含的节点
  - 基于降维的索引通常使用空间填充曲线来实现 GM 部分已经详细讲过了 简单概括就是使用递归二分把高维数据映射到 bytearray 然后 Geohash 会通过 base32 编码得到字符串
- 加入时间维度后 有三种主流的实现方式 即基于时间分片、基于空间分片以及时空维度同时进行索引
  - 基于时间分片是目前最主流的时空索引方式 比如 ES 使用空间填充曲线、PG 使用 R 树作为空间索引等 然后数据按照时间进行分片 个人认为如果灯塔的历史数据分析不需要跨很大的时间段 是可以使用这种方式的
  - 基于空间分片即把空间网格化后分表 由于查询需求 很少被实际应用
  - 时空同时进行索引 在 GM 中的 Z3 索引中有实现 大致可以理解为把时间也进行二分后和空间数据一起编码为 bytearray 在时空尺寸不匹配时会有严重的空间放大问题 详见京东 JUST 的论文
  - 另外 轨迹数据比较适合空间填充曲线 R 树等基本可以不考虑 空间填充曲线若使用 B+树 可能仍存在写入性能差的情况 但是很多非关系型数据库采用的是 LSM 树结构 写入性能会好很多
- 综上 要么基于时间分片 要么时空一起编码
  - 实际上 时空一起编码的索引 还是需要按时间分片 否则会有严重的空间放大问题
  - 既然如此 就变成决定时间参不参与编码了
  - 从感性上讲 我觉得时间参不参与不太影响查询性能 对于一个时空查询 其先会按照时间被分到不同分片上 中间的分片都是占满了时间段的 因此时间编码失去意义 只有两端的分片是可以用时间编码剪枝的 如果我们的查询时间范围总是大于一个分片的时间范围 那么时间参与编码意义是不大的
  - 另外还有一个查询是直接 scan 还是先递归到基本分辨率后再 scan 的问题 我认为直接 scan 即可 因为磁盘上的随机访问仍然需要扫过未查询的数据 只不过省去了判断数据是否符合条件的过程 而递归查询范围这个过程可能本身就比较慢 JUST 是直接 scan 的
- 参考：https://zhuanlan.zhihu.com/p/663029637

#### GeoMesa

- GM 本身是一个类似于索引引擎的工具 首先需要选取合适的存储系统 要么 Redis 要么 HBase 等数据库 或者也可以直接通过 FS 存储 但是性能较差
- 现有的 redisearch 只支持 2d 数据的复合索引 要引入高度的话 可能只能遍历筛选 取决于独占空间部分需不需要 3d point
- 我们使用 GM 的目的是为了加速大量历史数据的查询 进行对历史数据的分析 而对于数据的写入与实时性并无要求 那么有两种方案
  - 先考虑如何使用 mongodb 存储数据 然后通过 GM 进行查询
  - 一种是保留现有的 mongodb 数据库 也就是近期数据仍向 mongodb 写入 然后定期（比如 1 天）把数据迁移到适合存储大量时空数据的数据库中（比如 HBase） 并且这个迁移过程通过 GM 实现（比如把 mongo 数据批量读到内存里 在通过 GM 写入 HBase）也就自动创建了时空索引 后续查询也就提高了性能 （代码量会小一点）
  - 另一种则是抛弃 mongodb 直接通过 GM 持续写入数据 大致流程就是通过 mqtt 接受消息 把消息里的数据解析为 GM 的 simplefeature 类 然后通过 writer 写入 HBase 里 自动建立索引 省去了 mongodb 的部分 但是代码量会大一点
- 方便切换其他数据库指的是最后存储历史数据的数据库 索引层可以固定
- GM 现有的 API 都是大型分布式数据库 如果要测试 可以先部署两个容器试试
- GM 支持空间索引和时空索引 其中时空索引支持 2d point+时间 或是 2d 非 point（polygon line 等）+时间 其中 POINT 类型不知道是否支持 3d redis 由于本身不支持 3d 数据类型 所以没法存储 HBase 还没研究
- 原理部分
  - GM 如何写入数据？（以 HBase 为例）
    - 参考：https://zhuanlan.zhihu.com/p/164645879
    - 插入一个 feature 时 进行以下几步操作
      1. 预处理 假如数据没有 id 生成 uuid 然后为数据添加合适的属性 方便插入 HBase 表 为了防止数据倾斜 最终对于 id 进行哈希 放入对应分区
      2. 计算索引值 GM 会找到数据的 geo 属性和 date 属性 提取出数据 计算索引值 其中时间数据为了适应 GeoHash 机制 采用了 BinnedTime 机制 即相对于 1970 年 1 月 1 号形成有限的 chunk 接下来开始计算索引 geo 数据为 double 类型 dtg 则为 long 类型 先进行标准化等处理后 最终通过编码得到索引值
      3. 索引值会被编码为 byte 数组 写入 HBase 表中
      4. 最后数据被序列化 写入 HBase 表中 默认使用 kryo 序列化 也可以使用 avro
  - GM 如何进行数据的序列化？
    - 参考：https://zhuanlan.zhihu.com/p/164647326
    - 分两步 分别是序列化 feature 和 type 为什么需要序列化 type？因为插入数据时需要判断是否共用连接 GM 会把 type 序列化后存放在缓存里 方便后续反序列化出来进行判断 相当于一个全局的 schema
    - ![](https://image.blog.nwdnysl.site/20250224205422-99fe17067b124be578cba66601944766.png)
    - 具体序列化过程略 可以视为和 avro 一样根据 schema 压缩为二进制数据
  - GM 如何进行数据的索引？
    - 参考：https://zhuanlan.zhihu.com/p/164748160
    - Z2 索引：将二维空间编码为一维 以经纬度数据为例 通过空间填充曲线决定了数据的唯一顺序 也就映射到了一维整数上（这个过程相当于分别不断二分经纬度） 从而支持 1d 的键值索引 当需要进行 2d 的范围查询时 递归的寻找所有被区域包含的字符串 然后通过二分查找快速找到所有数据 或者对编码作主键建立 B+树索引
    - Z3 索引：将二维空间和时间编码到一维 时间按照 time period 切分 取每一段内的 offset 作为二分的依据 然后和空间数据一起编码为 bytearray 这种方法具有显著的空间放大问题 后续会讲到
    - XZ2 索引：将二维非点对象编码为一维 与 Z2 类似 通过找到空间对象的 mbb 来进行编码 相当于找到 Z2 编码中 mbb 的左下角方块的编码
    - XZ3 索引：将二维非点对象和时间编码为一维 类似

#### TrajMesa

- JUST 的前身 基于 GM 实现的分布式 NoSQL 轨迹查询引擎
- 工作可以分为三部分：预处理、建立索引以及查询
- 预处理
- 建立索引与存储
  - 论文提到 传统的垂直存储（也就是一个点存一行）对于轨迹数据来说不适合 包括查询一个轨迹比较慢、IO 次数多等问题 因此 TM 采用了水平存储 一行存一个轨迹 包括轨迹的元数据（mbr、时间范围等）、点 list（经压缩）、签名（一个 4\*4 的编码来描述轨迹的形状）、其他属性
  - TM 存在两份轨迹数据的副本表 分别是 IDTI 和 SRI 即 IDTQ 的索引表和 SRQ 的索引表
    - IDTI 表的 Key 为 `shard（随机数）+ 设备 id+BinNum（距离 RefTime 的第几个 Bin）+ EleCode（Bin 内的时间戳）+ 轨迹 id`
    - SRI 表基于 GM 的 XZ2 索引实现 Key 为 `shard + PosCode（与签名类似的 2\*2 编码 细化轨迹形状）+ XZ2（XZ2 产生的编码）+ 轨迹 id` 注意签名是基于 mbr 的 而 PosCode 是基于 XZ2 空间的
- 查询
  - 支持 4 种查询：某一个设备在一段时间内所有轨迹的查询（IDTQ）、空间范围查询（SRQ）、相似性查询（SQ）以及 knn 查询（KNNQ）
  - IDTQ：查询窗口=`可能的shard+设备 id+与时间范围相交的BinNum+每个Bin的偏移范围` 然后通过并发扫描 并删去不符合条件的轨迹
  - SRQ：查询窗口=`可能的shard+与空间范围相交的XZ2编码+每一个XZ2子空间对应的PosCode`
  - SQ 和 KNNQ 讲的比较晦涩 暂时没看懂

#### JUST

- 参考：https://zhuanlan.zhihu.com/p/300606530
- 基于 TM 作为底层存储的完整数据引擎系统 我们主要看其更新的时空索引部分
- Z2T 索引
  - Z3 索引用于时空数据的编码 然而存在空间放大问题 即索引中时间的粒度过大 会涉及到很多不必要的数据 为了解决这个问题 JUST 提出了 Z2T 索引
  - Z2T 索引很简单 就是把时间分片 索引变为`Num(T)+Z2` 其中 Num(T) 是时间分片的编号 查询时相当于先按时间范围剪枝 然后扫描 Z2 索引的 min-max 范围
- XZ2T 索引
  - XZ2T 索引用于非点对象的编码 与 Z2T 类似 只是把 Z2 换成了 XZ2
  - 我其实不懂上述索引和按时间分片有什么区别
- 除此之外 JUST 还实现了 SQL 查询和存储引擎 包括 Plugin Table（预定义 schema 的表）、View Table（缓存查询中间结果为 DF）

#### JUST-Traj

- 结合了 JUST 和 TrajMesa 的优点 实现了一个轨迹数据管理系统
- 索引
  - XZ2+索引：即 TM 里 SRI 的索引 XZ2 再加上一个 PosCode
  - XZT 索引：即 TM 里的 IDTI 索引 BinNum+EleCode
  - XZ2+T 索引：按时间分片后的 XZ2+索引

#### 其他时空引擎

- GeoMesa：支持各种分布式数据库以及本地 fs
  - 基本原理如上 是把高维数据使用 Z 曲线编码为字符串 然后查询时使用递归 支持 2d+时间的索引
  - 尝试过 cassandra 和 redis 查过文档 没找到支持 3d POINT 的方法 其 srid 也只支持 4326 也许默认只能用于经纬度
  - 文档：https://www.osgeo.cn/geomesa/index.html
- PostGIS：是 PostgreSQL 的一个插件 支持空间索引 存储 3d 空间数据 甚至支持 GeoServer 接入
  - 原理：有三种索引 GIST BRIN 和 SP-GiST 如果后续决定使用 可以做性能测试
    - GiST 即通用搜索树 支持各种数据类型 PG 实际上是在这个基础上实现了 R 树
    - BRIN 是一种轻量级索引类型 专为处理非常大的表而设计 它通过存储数据块范围（block range）的摘要信息 而不是每个数据行的索引值 从而显著减少索引的存储空间和维护成本 简单来说 有点像 LSM 树 存储了每个分块的最值 从而可以快速定位到某个范围内的数据
    - SP-GiST 是一种支持分区搜索树的通用索引方法 意为空间分区的 GIST 适用多维空间数据 通过四叉树、kd 树等递归分割数据
  - 文档中写到：坐标可以包含可选的 Z 和 M 坐标值。 Z 坐标通常用于表示高程。 M 坐标包含一个度量值，该值可以表示时间或距离。 如果几何图形值中存在 Z 或 M 值，则必须为几何图形中的每个点定义这些值。 如果几何图形具有 Z 或 M 坐标，则坐标尺寸为 3D; 如果它同时具有 Z 和 M，则坐标尺寸为 4D。
  - 也就是说可以存储 4d 的 POINT（x,y,z,m） 并支持上述三种索引 我简单试了一下 是可以存储 4d 的 如果要使用 就考虑把时间数据进行分片 肯定不能存整个时间戳
  - 文档：http://postgis.net/docs/manual-3.5/
- Big Query：是 Google 的一个云数据仓库 支持 SQL 查询 支持空间数据类型
  - 和 redisearch 有点像 支持存储 GEO 类型数据 并且进行空间查询 但是不支持时空索引
  - 另外 google 的数据库区域基本都在欧美 而且要付费 甚至 api 可能还需要代理 基本排除这个方案
  - 文档：https://cloud.google.com/bigquery/docs?hl=zh-cn
- Snowflake：是一个云数据仓库 支持 SQL 查询 和 Big Query 类似 支持 GEO 类型数据 以及一些空间查询 不支持时空索引 并且也需要付费 也可以排除
  - 文档：https://docs.snowflake.com/
- RedShift：亚马逊的云数据仓库 亚马逊云账户需要绑卡 因此我没有做测试
  - 看了下文档 和 PG 一样支持 ZM 的 POINT 类型
  - 文档：https://docs.aws.amazon.com/redshift/latest/dg/welcome.html
- GeoLake：基于湖仓的空间数据层 没找到相关文档 看 github 介绍 应该是支持 ZM 的空间类型数据 但是没有提及时间数据
- GeoWave：和 GM 类似的索引软件 支持分布式数据库和 fs 等 支持 3 维空间数据和时间 并且可以选择多种索引 基本上 GW 对于分布式键值数据库就像是 PG 对于 PostgreSQL
  - 试着用了一下 bbox 查询语法不支持 3 维 大概率不支持三维空间查询 不知道文档里说的支持 3 维存储和索引是什么意思
  - 文档中介绍空间索引时也举了 3 维+时间的例子 原理和 GM 一样 递归分解 代码语法基本也和 GM 一样
  - 文档：https://locationtech.github.io/geowave/overview.html
  - python 接口：https://locationtech.github.io/geowave/latest/pydocs/
- Elastic Search：分布式搜索引擎 查看了文档 支持 geo 类型的空间数据 但是仅限于 2d 经纬度数据 可以通过复合条件查询实现 3d+时间的范围查询 不知性能如何
  - 文档：https://www.elastic.co/docs
- 总结
  - GM 和 GW 的索引原理差不多 GW 声称支持 3d 空间数据 但是还未找到用法 如果找到了则可以直接使用 另外 Z3 索引存在空间放大问题 可能需要测试性能
  - PostGIS 支持 4d 空间数据 可以把时间数据分片后作为 M 维存储 但是需要进一步测试性能 GL 应该和 PG 差不多 但是前者显然更完善成熟
  - 几个云数据库基本排除 都只支持空间索引
  - ES 支持时空数据的查询 但是不清楚具体查询用到的索引如何
  - 如果都不支持 自己实现 无论时间参不参与编码 都需要先把时间分片 我觉得时间参与编码意义不大 因为时间字段是可以自然有序的

### PostGIS

- 主要任务是研究一下基于八叉树的 SP-GiST 索引

#### GiST

- 参考：https://habr.com/en/companies/postgrespro/articles/444742/
- GiST 和 B+树类似 区别在于 GiST 的可扩展性 B 树只支持大于小于等于的比较操作 而 G 可以支持相对位置运算符（R 树的左侧右侧等） 亦或是 RD 树的交集等运算符 在某种意义上 我们可以认为 G 是一种接口 是各种索引实现的一个基础框架
  - 结构：平衡树 所有叶节点的深度相等 每个节点代表一个集合区间（也就是一个谓词条件）且节点之间可以有交集 根节点是全集合区间 越往下集合区间越小 在搜索数据时使用 consistent 函数逐级作 dfs 搜索 将所有满足搜索条件的节点返回
  - 基于 R 树：R 树将平面拆分为多个矩形 一个节点代表一个矩形 叶节点代表空间对象 一致函数则是判断矩形是否相交 如果对空间数据进行 G 索引 则采用 R 树作为基础
  - 基于 RD 树：这是 PG 对于全文搜索采用的索引结构

#### SP-GiST

- 参考：https://habr.com/en/companies/postgrespro/articles/446624/
- 从名字可以看出 SP-GiST 是对于 G 的一种扩展 SP 指的是空间分区 SP 索引适用于任何值域空间可以递归划分为非相交区域的数据
  - 结构：非平衡树 因为每个节点的区域都是不相交的 每个内部节点存储子节点的指针 以及一个前缀值（可以被视为是所有子节点都满足的谓词） 叶节点存储实际数据的指针以及数据的值 搜索过程仍然以 consistent 函数为基础 即递归判断节点的 prefix 是否满足搜索条件
  - 基于四叉树：四叉树递归划分二维平面 内部节点的 prefix 即为四叉树的区域中心 叶节点则存储实际数据 可以是链表
  - 基于 kd 树：kd 树递归划分多维空间 prefix 是 kd 树的划分线坐标 叶节点存储实际数据
  - 基数树：基数树用于对字符串进行索引 prefix 是字符串的前缀 叶节点存储实际数据
  - SP 索引不支持排序以及唯一性约束 不去不支持在多个列上建立

#### CODE

- 空间查询：https://postgis.net/docs/manual-3.5/using_postgis_query.html#using-query-indexes
- PG 的空间数据类型：https://postgis.net/docs/manual-3.5/using_postgis_dbmanagement.html#PostGIS_Geometry
- https://postgis.net/workshops/postgis-intro/3d.html
- 学会如何创建索引并进行查询了 其中 3d 空间查询需要使用 intersects 函数等来模拟 另外 没法使用 gevel 来查看索引表数据 不知道如何验证八叉树

### 论文研读

#### 时空索引

- 结合时空关键字的轨迹范围查询混合索引结构：存储轨迹时 额外存储轨迹的文本信息 比如轨迹经过了 hospital、school 等地点 然后在关键字上做倒排索引 加快查询
- 除了使用 Z 曲线的 Geohash 以外 还有使用希尔伯特曲线的 Google S2 其先把球面投影到外切正方体上 然后在正方体的六个面上做希尔伯特曲线编码 S2 每一级之间的大小变化比 GH 要平缓许多 另外 似乎普遍认为希尔伯特的空间聚类效果更好
- 基于 LSM-OCTree 的时空流分布式调度和存储方案
  - 背景：时空数据流的索引更新和查询问题
  - 大致思路是先用八叉树预处理空间 LSM 树中一个数据块代表了一个或多个邻近的八叉树节点 合并时利用八叉树的空间性质进行合并 加快效率 查询时 由于内存中的 memtable 存放的是近期数据的八叉树 因此对于短时间的查询有较高的效率（相当于直接在内存的八叉树里进行查询）结论中提到 索引的更新效率大于普通八叉树索引（没有和 HBase 的时空索引对比 我觉得是比不过的） 查询效率比 HBase 的时空索引方案提升百分之二十左右 但是是在短时间范围内的查询 怀疑就是在内存中查询的 涉及到磁盘中 sst 的效率估计不如时空索引 总的来说没什么用 这个结合 LSM 和 OCTree 的思路
- 基于轨迹大数据时空分布的索引与查询方法
  - 背景：基于空间等分的空间索引通常会遇到数据分布不均匀的问题 比如 GH 将查询拆分为子查询 其中因为数据不均 子查询的耗时各不相同（不利于并发？另外我觉得这里的子查询的立方体是等大小的 而非像 GM 那样匹配到前缀就停止 所以后面才能解决数据不均的问题 要不然每一次查询使用的立方体实际上都是不一样的）并且文章提到 当查询空间远大于单位立方体时 子查询数量过多而影响查询性能
  - 为此作者提出一种基于历史数据分布规律进行合并分区的方法 具体而言 先导入合适的历史数据 设定一个阈值 按照 Z 曲线的编码顺序遍历每一个立方体 合并直至数据量大于阈值 然后进行下一个立方体的合并 然后建立从 GH 编码到分区编号的倒排索引 存储点数据时 分区编号和 GH 编码一起作为行键一部分 以论文的示例来说 查询时可以将子查询数量变少（因为某些区域合并）并且每个子查询的耗时差不多
  - 实验结论：随着阈值变大 查询耗时减小直至稳定 另外 建立索引分区之后 插入未来数据后 查询耗时基本不变 说明在相同时间尺度下 数据分布的不均匀特性是相似的（也就是每天轨迹数据分布相似）
  - 作者提到两个问题 一是 GH 固定查询分区大小 导致当空间较大时子查询数量过多 这一点 GM 通过前缀匹配解决 二是数据分布不均匀 这篇文章的方法应该是不适用于 GM 的查询逻辑的 因为每次查询的子查询大小都是不一样的
  - 虽然文章介绍的方法不适用 但让我想到一个思路：无人机的轨迹数据在每个时间段内的分布可能也是相似的 因此做历史数据分析时 用户可能也会倾向于频繁查询某一些特定空间范围的数据 虽然数据会随着时间变化 但是这个查询空间所生成的子查询窗口（也就是一维编码的数组）应该是固定的 如果维护一个表 记录次数最频繁的查询空间范围 就可以不用反复分解空间范围生成查询窗口了 这个类似于缓存的表也可以放在内存里加快效率
- 学习索引
  - Kraska 等人提出 使用神经网络代替传统数据结构构建索引 大致思路是让模型学习数据分布的规律 从而预测数据的位置 直接访问即可
  - RMI：递归模型索引 数据集被划分为多个子集 组织为树形结构 根模型是路由模型 将查询分配到子节点 而叶模型是最终的数据预测模型 训练需要大量时间资源 并且更新写入需要重新训练模型
  - FITing-Tree：用线性模型拟合底层数据分布 将 B 树索引的叶节点从数据改为模型参数
  - ALEX：支持高效插入数据
  - 其余：https://zhuanlan.zhihu.com/p/625142215
  - 参考：https://zhuanlan.zhihu.com/p/649563211
- 基于改进的 K-means 聚类分区均匀化空间学习索引
  - 背景：提出适用于空间数据的学习索引方法
  - 学习索引需要数据降维 作者注意到使用 Z 曲线降维后的空间数据在一维的分布总是可以被划分为多个近似均匀分布的区间 因此先对数据进行聚类 划分为各个线性区间 然后用线性回归进行拟合
  - 范围查询即通过模型找到上下限位置 然后扫描并二次过滤
  - 结论：使用分段线性函数替代神经网络拟合数据分布 结构简单且查询效率高
- Efficient Cost Modeling of Space-filling Curves
  - 背景：SFC 用于编码空间数据 但是现有的某一种特定的 SFC 往往在查询性能上不理想 希望基于成本函数以及数据分布特征来选择合适的 SFC 但是一般计算的开销较大
  - 作者提出了基于强化学习选择 sfc 的方法 其提出的成本算法是 On 的 计算时间快 并且定义一种特殊的 SFC 称为 BMC 可以生成不同的曲线类型 最终实验表明学习效率与选择出的 sfc 的查询性能都优于现有方法
- https://ieeexplore.ieee.org/document/10598151
  - 背景：缺少合适的云存储服务来存储时空数据 因此作者实现了一种基于云服务的轨迹数据管理系统
  - 存储架构：包括内存层 磁盘层 以及对象存储层（也就是云服务层）内存层中存放了所有数据的索引以及部分缓存块 实际数据则根据热度存放在磁盘层或对象存储层
  - 存储结构
    - 轨迹数据具有时空连续性 因此将轨迹数据基于时间进行 chunk 的划分 这同时使得连续的 chunk 往往在空间上也有局部性 后面会提到这个局部性是怎样被利用来提高查询性能的
    - 具体而言 一个 oid 对应一个类似于链表的 chunk 列表 其中包含一个头部 chunk 和剩余的不可变 chunk 头块是这个设备最新的轨迹数据 不可变块则是曾经的头块 所有 chunk 都有一个逻辑指针指向下一个 chunk 它们具有轨迹上的连续性 之所以是逻辑指针而非物理指针 是因为 chunk 可能被存储在不同的地方 因此作者实现另一个存储管理器来找到逻辑指针对应的物理存储位置 相当于一个映射表
    - 索引会完全存储在内存中 除此之外 头块以及部分最新的不可变块也会存储在内存中 以提高查询性能
  - 索引设计
    - head chunk
      - 头块包含了 oid 时间范围等元信息 一旦头块转换为不可变块 其索引会被删除
      - 头块的索引是一个空间网格索引 每一个网格维护了一个 postings list 其记录了哪些 oid 的头块轨迹数据落在这个网格内
      - 由于空间不均 列表的大小会有所不同 作者提出一个分裂方法 我暂时没有细看
    - immutable chunk
      - 不可变块的元数据包括时间范围 mbb oid 与 chunkid 指针
      - 不可变块的索引是一个 b 树变种 键是时间范围 一个叶节点存储了时间范围内的多个 chunk 索引还会维护一个指向最新叶节点的指针 从而可以快速插入下一个索引
      - 为了解决轨迹数据只占 mbb 的一部分而导致的假阳性的问题 采用一个空间位图来描述轨迹的形状
    - 索引全部位于内存中是这个系统可以高效进行查询的关键 作者进行了计算 得出 64GB 内存可以存储 16TB 的轨迹数据的索引
  - 查询过程
    - ID-T 查询：查询某个设备在一段时间内的所有轨迹数据
      - 判断头块是否符合时间范围
      - 访问不可变块的索引树 二分找到时间范围的下限叶节点 然后顺序遍历到第一个符合 oid 的节点
      - 从这个 chunkid 开始 沿着指针遍历 直到某个节点不再符合时间范围
      - 合并 chunkid 到实际存储中取出数据
    - ST-查询：查询某个时空范围内的轨迹数据
      - 访问头块的网格索引 找到所有符合空间范围的网格 取出 oids
      - 根据 oids 找到头块 根据头块的元信息进一步判断是否符合空间范围 取出所有符合的数据
      - 访问不可变块的索引树 先根据时间范围找到下限叶节点 然后顺序遍历叶节点
      - 遍历时通过空间范围和空间位图进一步判断是否符合条件 取出所有 chunkid 到实际存储中取出数据
  - 其它优化
    - 不可变块在超出阈值后会被持久化到磁盘 在存储持久化之前 会先将所有块按照 oid 和空间信息进行排序 这使得这一部分的 chunk 中 同一个 oid 的数据是连续的 保证了局部性
    - 访问磁盘的多个 chunk 时 管理器会利用 chunk 的局部性 也即会尽可能将多个 chunk 的随机访问变为一个顺序访问
    - 数据迁移基于 LRU 以及存储成本模型
- https://ieeexplore.ieee.org/document/10597771/
  - 基于键值存储的轨迹数据管理系统
- https://ieeexplore.ieee.org/document/10598038/

#### 查询优化

- 基于机器学习的数据库技术综述

  - 论文介绍了机器学习在数据库中的应用 包括八个方面：数据库运维、​ 数据存储、​ 优化器与执行器、​ 查询优化、​ 负载管理、​ 安全与隐私、​ 自管理、​ 支持 ML 的数据库
  - 我主要关注查询优化这一块 包括 SQL 重写 索引推荐和自然语言查询
    - SQL 重写包括外部和内部重写 外部重写一般由人工完成 将 SQL 语句进行一定原则的重写 比如避免全表扫描操作 内部重写则是在优化器内完成的 对 SQL 树进行修改和替换
    - 索引推荐：常用的索引都是较通用的数据结构 没有对数据分布进行利用 也没有使用深度学习模型等
    - 视图推荐：从查询集合中提取高频的子查询 并将其存储为视图以提升查询性能 一般分为两步 先从不同语句中识别等价的子查询作为候选视图 然后对候选视图进行评估 选择合适的视图
  - 另外 广义的查询优化还包含了数据库内部的优化器和执行器
    - 基数估计：指的是估计一个查询结果的大小 用于估计查询的成本
    - 计划选择：计划选择器会生成不同的连接计划 计划数量对于表的数量是指数级的 因此找到一个最优计划是 NP 难问题 这一步可以使用机器学习来进行优化
    - 分布式协同：在集群上 数据划分和任务调度的均匀尤为重要 根据节点状态进行自动的划分是一个重要的研究方向
  - 基数估计
    - 数据库代价估计分为两部分：基数估计和代价模型 前者即使是在现在的数据库中仍存在较大误差
    - 传统基数估计方法
      - 直方图：将数据划分为多个桶 统计每个桶的大小 通过桶的大小来估计基数
      - 数据画像
      - 基于采样的方法
    - 机器学习方法
      - 面向查询的基数估计：比如使用 RNN 学习数据表、查询条件与连接条件之间的关系 预估数据库可能存在的基数
      - 面向执行计划的基数估计：使用图神经网络来学习执行计划的特征 进而估计基数？
  - 查询计划选择
    - 静态计划选择方法：使用确定的代价模型来选择最优的执行计划
      - 基于 DP 的方法：类似于 dp 计算矩阵乘法的代价 虽然去掉了冗余计算 但是仍在指数级的状态空间内进行搜索
      - 基于启发式的方法：基于启发式算法生成随机的执行计划 但是没有保证找到最优解
    - 机器学习方法：静态计划依赖于估计器的优劣 换言之 估计器评估出的最优解在实际执行情况下可能并不是最优的
      - 基于估计器的自适应性方法：这类方法认为计划的优劣由估计器限制 因此在执行计划时动态的更新估计器的参数 使得估计器的评估结果更接近真实值
      - 基于 RL 的方法：将 Join 条件作为动作空间 状态空间由所有连接树组成 执行的代价作为奖励函数 求解策略函数以最小化长时奖励 即最小化执行代价
  - 索引自动推荐
    - 索引生成：即学习型索引
      - 范围索引：使用神经网络来学习数据分布的规律 直接预测数据的位置
      - 点索引：哈希索引
      - 布隆过滤器：可以是学习一个二元分类器来判断数据是否存在 也可以是学习一个哈希函数使得键和非键的冲突率最小化
    - 索引选择：即索引推荐
      - 在线方法：工作负载分析 索引方案选择 索引方案实现
      - 离线方法：单查询优化 工作负载优化
      - 半自动化索引调整：结合在线和离线方法
      - 机器学习方法：ITLS 将学习分类器应用在索引推荐上 通过遗传算法来选择索引
  - 物化视图选择
    - 识别等价子查询：包括基于符号和基于逻辑语义的判断
    - 视图选择：基于查询的频率和代价来选择 包括基于 DAG 和基于 ILP 的方法
  - 基于 NLP 的查询技术：比如 LLM 用于自然语言查询

- 轩辕:AI 原生数据库系统
  - AI 建议型 DB
    - 负载管理
    - 查询优化：SQL 重写 索引推荐 视图推荐 分区策略推荐
    - 数据库监控
    - 数据库安全
  - AI 辅助型 DB
    - 自配置工作负载与参数
    - 自优化：优化代价模型 基数估计 计划选择
    - 自监控
    - 自诊断
    - 自愈
    - 自安全
  - AI 增强型 DB
    - 配置优化
    - 基数估计
    - 索引推荐
    - 连接顺序选择
    - 端到端查询优化
  - AI 自组装型 DB
    - 数据库自组装
    - 异构计算架构
  - AI 自设计型 DB

## 工作内容

### 组会汇报

- 由于索引层用 java 实现 没法和 python 进程共享锁 有几个解决方法：
  1. 把 datahandler 改为 java 实现 逻辑不用变 因为上锁所以不会丢失任何数据
  2. 数据缓存在 redis 索引层会定期 flush flush 时不能直接删除所有键 需要使用 arrtrim 操作删除一定长度的键（性能未知 肯定比直接删 key 要慢） 需要定时删除过期 slot 键（因为 flush 不会删键 这个定时是指 slot 过期 也就是现在的 20min）因此极小概率下也会丢失过期数据 √
     - 初步试了一下 写入 100 多个数据 加上删除的操作 大概在 300ms 左右 因为是异步写入 因此只要保证写入比设备数据产生的快就行了
  3. 按 slot 分割 已经过期的 slot 会被 flush buffer 中缓存当前 slot 的数据 会比较占用内存 而且 flush 时间会和 slot 划分时间绑定 其中极小概率会出现并发问题导致数据丢失（只有这个数据是过期的才会发生）
  4. 不用时间分片 那么存储层只需要插入数据即可 flush 时一个个删除 但是个人认为时间分片还是需要的 因为 Z3 索引存在空间放大问题 不知道现在的 GM 有没有解决
- 简单对 st 查询进行了测试 在服务器上测 查一个小格子的空间 时间取 20min 内的几个范围 基本上都在 300ms 以内 取空间范围为包含全部 查询更快
- 我也按照文档说明将 Z3 索引的时间间隔改为了 day 但是没有显著提升 https://www.osgeo.cn/geomesa/user/datastores/index_config.html
- 加入高度字段 随机生成数值 再进行范围查询 基本都在 500ms 以内（未添加索引）为高度添加 attr 索引后 没啥区别（这是符合预期的 因为 sth 查询没法利用到 attr 索引 理论上只能 scan 来筛选 后续可以加入多线程来应对高性能要求）
- 简单改写了一下代码 结合原本的查询下推逻辑 目前逻辑是遍历所有 timeslot 和所有 node 形成查询窗口 下发给各个节点 然后聚合上来 没有什么筛选逻辑 性能和查询单个节点差不多 500ms 左右 后续可以结合新的下推逻辑 并做一些性能的优化
- 加入多线程 scan 下面是性能对比
  - 经纬度 0.02x0.01 高度 40-50 前者约 300-400ms 后者差不多 数据量太小 拉不开差距
  - 经纬度全部 高度 20-90 还是维持在 200-400ms 之间 推测数据量极大（达到百兆级别）性能会有所差异
  - 好消息是 json 转换的过程几乎不耗时

### TODO

#### 优先

- 基于 GM 先写一个索引层 只需要 2d+时间索引即可 先基于点存储查询写 以后可以拓展到轨迹数据
  - 还需实现查询接口 需要结合之前的查询下推
  - 表名不可以带斜杠 需要考虑一下如何修改
  - 目前 GM 的 schema 是固定为 taxi 的 后续加入转换逻辑适用于不同设备 转换逻辑可以是启动时发送请求获取 schema 然后存在内存里待用
  - GM 的时间索引可能不如遍历筛选 可能需要测试
- 找一找 3d 带时间的数据集
- 查论文 谷歌学术 图书馆网站的电子数据库 VLDB ICDE SIGMOD ICDM EDBT 等会议 一是空间索引相关 二是查询优化 queryplan 清华李国良 ai 优化查询计划
- problem 冲突的设备处于不同区域时 需要共享数据 让节点 push 转发逻辑如何解决？首先要决定谁来计算冲突解决
- 另外 时延比较差 可能需要优化
- 后续可能要参与的工作
  - SQL 生成 类似的 灯塔的索引层也可以通过自然语言生成查询 最终返回结果 其中转换过程自己定 比如 NL 转换为时空具体范围 然后进行查询 或者转化为具体的查询窗口 另外比较重要的是 要根据数据的存储进行优化 这个是比较抽象的 具体方式可以用本地的 LLM
  - 发票的图神经网络 对于边敏感的神经网络可以用于识别行贿违法的 pattern 然后用模型学习并推理出哪些是异常的 另外还有个知识图谱
  - cy 有个联邦学习的论文要发 可以参与
    - Causal Discovery with Reinforcement Learning
      - 用 RL 来学习模型 发现数据之间的因果关系提取为 DAG
    - GANBLR: A Tabular Data Generation Mode
      - 表格数据生成模型 输入真实表格数据 模型会学习特征 生成相似的表格数据
    - 背景是联邦学习 不同客户端需要上传数据给服务器用于训练任务模型 但是数据是隐私的 需要加密
    - 之所以用贝叶斯模型处理表格数据 是因为 PCA 只能表达线性关系 而贝叶斯模型可以表达非线性关系 也即因果关系
    - 不同客户端之间的数据特征是不对齐的 因此需要对数据进行对齐 对齐就需要客户端上传贝叶斯模型来生成数据
    - 为了不涉密 就需要用 diffusion 模型来加密贝叶斯模型 让服务器解密为一个相似但是不同的贝叶斯模型
    - 之所以不用 diffusion 直接生成数据 一是过于简单可能会被破解 二是直接 diffuse 数据可能会破坏表格数据的因果关系 而贝叶斯模型可以保留这点
  - kfm 在做查询下推相关 我可以看看相关论文 结合到索引层

#### 一些可以 fix 的小细节

- realtime_map 中接收消息时的 time 和消息中的 time 有延迟 差不多是 2s 不知道为什么
- 节点刚好处于区域边界时可能会有点问题 暂时没做测试
- 断开连接时如何告知节点 目前是心跳计时实现的 考虑是否要更加实时
- 如果只是发给每个单独的设备 似乎没有必要汇聚到中心？以后如果 push 给应用 一个应用可能需要多个设备的数据 比如一个应用商只负责 taxi 的数据
- 目前数据缺少方向 因此先以设备为中心划分矩形范围 后续加入方向后再进行修改
- jedis 中取出的 Document 类中的 properties 不知道为什么是$={}的格式 导致必须要解析 string
- B 与 A 的通信需要发送独占空间 数据比较大 除了让 zhd 进一步压缩以外也可以使用 avro 进行压缩
- 合并大圈小圈数据很 sb 看看能不能把 B 的小圈数据格式改一下
- java 这边的 avro 由于我使用了 fastjson 导致需要先解析为 jsonobject 再进行使用 需要手动分类序列化 未来可以考虑直接使用 avro 的 record 类
- 当数据较多时 会超时（正常） 当数据很多时 space-manage 会挂掉 报错如下：![alt text](image.png) 看起来是 mqtt 的问题 但是 mqtt 没报错
- avro 的压缩 如何自动生成 schema 文件？如何定义统一的 realtime_map 的 schema？（环境变量）
- 节点内部的通信无需使用 mqtt 可以使用 redis 共享 比如 sm 和 ceds 之间的通信（但是除开共享以外 还需要通知）
- 现在态势数据没有 是因为 mergecircle 里会取大圈和小圈的交集 而现在小圈没有数据

#### 性能需求

- 现在是中心主动 push 瓶颈可能在两个地方 一个是每 1s 计算态势数据来不来得及（主要是节点的强化学习） 另一个是中心需要并发发送态势数据给各个设备
- 一种是加快计算频率 看看到多少会出现问题
- 另一种是增加订阅设备的数量 看看到多少会出现问题

#### 代码整合

- 三个部分的代码 CEDS 八叉树空间 小圈计算 称为 A B C
- 节点收到请求开始计算：
  - 大圈
    1. 在 A 的 situ_cal 中 取出 redis 所有的设备
    2. 对于每个设备 使用 redisearch 的范围查询得到范围内的设备 即大圈数据
  - 小圈
    1. 在 A 的 situ_cal 中 直接给 B 通过 mqtt 发送异步请求 topic 为 `situation/send/{timestamp} 3. B 得到 C 返回的小圈后 将小圈编码为八叉树格式 然后根据存储的每个设备的特性（优先级等）以及形状进行冲突的解决 最后返回以八叉树格式表示的小圈数据给 A topic 为 situation/receive/{timestamp}on/receive/{timestamp}
- 只需要改为使用 redis 然后使用 mqtt 即可 计算独占空间的小圈只需要每个设备的位置数据 然后使用元数据计算 独占空间的大圈则需要发给 C

### 项目架构

#### 并发处理

- 节点计算态势数据用了线程池进行并发
- 向邻居节点发送请求也可以并发处理 不过由于 http 有长连接机制 请求应该不会重新建立连接 且一个设备的计算最多也就发送 3 个请求（大多数情况是 1 个） 所以并发处理的意义不大
- 中心对于消息的处理使用线程池并发
  - 本来的思路是使用 TPE（ThreadPoolExecutor）类初始化线程池 然后用一个 ConcurrentHashMap 来存储每个请求的线程对象 请求的第一个消息会被线程池随机分配一个线程对象 后续的消息取出 map 里的线程对象即可保证同一个请求的消息被同一个线程处理 但是我不知道如何使用线程池指定某一个线程处理...
  - 现在的实现比较简单 就是一个 List<ExecutorService> 里面存了 n 个单线程池 每次请求根据时间戳取模分配一个线程池处理 不知道可不可以保证均匀
  - 中心并发的意义在于高并发请求 对于串行请求无提升
  - **改为 push 后 这部分就没什么意义了**
- **中心发送态势数据给各个设备**：现在是 for 循环+publish 不知道 mqtt 的 publish 是不是异步的 可能需要多线程或者至少保证异步发送

#### 总体架构

##### 物联网设备

- sender.py: 模拟设备发送数据 通过 mqtt 向边缘节点实时发送数据

##### 边缘节点

- realtime_map.py: 在内存中维护两个结构 `realtime_map`（设备的实时数据）和 `online_clients`（在线设备列表）其中 realtime_map 存储在 redis 中 方便使用 `redisearch` 进行地理空间查询
  - 节点收到设备的数据后更新 `realtime_map` 与 `online_clients` 同时上传 realtime_index 给中心
  - 另起一个子线程判断设备是否离线 标准是每隔 2s 进行检查 如果上一次收到数据的时间距离现在超过 2s 就认为设备离线
  - 使用 flask 提供接口`/query` 用于其他边缘节点进行态势数据计算时 来查询范围内的设备数据
- situation_calculation.py: 计算态势数据的模块 连接到中心节点的 mqtt 并订阅`/situation/{node_id}`的 topic 接受到中心发来的消息（payload 为 timestamp）后开始计算态势数据 步骤为
  1. 从本节点的 redis 和邻近节点的 `/query` 接口中获取需要的实时数据 其中使用了 `shapely` 库来判断大圈与邻居是否相交
  2. 节点把每个设备 realtimeData 中的时间戳作为态势数据的时间戳放入 用于中心节点的合并去重
  3. 用这些数据计算每个设备的态势数据 使用线程池进行多线程并发提高性能
  4. 封装好最终的态势数据 放入中心消息队列的`/situation/center/{timestamp}`的 topic 中 消息格式如下：
     ```
     {
         "nodeId": 1,
         "taxi/0001": {
             "TIMESTAMP": 1372662000, # 用于合并去重
             "BIGCIRCLE": {...},      # 其它设备的态势数据
             "SMALLCIRCLE": {         # 独占空间
                  "smallArea": ... ,
                  "largeArea": ... ,
              },
          },
         ...
     }
     ```
     ```
      {
          "taxi/0001": {
              "TIMESTAMP": 1372662000, # 用于合并去重
              "BIGCIRCLE": {...},      # 其它设备的态势数据
          },
          ...
      }
     ```
- process_manager.py：进程管理器 负责管理数据采集进程的生命周期
- data_handler.py: 数据采集进程 定期存储数据到 mongodb

##### 中心

- 中心会在内存中维护一个名为 `situationData` 的结构 其包含两部分数据 一个是 map 存储态势数据 其结构类似于：
  ```
  {
      "timestamp": {                            # 发起这次态势数据计算时的时间戳
          "nodes": {                            # 所有节点是否已经收到消息
              1: False,
              2: False,
              ...
              12: False,
          },
          "data": {                             # 节点计算的态势数据
              "TAXI_ID": {...},
              "DRONE_ID": {...},
          },
          "cnt": 12,                            # 还未收到消息的节点数
          "done": False,                        # 是否已经完成
      },
      ...
  }
  ```
  另一个是 `devices` 维护了哪些设备订阅态势数据 记录他们的设备 id 以及这个设备的 region 参数
- 中心每隔 1s 进行一次态势数据的计算 先向 situationData 中插入新的键值对 随后向中心的消息队列的 `situation/{nodeID}` topic 中放入 payload 为`timestamp`的消息 告知各个节点开始计算
- 中心收到节点返回的消息后立马进行合并 根据时间戳判断最新态势数据 如果 `nodes` 中已经收到过此节点的数据 则说明消息重复了 不处理 否则置为 TRUE 然后 cnt--
- 当发现所有 node 都已合并（即 cnt==0）则将 `done` 字段置为 TRUE 并遍历 devices 将态势数据发送到`situation/app/{deviceID}` topic 中 等待设备自取
- scheduler 类中还设置了两个定时任务
  - checktimeout 任务会定期查看每个请求的时间戳是否已经过时 2s 如果过时 2s 就删除 这个主要是为了防止因为某一个节点挂掉而导致一个未完成的态势数据一直存在内存里的情况
  - persistData 任务定期将已经完成的请求的态势数据持久化到数据库中 以便后续进行数据分析 这里不做处理直接作为文档存入 MongoDB 中（测试时怕数据溢出 所以每 1 分钟就把这个表 drop 掉）
