---
title: 从零开始深度学习
date: 2024-09-21 19:55:00
categories: AI
tags:
  - 深度学习
index_img:
banner_img:
excerpt: "闲来无事 跟着李沐大佬的教程来学习一下深度学习的知识 本文档做笔记用"
---

闲来无事 跟着李沐大佬的教程来学习一下深度学习的知识 本文档做笔记用

# CH3 安装环境

- 我有 NVIDIA 的独显 因此可以安装 CUDA 使用 GPU 来跑深度学习 去官网下载新版即可 注意如果不想装在系统盘 需要自定义安装
- 安装 Anaconda 或是 miniconda 我选择 Anaconda 可以参考这个：https://www.jianshu.com/p/eaee1fadc1e9

  注意如果安装了 python 版本为 3.12 的 anaconda 在安装 d2l 时会报错 我选择使用`conda create --name d2l python=3.9 -y`指令创建一个 3.9 版本 python 的环境来进行学习

- 安装 Pytorch 去官网下：https://pytorch.org/get-started/locally/
- 安装 jupyter 和 d2l 使用 pip 即可
- 下载提供的笔记本 本地运行 jupyter 进入某一个章节进行测试

## （可选）使用 VS Code+Jupyter 来快速浏览笔记本

每次都使用 anaconda 的 shell 运行 jupyter 非常不方便 可以直接下载 vscode 的 jupyter 扩展 然后选择内核为对应的 python（比如我用 3.9） 然后就可以在 vscode 里浏览笔记啦

# CH4 数据处理

- torch 提供和 numpy 类似的一系列数组操作 不过在这里数组其实是张量（tensor）

- os 库可以对文件进行操作 panda 库可以对 csv 文件进行读取插值等操作

- 使用 fillna 函数对数据进行插值 get_dummies 函数可以把 NaN 和其他值分开为两列

# CH8 线性回归+基础优化算法

## 线性回归

- 线性模型即输出=n 维输入的加权线性组合 可以看做是单层神经网络 即只有输入层和输出层的神经网络

- 损失函数是用于衡量实际值和预测值差距的 在回归问题中最常用的损失函数是平方损失 即方差

- 求解过程即最小化损失函数的过程 实际上就是最小二乘法

- 因为损失是凸函数 所以线性回归是具有显式解的

- 基本过程就是 读取数据 定义模型 定义损失 定义算法 初始化参数 开始训练

## 基础优化算法

- 梯度下降法：沿反梯度方向更新参数求解
- 小批量随机梯度下降：采用某几个训练集的样本来计算梯度 近似损失 减少开销 这是深度学习默认的求解算法 重要参数是批量大小和学习率

# CH9 Softmax 回归+损失函数+图片分类数据集

## Softmax 回归

- Softmax 回归并不是回归 而是分类算法 输出有多个 分别是每个类的置信度

- 对于类别进行一位编码 即哪个类别就置哪一位为 1 使用 MSE 损失训练 取置信度最大的类别为预测值 期望预测类别的置信度要远大于其他类别 得到更加置信的结果

- 我们希望输出符合概率的形式 即非负且和为 1 因此我们把输出向量做 softmax 函数处理 即指数函数归一化 这样每个输出就可以作为预测概率

- 我们常使用交叉熵来衡量概率向量的差距 因此损失函数采用预测向量和真实向量的交叉熵 而其梯度刚好就是真实概率和预测概率的差（可推导）

- 实现流程

  首先说明这个模型是怎么样的：输入一个特征向量 x（比如输入一张图片就是展开的向量）对这个向量 x 进行加权线性组合以及附加偏移可以产生一个向量 y 其维度和分类数一致 最后对 y 进行 softmax 处理 得到同维度的向量 P 这个 P 就是置信度向量 我们需要得到最优的加权线性组合的参数 即矩阵 W 和偏移向量 b

  1. 因为 softmax 的输入是向量 因此把图片展开成向量 不过会损失很多空间信息 数据集有 10 个类别 则输出向量的维度就是 10
  2. 初始化参数 包括权重矩阵 W 和偏移向量 b
  3. 定义 softmax 函数 即对于一个向量做指数归一化
  4. 定义模型函数 即把一批次的输入向量展开成矩阵 比如一批次是 256 每一个输入向量有 784 个特征（图片有 28\*28 像素） 那么矩阵就是 256\*784 的 然后右乘 W 矩阵（784\*10） 产生 256\*10 的矩阵 在加上偏移向量 b（通过广播机制扩展成同维度的矩阵） 得到了最终的输出矩阵 即 256 个维度为 10 的置信度向量
  5. 实现交叉熵损失函数 需要用到花式索引
  6. 实现一个准确率函数 计算预测置信度和实际置信度一致的百分比
  7. 进行训练 使用梯度下降法进行迭代 最后使用测试集来测试准确率

## 常用损失函数

- MSE：均方误差
- L1：绝对值误差
- Huber 鲁棒：分段采用均方和绝对值误差 使得损失函数一阶可微

# CH10 感知机

## 单层感知机

- 单层感知机模型是一种二分类模型 其求解算法等价于梯度下降法 有收敛定理 确保在有限次迭代后可以找到一个解
- 单层感知机无法解决 XOR 问题 因为单层感知机只能产生一条分割线 而 XOR 需要两条分割线

## 多层感知机

- 多类分类即在 softmax 回归的基础上加上隐藏层 这样就实现了非线性模型
- **隐藏层数**和**隐藏层的神经单元数**都是超参数 输入维数很高而输出维数很低时 可以通过多层隐藏层来慢慢的降维压缩信息
- 通常会调高隐藏层数而不是隐藏层的神经单元数 这是因为隐藏层的神经单元数过多会导致过拟合
- 隐藏层需要非线性的激活函数 否则多层感知机和线性模型等价

## 激活函数

- ReLU：即 max(0,x)
- Sigmoid：即 1/(1+exp(-x)) 投影到(0,1)之间
- Tanh：即 (exp(x)-exp(-x))/(exp(x)+exp(-x)) 投影到(-1,1)之间

# CH11 模型选择、欠拟合和过拟合

## 模型选择

- 训练误差和泛化误差：训练误差是模型在训练集上的误差 泛化误差是模型在测试集上的误差 我们关心的是后者
- 验证数据集和测试数据集：验证数据集用于评估模型**超参数**好坏并据此调参选模 测试数据集用于评估模型的泛化误差 两者绝对不能混用
- K 折交叉验证：将数据集分成 K 份 每次取一份作为验证集 其余作为训练集 进行 K 次训练和验证 最后取平均值作为模型的泛化误差

## 过拟合和欠拟合

- 欠拟合：模型无法得到较低的训练误差 通常是因为模型太简单
- 过拟合：模型在训练集上表现很好但在测试集上表现很差 通常是因为模型太复杂
- 模型容量：模型拟合各种函数的能力 容量越大 可以记住的数据特征越多 训练误差会随着模型容量的增加而减小 但泛化误差会先减小后增大
- 估计模型容量：参数个数和参数值范围是两个重要的因素
- VC 维：模型可以完美拟合任意数据集的最大容量 比如 2 维输入的线性模型的 VC 维是 3

# CH12 权重衰退

- 通过限制参数值的选择范围来降低模型的复杂度 从而减少**过拟合** 通常限制权重而不是偏移
- L2 范数正则化：即在损失函数中加入权重的平方和的一半作为罚函数 通过调整超参数 λ 来调整正则化的强度 是一种柔性约束
- 权重衰退通过 L2 范数正则化使得模型参数不会过大 从而控制模型的复杂度 减少过拟合

# CH13 丢弃法

- 动机：一个好的模型 对于多噪音的数据集应该是**鲁棒**的 在数据中加入随机噪音等价于 Tikhonov 正则化
- 丢弃法对于每一个元素做如下扰动：以概率 p 丢弃该元素 以概率 1-p 保留该元素 从而保证期望不变 实现模型复杂度的控制 让模型更加抗干扰
- 丢弃概率 p 是一个超参数
- 通常将丢弃法作用于隐藏层的输出 即去掉某些神经元 只在训练时使用 在测试时不使用
- 丢弃法有点像是丢弃神经元后的各个子神经网络的集成 不过现在更多认为是一种正则化方法
- dropout 是现在比较主流的随机超参数调整方法

# CH14 数值稳定性+模型初始化和激活函数

## 数值稳定性

- 在深度模型中由于层数过多 可能会有多个数累乘的情况 这样会导致数值不稳定 从而导致梯度消失或爆炸

- 梯度消失：梯度在反向传播过程中会不断的累乘 如果梯度小于 1 那么会不断的缩小 最后会消失

- 梯度爆炸：梯度在反向传播过程中会不断的累乘 如果梯度大于 1 那么会不断的放大 最后会爆炸

## 模型初始化和激活函数

- 思路：让每一层的输出方差尽可能相等 可以使得每一层的梯度尽可能相等 从而避免梯度消失或爆炸
- 权重初始化：为了让每一层的输出方差尽可能相等 可以使用 Xavier 初始化方法 可以取均匀分布或正态分布
- 激活函数：为了让每一层的期望和方差尽可能相等 激活函数必须近似于 f(x)=x 对于 relu 和 tanh 满足这个条件 而 sigmoid 则需要进行适当的线性变换
- 总结：权重初始化和激活函数相当于对每一层的输出进行归一化 并不会丢失信息 让模型的数值稳定性更好 从而避免梯度消失或爆炸

# CH15 Kaggle 比赛：房价预测

- 先数据预处理：对于缺失值进行填充 对于类别特征进行 one-hot 编码 对于数值特征进行标准化
- 选择模型：这里选择了一个简单的单层线性回归模型
- K 折交叉验证：这里选择了 5 折交叉验证
- 调整超参数：不断改变超参数 选出交叉验证平均误差最小的超参数
