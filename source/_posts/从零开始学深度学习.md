---
title: 从零开始深度学习
date: 2024-09-21 19:55:00
categories: AI
tags:
  - 深度学习
index_img:
banner_img:
excerpt: "闲来无事 跟着李沐大佬的教程来学习一下深度学习的知识 本文档做笔记用"
---

闲来无事 跟着李沐大佬的教程来学习一下深度学习的知识 本文档做笔记用

# 安装环境

- 我有 NVIDIA 的独显 因此可以安装 CUDA 使用 GPU 来跑深度学习 去官网下载新版即可 注意如果不想装在系统盘 需要自定义安装
- 安装 Anaconda 或是 miniconda 我选择 Anaconda 可以参考这个：https://www.jianshu.com/p/eaee1fadc1e9

  注意如果安装了 python 版本为 3.12 的 anaconda 在安装 d2l 时会报错 我选择使用`conda create --name d2l python=3.9 -y`指令创建一个 3.9 版本 python 的环境来进行学习

- 安装 Pytorch 去官网下：https://pytorch.org/get-started/locally/
- 安装 jupyter 和 d2l 使用 pip 即可
- 下载提供的笔记本 本地运行 jupyter 进入某一个章节进行测试

## （可选）使用 VS Code+Jupyter 来快速浏览笔记本

每次都使用 anaconda 的 shell 运行 jupyter 非常不方便 可以直接下载 vscode 的 jupyter 扩展 然后选择内核为对应的 python（比如我用 3.9） 然后就可以在 vscode 里浏览笔记啦

# 数据处理

- torch 提供和 numpy 类似的一系列数组操作 不过在这里数组其实是张量（tensor）

- os 库可以对文件进行操作 panda 库可以对 csv 文件进行读取插值等操作

- 使用 fillna 函数对数据进行插值 get_dummies 函数可以把 NaN 和其他值分开为两列

# 线性回归

- 线性模型即输出=n 维输入的加权线性组合 可以看做是单层神经网络 即只有输入层和输出层的神经网络

- 损失函数是用于衡量实际值和预测值差距的 在回归问题中最常用的损失函数是平方损失 即方差

- 求解过程即最小化损失函数的过程 实际上就是最小二乘法

- 因为损失是凸函数 所以线性回归是具有显式解的

- 基本过程就是 读取数据 定义模型 定义损失 定义算法 初始化参数 开始训练

# 基础优化算法

- 梯度下降法：沿反梯度方向更新参数求解
- 小批量随机梯度下降：采用某几个训练集的样本来计算梯度 近似损失 减少开销 这是深度学习默认的求解算法 重要参数是批量大小和学习率

# Softmax 回归

- Softmax 回归并不是回归 而是分类算法 输出有多个 分别是每个类的置信度

- 对于类别进行一位编码 即哪个类别就置哪一位为 1 使用 MSE 损失训练 取置信度最大的类别为预测值 期望预测类别的置信度要远大于其他类别 得到更加置信的结果

- 我们希望输出符合概率的形式 即非负且和为 1 因此我们把输出向量做 softmax 函数处理 即指数函数归一化 这样每个输出就可以作为预测概率

- 我们常使用交叉熵来衡量概率向量的差距 因此损失函数采用预测向量和真实向量的交叉熵 而其梯度刚好就是真实概率和预测概率的差（可推导）

- 实现流程

  首先说明这个模型是怎么样的：输入一个特征向量 x（比如输入一张图片就是展开的向量）对这个向量 x 进行加权线性组合以及附加偏移可以产生一个向量 y 其维度和分类数一致 最后对 y 进行 softmax 处理 得到同维度的向量 P 这个 P 就是置信度向量 我们需要得到最优的加权线性组合的参数 即矩阵 W 和偏移向量 b

  1. 因为 softmax 的输入是向量 因此把图片展开成向量 不过会损失很多空间信息 数据集有 10 个类别 则输出向量的维度就是 10
  2. 初始化参数 包括权重矩阵 W 和偏移向量 b
  3. 定义 softmax 函数 即对于一个向量做指数归一化
  4. 定义模型函数 即把一批次的输入向量展开成矩阵 比如一批次是 256 每一个输入向量有 784 个特征（图片有 28\*28 像素） 那么矩阵就是 256\*784 的 然后右乘 W 矩阵（784\*10） 产生 256\*10 的矩阵 在加上偏移向量 b（通过广播机制扩展成同维度的矩阵） 得到了最终的输出矩阵 即 256 个维度为 10 的置信度向量
  5. 实现交叉熵损失函数 需要用到花式索引
  6. 实现一个准确率函数 计算预测置信度和实际置信度一致的百分比
  7. 进行训练 使用梯度下降法进行迭代 最后使用测试集来测试准确率

# 常用损失函数

- MSE：均方误差
- L1：绝对值误差
- Huber 鲁棒：分段采用均方和绝对值误差 使得损失函数一阶可微
