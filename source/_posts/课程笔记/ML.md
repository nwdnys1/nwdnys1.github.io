---
title: 《机器学习》课程笔记
date: 2025-02-19 14:36:41
categories: 课程笔记
tags:
  - 机器学习
  - 人工智能
  - 深度学习
index_img:
banner_img:
excerpt: " "
---

## L1 - 机器学习概述

### AI 发展简史

- 萌芽期
- 兴奋期
- 爆发期
- 数据驱动
- 深度学习

### AI vs ML vs DL

- AI>ML>DL 机器学习是需要从样例中学习数据规律的技术 而深度学习则是其中自动学习特征的一种方法

### WHAT IS ML？

- 从数据（数据集）中学习规律（模型） 不断优化性能（优化算法）以实现目标（损失函数）的算法

### 分类

- 监督学习：每个输入样本都有一个标签 比如分类问题、回归问题
- 无监督学习：没有标签的数据集 比如聚类问题、降维问题
- 强化学习：通过与环境交互来学习 环境包括奖励和惩罚等机制 目标是最大化长期奖励

### 泛化性

- 泛化性是指模型对未知数据的适应能力 比如一个分类模型是否能处理未见过的数据
- 过拟合：模型在训练集上表现很好 但在测试集上表现很差 通常是因为模型把训练数据的噪声因素也一并学习 相当于只会背题的学生 泛化性差
  - 措施
    1. 数据划分：训练集、验证集、测试集
    2. 交叉验证：K 折交叉验证 即每份数据集轮流作为验证集 解决数据量不足的问题
    3. 早停：出现过拟合迹象时及时停止训练（即训练损失下降 但是验证损失上升）
    4. 正则化：对参数增加惩罚项 限制模型的复杂度
- 欠拟合：模型在训练集和测试集上表现都很差 通常是因为模型太简单 无法拟合数据的复杂性 相当于智商不够的学生 泛化性差

## L2 - 线性回归

- 线性回归是一种用于预测连续值的监督学习算法

### 模型结构

- 基本形式：y = wx + b 其中 w 是权重 x 是输入 b 是偏置
- 训练过程：根据误差来调整 w 和 b
- 预测过程：根据新输入的 x 来预测 y
- 损失函数：均方误差（MSE）是最常用的损失函数 即预测值与真实值的差的平方的均值
- 优化方法：梯度下降是最常用的优化方法 即根据损失函数的梯度来调整参数
- 解析解：w = (X^T X)^-1 X^T y 其中 X 是输入矩阵 y 是输出向量 存在矩阵不可逆的情况 可以通过正则化来解决

## L3 - 分类问题

- 分类问题是一种用于预测离散值的监督学习算法 包括决策树、随机森林等

### 决策树

- 基本思想：数据的分布可能是非线性的 决策树通过递归地划分数据空间来进行分类
- 基本结构：根节点、内部节点、叶节点 其中叶节点代表类别 非叶结点代表特征判断条件
- 训练过程（递归构造决策树）
  1. 构造根节点
  2. 选择一个最合适的特征进行划分
  3. 根据特征将数据集划分为若干子集
  4. 创建子节点 递归地对子集进行划分
  5. 递归进行 2-4 直到满足停止条件（包括叶子节点都属于同一类、叶子节点样本数量太小、没有更多特征可用）
- 训练算法（如何选择最合适的特征）
  - ID3：基于信息增益 即选择能够最大程度降低信息熵的特征 每一轮遍历所有特征 计算信息增益 注意特征分界值通常选择中位数或是平均值
  - CART：基于基尼指数
  - C4.5：基于信息增益比
- 优点
  - 简单直观 易于理解 可视化性强
  - 预处理要求低 可以处理缺失值
  - 可以处理非线性边界
  - 数据驱动 可以任意精度拟合数据集
- 缺点
  - 容易过拟合 会把噪声数据划分为一个叶子节点
  - 为了解决过拟合问题 下面介绍随机森林

### 随机森林

- 基本思想：将数据随机划分为若干子集 分别训练若干个决策树 最后通过投票来决定最终分类结果 由于噪声数据最多只会影响一个决策树 因此不会影响最终的分类结果
- 优点
  - 比决策树更准确
  - 训练并行化 速度快
  - 可以处理高维数据
  - 可以处理缺失值
  - 结果可解释性强
  - 可以检测特征之间的相关性
  - 对于不平衡数据集效果好 可以平衡误差

## L4 - 贝叶斯分类

- 基本思想：将分类预测表示为概率分布的形式 通过贝叶斯定理来计算后验概率 选取概率最大的类别作为预测结果
- 计算后验概率时 需要提取很多特征的联合概率分布 为了简化计算 引入了**朴素贝叶斯假设** 即假设特征之间相互独立 这样联合分布就可以分解为各个特征的边缘分布的乘积 而边缘分布概率只需要通过统计计算即可
- 一旦某一中特征的概率为 0 那么整个后验概率就为 0 为了解决这个问题 引入了**拉普拉斯平滑** 即为每个事件的 cnt+1 保证非零
- 算法过程：（以垃圾邮件分类为例）
  1. 计算每个分类的先验概率 比如 P(垃圾邮件) = 0.1 P(非垃圾邮件) = 0.9
  2. 根据训练集统计出每个特征的条件概率 比如 P(“免费”|垃圾邮件) = 0.8 P(“免费”|非垃圾邮件) = 0.1
  3. 对于一个新样本 根据贝叶斯定理计算后验概率 比如 P(垃圾邮件|“免费” “优惠”) = 0.2 P(非垃圾邮件|“免费” “优惠”) = 0.3 取概率最大的类别作为预测结果 即非垃圾邮件
- 优点
  - 非常快速 文本分类效果好
- 缺点
  - 假设过于简单 可能导致欠拟合

## L5 - KNN

- 基本思想：通过计算新样本与训练集中所有样本的距离 来找到与新样本最近的 k 个样本 然后根据这 k 个样本的类别来预测新样本的类别
- 距离计算：常用的距离计算方法有欧氏距离 曼哈顿距离 闵可夫斯基距离 余弦相似度等
- 算法过程
  1. 预处理数据 包括归一化、标准化等
  2. 计算新样本与训练集中所有样本的距离
  3. 找到与新样本最近的 k 个样本
  4. 根据这 k 个样本的类别来预测新样本的类别
- K 的选择
  - 维诺图：将区域划分为不同的类别区域 使得每个区域的点到其种子点的距离最小
  - 当 K 较小时 容易受到噪声的影响 会导致过拟合
  - 当 K 较大时 容易受到样本不均衡的影响 会导致欠拟合
  - K 合适时 分界线比较平滑
- 优点
  - 简单直观 易于理解
  - 无需训练
- 缺点
  - 数据量太大时计算量大
  - 数据维度太高时效果不好

## L6 - 逻辑回归

- 基本思想：逻辑回归是一种用于预测分类问题的监督学习算法 使用判别函数来判定样本的类别 可以认为逻辑回归拟合的是类别的边界

### 感知机

- 判别函数：g(x) = w1x1 + w2x2 + w0 大于和小于 0 分别代表两个类别
- 训练过程：对于每一个数据 如果分类正确 则不做任何操作 如果分类错误 则 w+=学习率\*样本
- 缺点：判别函数输出的是两个离散点 没法进行优化 换句话说 我们无法知道一个样本属于某一类的概率

### 逻辑回归

- 逻辑函数：log(y/(1-y)) 其中 y 是样本属于某一类的概率
  - 经推导 若样本呈正态分布 逻辑函数可以被表示为 wT \* x + w0 即 x 的线性函数
  - 再经反推得 y=1/(1+e^(-wTx+w0)) 即 sigmoid 函数 其具有概率性质
  - sigmoid 的导数：y=sigmoid(x) 则 dy/dx = y(1-y)
- 判别函数：g(x) = 1/(1+e^(-wTx+w0)) 即上面推导的逻辑函数 大于和小于 0.5 分别代表两个类别
- 损失函数：交叉熵损失函数 即真实值和预测值的交叉熵
- 训练过程：梯度下降法 不断调整 w 和 w0 使得损失函数最小化即可
- 优点
  - 判别函数具有概率性质 可以代表样本属于某一类的概率

### softmax 回归

- 逻辑回归是二分类问题的解决方案 对于多分类问题 可以使用 softmax 函数 输出一个概率向量 其具有两个性质
  - 当有一个较大值时 其输出值接近 1 其它值接近 0
  - 连续可微
- 判别函数：g(x) = softmax(wTx + w0) 即将逻辑回归的判别函数推广到多分类问题
- 损失函数：交叉熵损失函数 即最大化正确类别的概率
- 训练过程：梯度下降法

## L7 - SVM

- 线性分类只能找到一个超平面来划分数据集 但是不能找到效果最优的超平面 通常距离各个类别的边界最远的超平面效果最好
- 如果超平面为 wTx+w0=0 且 wTx+w0=±1 为两个类别的边界 则两个超平面之间的距离为 2/||w|| 在给定约束（所有样本都在边界之外）的情况下 最大化这个距离即可 即一个最优化问题（二次规划问题）
- 基本思想：SVM（支持向量机）就是在对这个优化问题进行求解
- 模型结构和逻辑回归一样
- 损失函数：hinge 损失函数 即 max(0, 1-y(wTx+w0)) 再加上正则化项||w||^2 确保 w 不会过大
- 优化方法：梯度下降法 导数需要分类计算

## L8 - MLP

- 基本思想：感知机基于人的神经元模型 一个感知机可以看作一个神经元 一个多层感知机可以看作一个神经网络
  - 感知机的结构：y = f(wTx + w0) 其中 f 是激活函数
  - 单层感知机的缺陷：只能解决线性可分问题 比如 xor 问题不能解决
  - 于是引入了多层感知机 即具有隐藏层的感知机
  - 为了拟合非线性问题 需要引入非线性激活函数 比如 ReLU
  - 最终的 MLP 可以看做是任意的非线性函数模型
- 训练过程：
  - 正向传播：从输入层到输出层计算每一层的输出
  - 反向传播：利用链式求导计算梯度 相当于每个参数计算相对于下一个神经元的梯度 不断传递 最终累乘
  - 参数更新：使用反向传播得到的梯度来更新参数

### 深度学习

- MLP 有两种 一种是浅层但是较宽的网络 一种是深层但是较窄的网络 其中后者在 2012 年的 ImageNet 比赛中取得了突破性的进展 从而引发了深度学习的热潮
- 深层 MLP 的优势在于可以模块化地学习特征 从而可以学习到更加复杂的特征
- 深度学习严格的定义是深度表征学习 即一层一层的抽象出数据的特征
- 常见的深度学习模型
  - DNN：深度神经网络 即多层感知机 经常作为其他模型的中间层 而不直接使用
  - CNN：卷积神经网络 适用于图像处理任务
  - RNN：循环神经网络 适用于序列数据处理任务
  - Transformer：适用于自然语言处理任务

## L9 - LM

- 语言模型是一种用于预测下一个词的监督学习算法
- 在构造模型之前 需要先对文本进行预处理 首先是文本的向量化
  - one-hot 编码：将每个词映射为一个唯一的整数 维度太大 并且所有词之间互相正交 没有相似性
  - word2vec：将每个词映射为一个低维的稠密向量 使得相似的词在向量空间中距离较近 有两种模型可以采用
    - CBOW（词袋模型）
      - 作用是输入上下文的多个词 输出预测的中心词
      - 结构
        - 输入层：输入上下文词的 one-hot 编码
        - 矩阵 W：将 one-hot 编码映射为低维词向量 然后对词向量求和或求平均
        - 矩阵 W'：输入平均词向量 输出概率分布向量 代表中心词的概率
      - 训练：梯度下降法 当真实的中心词概率太低时 通过梯度下降法调整参数
      - 最终矩阵 W 就是 word2vec 的映射表
    - Skip-gram（跳字模型）
      - 作用是输入中心词 输出预测的上下文词
- 定义：语言模型的本质是预测下一个词的概率 即 P(wt|w1,w2,...,wt-1) 通过最大化这个概率来训练模型
- 语言模型需要支持变长输入、具有长期依赖性、保留语序信息等特性 于是引入了 RNN 模型

### RNN

- RNN（循环神经网络）是一种具有记忆性的神经网络 每一次的输出由上一次的输出和当前的输入共同决定 即 yi=f(yi-1, xi) 也就带上了之前所有数据的特征 适合预测时间序列数据与自然语言处理
- 应用
  - 语言分类：输入一个句子 输出一个向量 代表这个句子的类别
  - 语句生成：输出的向量进行 softmax 处理 得到一个概率分布 代表下一个词的概率
  - 理论上来说 只要能 tokenize 就可以用 RNN 来进行预测生成
- LSTM（Long Short-Term Memory）：长短期记忆网络 是 RNN 的一种变体 通过门控机制来控制信息的流动 具体而言 LSTM 有三个门：遗忘门、输入门、输出门 遗忘门决定了之前的状态有多少会被遗忘 输入门决定了新的状态有多少会被加入 输出门决定了这个状态有多少会被输出 虽然 LSTM 仍不能并行 但是单个 LSTM 单元内部的计算的并行度较好 比较适合长期记忆的学习

## L10 - Transformer

### RNN Encoder-Decoder

- RNN Encoder-Decoder 是一种用于序列到序列的模型 由两个 RNN 组成 一个编码器和一个解码器 编码器将输入序列编码为一个固定长度的向量 解码器将这个向量解码为输出序列
- 应用
  - 机器翻译：输入一个句子 输出翻译后的句子
  - 对话生成：输入一个问题 输出回答
- 缺点
  - RNN 的并行性差 训练速度慢
  - RNN 对于长序列的理解能力较差 这是因为不论多长的序列都会被编码为一个固定长度的向量 对于长序列 向量无法包含所有的信息
- 为了解决这个问题 引入了注意力机制
  - 生成输出序列的每个单词时 动态的考虑输入序列 也就是说一个输出单词依赖于输入序列的某几个单词产生的向量 而不是依赖于固定编码器产生的向量
  - 具体而言 对于输出的第 i 个单词 会先根据权重取出输入序列的某些单词 然后生成向量 作为解码器的输入
  - 权重参数由一个神经网络来训练计算得到

### Transformer

- Transformer 是一种基于注意力机制的模型 与之前的 RNN Encoder-Decoder 模型相比 其可以并行化训练 速度更快
- Transformer 并行化的关键是 Self-Attention 机制
  - RNN 对于语句的理解是顺序的 即从头开始遍历每个单词 从而理解语句的意思
  - Self-Attention 机制是并行的 在编码时 其会反复理解输入语句 对于输入的第 i 个单词 需要进行搜索 找到输入序列中与其相关的单词 然后根据这些单词计算出一个向量 这个向量就是第 i 个单词新的理解
  - 具体而言
    1. 将每个单词 embedding 为一个向量 H
    2. 通过三个矩阵 Wq、Wk、Wv 将每个单词的向量映射为三个向量 Q、K、V
    3. 计算每个单词组合的 Q 和 K 的点积 最终得到一个权重矩阵 A 代表了单词 i 与单词 j 的相关性 也即注意力权重
    4. 用 softmax 函数将权重矩阵归一化
    5. Hi’ = ∑(Aij \* Vj) 其中 Aij 是单词 i 对于单词 j 的注意力 Vj 是单词 j 的向量 这个 Hi’ 就是单词 i 的新的理解
  - sa的计算过程类似于矩阵运算 因此显然可以并行化 具体而言
    1. Q = H \* Wq, K = H \* Wk, V = H \* Wv
    2. A = softmax(Q \* K^T / sqrt(dk)) 其中 dk 是 K 的维度
    3. H’ = A \* V
