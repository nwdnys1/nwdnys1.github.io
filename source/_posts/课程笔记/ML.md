---
title: 《机器学习》课程笔记
date: 2025-02-19 14:36:41
categories: 课程笔记
tags:
  - 机器学习
  - 人工智能
  - 深度学习
index_img:
banner_img:
excerpt: " "
---

## L1 - 机器学习概述

### AI 发展简史

- 萌芽期
- 兴奋期
- 爆发期
- 数据驱动
- 深度学习

### AI vs ML vs DL

- AI>ML>DL 机器学习是需要从样例中学习数据规律的技术 而深度学习则是其中自动学习特征的一种方法

### WHAT IS ML？

- 从数据（数据集）中学习规律（模型） 不断优化性能（优化算法）以实现目标（损失函数）的算法

### 分类

- 监督学习：每个输入样本都有一个标签 比如分类问题、回归问题
- 无监督学习：没有标签的数据集 比如聚类问题、降维问题
- 强化学习：通过与环境交互来学习 环境包括奖励和惩罚等机制 目标是最大化长期奖励

### 泛化性

- 泛化性是指模型对未知数据的适应能力 比如一个分类模型是否能处理未见过的数据
- 过拟合：模型在训练集上表现很好 但在测试集上表现很差 通常是因为模型把训练数据的噪声因素也一并学习 相当于只会背题的学生 泛化性差
  - 措施
    1. 数据划分：训练集、验证集、测试集
    2. 交叉验证：K 折交叉验证 即每份数据集轮流作为验证集 解决数据量不足的问题
    3. 早停：出现过拟合迹象时及时停止训练（即训练损失下降 但是验证损失上升）
    4. 正则化：对参数增加惩罚项 限制模型的复杂度
- 欠拟合：模型在训练集和测试集上表现都很差 通常是因为模型太简单 无法拟合数据的复杂性 相当于智商不够的学生 泛化性差

## L2 - 线性回归

- 线性回归是一种用于预测连续值的监督学习算法

### 模型结构

- 基本形式：y = wx + b 其中 w 是权重 x 是输入 b 是偏置
- 训练过程：根据误差来调整 w 和 b
- 预测过程：根据新输入的 x 来预测 y
- 损失函数：均方误差（MSE）是最常用的损失函数 即预测值与真实值的差的平方的均值
- 优化方法：梯度下降是最常用的优化方法 即根据损失函数的梯度来调整参数
- 解析解：w = (X^T X)^-1 X^T y 其中 X 是输入矩阵 y 是输出向量 存在矩阵不可逆的情况 可以通过正则化来解决

## L3 - 分类问题

- 分类问题是一种用于预测离散值的监督学习算法 包括决策树、随机森林等

### 决策树

- 基本思想：数据的分布可能是非线性的 决策树通过递归地划分数据空间来进行分类
- 基本结构：根节点、内部节点、叶节点 其中叶节点代表类别 非叶结点代表特征判断条件
- 训练过程（递归构造决策树）
  1. 构造根节点
  2. 选择一个最合适的特征进行划分
  3. 根据特征将数据集划分为若干子集
  4. 创建子节点 递归地对子集进行划分
  5. 递归进行 2-4 直到满足停止条件（包括叶子节点都属于同一类、叶子节点样本数量太小、没有更多特征可用）
- 训练算法（如何选择最合适的特征）
  - ID3：基于信息增益 即选择能够最大程度降低信息熵的特征 每一轮遍历所有特征 计算信息增益 注意特征分界值通常选择中位数或是平均值
  - CART：基于基尼指数
  - C4.5：基于信息增益比
- 优点
  - 简单直观 易于理解 可视化性强
  - 预处理要求低 可以处理缺失值
  - 可以处理非线性边界
  - 数据驱动 可以任意精度拟合数据集
- 缺点
  - 容易过拟合 会把噪声数据划分为一个叶子节点
  - 为了解决过拟合问题 下面介绍随机森林

### 随机森林

- 基本思想：将数据随机划分为若干子集 分别训练若干个决策树 最后通过投票来决定最终分类结果 由于噪声数据最多只会影响一个决策树 因此不会影响最终的分类结果
- 优点
  - 比决策树更准确
  - 训练并行化 速度快
  - 可以处理高维数据
  - 可以处理缺失值
  - 结果可解释性强
  - 可以检测特征之间的相关性
  - 对于不平衡数据集效果好 可以平衡误差

## L4 - 贝叶斯分类

- 基本思想：将分类预测表示为概率分布的形式 通过贝叶斯定理来计算后验概率 选取概率最大的类别作为预测结果
- 计算后验概率时 需要提取很多特征的联合概率分布 为了简化计算 引入了**朴素贝叶斯假设** 即假设特征之间相互独立 这样联合分布就可以分解为各个特征的边缘分布的乘积 而边缘分布概率只需要通过统计计算即可
- 一旦某一中特征的概率为 0 那么整个后验概率就为 0 为了解决这个问题 引入了**拉普拉斯平滑** 即为每个事件的 cnt+1 保证非零
- 算法过程：（以垃圾邮件分类为例）
  1. 计算每个分类的先验概率 比如 P(垃圾邮件) = 0.1 P(非垃圾邮件) = 0.9
  2. 根据训练集统计出每个特征的条件概率 比如 P(“免费”|垃圾邮件) = 0.8 P(“免费”|非垃圾邮件) = 0.1
  3. 对于一个新样本 根据贝叶斯定理计算后验概率 比如 P(垃圾邮件|“免费” “优惠”) = 0.2 P(非垃圾邮件|“免费” “优惠”) = 0.3 取概率最大的类别作为预测结果 即非垃圾邮件
- 优点
  - 非常快速 文本分类效果好
- 缺点
  - 假设过于简单 可能导致欠拟合

## L5 - KNN

- 基本思想：通过计算新样本与训练集中所有样本的距离 来找到与新样本最近的 k 个样本 然后根据这 k 个样本的类别来预测新样本的类别
- 距离计算：常用的距离计算方法有欧氏距离 曼哈顿距离 闵可夫斯基距离 余弦相似度等
- 算法过程
  1. 预处理数据 包括归一化、标准化等
  2. 计算新样本与训练集中所有样本的距离
  3. 找到与新样本最近的 k 个样本
  4. 根据这 k 个样本的类别来预测新样本的类别
- K 的选择
  - 维诺图：将区域划分为不同的类别区域 使得每个区域的点到其种子点的距离最小
  - 当 K 较小时 容易受到噪声的影响 会导致过拟合
  - 当 K 较大时 容易受到样本不均衡的影响 会导致欠拟合
  - K 合适时 分界线比较平滑
- 优点
  - 简单直观 易于理解
  - 无需训练
- 缺点
  - 数据量太大时计算量大
  - 数据维度太高时效果不好

## L6 - 逻辑回归

- 基本思想：逻辑回归是一种用于预测分类问题的监督学习算法 使用判别函数来判定样本的类别 可以认为逻辑回归拟合的是类别的边界

### 感知机

- 判别函数：g(x) = w1x1 + w2x2 + w0 大于和小于 0 分别代表两个类别
- 训练过程：对于每一个数据 如果分类正确 则不做任何操作 如果分类错误 则 w+=学习率\*样本
- 缺点：判别函数输出的是两个离散点 没法进行优化 换句话说 我们无法知道一个样本属于某一类的概率

### 逻辑回归

- 逻辑函数：log(y/(1-y)) 其中 y 是样本属于某一类的概率
  - 经推导 若样本呈正态分布 逻辑函数可以被表示为 wT \* x + w0 即 x 的线性函数
  - 再经反推得 y=1/(1+e^(-wTx+w0)) 即 sigmoid 函数 其具有概率性质
  - sigmoid 的导数：y=sigmoid(x) 则 dy/dx = y(1-y)
- 判别函数：g(x) = 1/(1+e^(-wTx+w0)) 即上面推导的逻辑函数 大于和小于 0.5 分别代表两个类别
- 损失函数：交叉熵损失函数 即真实值和预测值的交叉熵
- 训练过程：梯度下降法 不断调整 w 和 w0 使得损失函数最小化即可
- 优点
  - 判别函数具有概率性质 可以代表样本属于某一类的概率

### softmax 回归

- 逻辑回归是二分类问题的解决方案 对于多分类问题 可以使用 softmax 函数 输出一个概率向量 其具有两个性质
  - 当有一个较大值时 其输出值接近 1 其它值接近 0
  - 连续可微
- 判别函数：g(x) = softmax(wTx + w0) 即将逻辑回归的判别函数推广到多分类问题
- 损失函数：交叉熵损失函数 即最大化正确类别的概率
- 训练过程：梯度下降法

## L7 - SVM

- 线性分类只能找到一个超平面来划分数据集 但是不能找到效果最优的超平面 通常距离各个类别的边界最远的超平面效果最好
- 如果超平面为 wTx+w0=0 且 wTx+w0=±1 为两个类别的边界 则两个超平面之间的距离为 2/||w|| 在给定约束（所有样本都在边界之外）的情况下 最大化这个距离即可 即一个最优化问题（二次规划问题）
- 基本思想：SVM（支持向量机）就是在对这个优化问题进行求解
- 模型结构和逻辑回归一样
- 损失函数：hinge 损失函数 即 max(0, 1-y(wTx+w0)) 再加上正则化项||w||^2 确保 w 不会过大
- 优化方法：梯度下降法 导数需要分类计算

## L8 - MLP

- 基本思想：感知机基于人的神经元模型 一个感知机可以看作一个神经元 一个多层感知机可以看作一个神经网络
  - 感知机的结构：y = f(wTx + w0) 其中 f 是激活函数
  - 单层感知机的缺陷：只能解决线性可分问题 比如 xor 问题不能解决
  - 于是引入了多层感知机 即具有隐藏层的感知机
  - 为了拟合非线性问题 需要引入非线性激活函数 比如 ReLU
  - 最终的 MLP 可以看做是任意的非线性函数模型
- 训练过程：
  - 正向传播：从输入层到输出层计算每一层的输出
  - 反向传播：利用链式求导计算梯度 相当于每个参数计算相对于下一个神经元的梯度 不断传递 最终累乘
  - 参数更新：使用反向传播得到的梯度来更新参数

### 深度学习

- MLP 有两种 一种是浅层但是较宽的网络 一种是深层但是较窄的网络 其中后者在 2012 年的 ImageNet 比赛中取得了突破性的进展 从而引发了深度学习的热潮
- 深层 MLP 的优势在于可以模块化地学习特征 从而可以学习到更加复杂的特征
- 深度学习严格的定义是深度表征学习 即一层一层的抽象出数据的特征
- 常见的深度学习模型
  - DNN：深度神经网络 即多层感知机 经常作为其他模型的中间层 而不直接使用
  - CNN：卷积神经网络 适用于图像处理任务
  - RNN：循环神经网络 适用于序列数据处理任务
  - Transformer：适用于自然语言处理任务

## L9 - LM

- 语言模型是一种用于预测下一个词的监督学习算法
- 在构造模型之前 需要先对文本进行预处理 首先是文本的向量化
  - one-hot 编码：将每个词映射为一个唯一的整数 维度太大 并且所有词之间互相正交 没有相似性
  - word2vec：将每个词映射为一个低维的稠密向量 使得相似的词在向量空间中距离较近 有两种模型可以采用
    - CBOW（词袋模型）
      - 作用是输入上下文的多个词 输出预测的中心词
      - 结构
        - 输入层：输入上下文词的 one-hot 编码
        - 矩阵 W：将 one-hot 编码映射为低维词向量 然后对词向量求和或求平均
        - 矩阵 W'：输入平均词向量 输出概率分布向量 代表中心词的概率
      - 训练：梯度下降法 当真实的中心词概率太低时 通过梯度下降法调整参数
      - 最终矩阵 W 就是 word2vec 的映射表
    - Skip-gram（跳字模型）
      - 作用是输入中心词 输出预测的上下文词
- 定义：语言模型的本质是预测下一个词的概率 即 P(wt|w1,w2,...,wt-1) 通过最大化这个概率来训练模型
- 语言模型需要支持变长输入、具有长期依赖性、保留语序信息等特性 于是引入了 RNN 模型

### RNN

- RNN（循环神经网络）是一种具有记忆性的神经网络 每一次的输出由上一次的输出和当前的输入共同决定 即 yi=f(yi-1, xi) 也就带上了之前所有数据的特征 适合预测时间序列数据与自然语言处理
- 应用
  - 语言分类：输入一个句子 输出一个向量 代表这个句子的类别
  - 语句生成：输出的向量进行 softmax 处理 得到一个概率分布 代表下一个词的概率
  - 理论上来说 只要能 tokenize 就可以用 RNN 来进行预测生成
- LSTM（Long Short-Term Memory）：长短期记忆网络 是 RNN 的一种变体 通过门控机制来控制信息的流动 具体而言 LSTM 有三个门：遗忘门、输入门、输出门 遗忘门决定了之前的状态有多少会被遗忘 输入门决定了新的状态有多少会被加入 输出门决定了这个状态有多少会被输出 虽然 LSTM 仍不能并行 但是单个 LSTM 单元内部的计算的并行度较好 比较适合长期记忆的学习

## L10 - Transformer

### RNN Encoder-Decoder

- RNN Encoder-Decoder 是一种用于序列到序列的模型 由两个 RNN 组成 一个编码器和一个解码器 编码器将输入序列编码为一个固定长度的向量 解码器将这个向量解码为输出序列
- 应用
  - 机器翻译：输入一个句子 输出翻译后的句子
  - 对话生成：输入一个问题 输出回答
- 缺点
  - RNN 的并行性差 训练速度慢
  - RNN 对于长序列的理解能力较差 这是因为不论多长的序列都会被编码为一个固定长度的向量 对于长序列 向量无法包含所有的信息
- 为了解决这个问题 引入了注意力机制
  - 生成输出序列的每个单词时 动态的考虑输入序列 也就是说一个输出单词依赖于输入序列的某几个单词产生的向量 而不是依赖于固定编码器产生的向量
  - 具体而言 对于输出的第 i 个单词 会先根据权重取出输入序列的某些单词 然后生成向量 作为解码器的输入
  - 权重参数由一个神经网络来训练计算得到

### Transformer

- Transformer 是一种基于注意力机制的模型 与之前的 RNN Encoder-Decoder 模型相比 其可以并行化训练 速度更快
- Transformer 并行化的关键是 Self-Attention 机制
  - RNN 对于语句的理解是顺序的 即从头开始遍历每个单词 从而理解语句的意思
  - Self-Attention 机制是并行的 在编码时 其会反复理解输入语句 对于输入的第 i 个单词 需要进行搜索 找到输入序列中与其相关的单词 然后根据这些单词计算出一个向量 这个向量就是第 i 个单词新的理解
  - 具体而言
    1. 将每个单词 embedding 为一个向量 H
    2. 通过三个矩阵 Wq、Wk、Wv 将每个单词的向量映射为三个向量 q、k、v
    3. 计算每个单词组合的 q_i 和 k_j 的点积 α_ij 代表了单词 i 对单词 j 的注意力权重
    4. 用 sqrt(dk)和 softmax 函数将权重矩阵归一化 得到注意力权重 Aij
    5. Hi’ = ∑(Aij \* Vj) 其中 Aij 是单词 i 对于单词 j 的注意力 Vj 是单词 j 的向量 这个 Hi’ 就是单词 i 的新的理解
  - SA 的计算过程类似于矩阵运算 因此显然可以并行化 具体而言
    1. Q = H \* Wq, K = H \* Wk, V = H \* Wv
    2. A = softmax(Q \* K^T / sqrt(dk)) 其中 dk 是 K 的维度
    3. H’ = A \* V
- RNN 可以保留语句的顺序 而 SA 机制是不保留顺序的 因此 TF 引入了位置编码（Positional Encoding）
  - 一个位置对应一个唯一的位置向量 e_i 这个 e_i 会被加到单词的向量 h_i 上一起输入到模型中
- 将 RNN Encoder-Decoder 模型中的 RNN 替换为 Self-Attention 就得到了 Transformer 模型
- TF 还可以用 Resudual Connection 和 Layer Normalization 来加速训练
  - Residual Connection：将输入直接加到输出上 使得模型更容易训练
  - Layer Normalization：对每一层的输出进行归一化 使得模型更容易训练
- Attention Masks
  - 在计算注意力权重时 需要对输入序列进行掩码处理 以防止模型看到未来的信息 或是被填充的部分影响
  - 掩码一般是用负无穷来消除掉对应权重
  - 掩码有两种
    - Self-Attention Mask：用于掩码填充部分
    - Cross-Attention Mask：用于 Decoder 忽略输入序列中填充部分
    - Causal Mask：用于 Decoder 掩码未来的信息
- 训练方法：梯度下降

- TF 存在的问题是训练数据不够 当数据量较小时 容易过拟合 然而我们可以把所有语言任务视为同一个任务 也就可以统一所有语言数据进行训练 这也就是预训练模型和大语言模型的由来

### 预训练模型

- 预训练模型有两步
  1. 预训练：在大规模的无监督数据集上进行训练 使得模型能够学习到语言的基本规律
  2. 微调：在小规模的有监督数据集上进行训练 使得模型能够适应特定的任务

#### BERT

- BERT（Bidirectional Encoder Representations from Transformers）是一种预训练模型 其使用了双向的 Transformer 编码器
- BERT 的预训练任务有两个
  - Masked Language Model：随机掩码输入序列中的一些单词（即用 mask 替换）然后训练模型去预测这些单词
  - Next Sentence Prediction：预测两个句子是否相邻 即预测语句 B 在语句 A 后面的概率
- BERT 的微调任务是将预训练的模型应用到特定的任务上 比如要做文本分类
  - 只需要在 BERT 的基础上加一个线性层和 softmax 层作为分类器即可
- BERT 不适合生成任务 只能用于分类任务

#### GPT

- GPT（Generative Pre-trained Transformer）是一种预训练模型 其使用了单向的 Transformer 解码器

## L11 - LLM

### Fine-tuning

- 微调即根据特定任务的数据集来训练模型 使得模型能够适应特定的任务
- hugging-face 提供了很多开源的预训练模型 可以直接下载下来进行微调
- 微调需要的计算量比较大 PEFT（Parameter Efficient Fine-Tuning）是一种微调方法 其只需要微调模型与任务相关的一部分参数 而不是全部参数
- LoRA（Low-Rank Adaptation）是一种 PEFT 方法 其通过将模型的权重矩阵分解为两个低秩矩阵来减少参数量
- LLaMA 是一个微调工具

### Prompt Engineering

- 提示词（Prompt）是一种用于引导模型生成特定输出的文本 其可以是一个问题、一个句子或一个段落
- 最基本的 Prompt 是直接给出问题或任务的描述
- Few-shot Prompt：给定一些示例来引导模型生成特定输出
- Chain-of-thought Prompt：给定一些推理过程来引导模型生成特定输出
- ReAct Prompt：给定一些反应过程来引导模型生成特定输出

### RAG

- RAG（Retrieval-Augmented Generation）是一种结合了检索和生成的模型 其通过检索相关的信息来增强生成的能力
- 具体而言
  1. 首先根据输入的文本检索相关的信息
  2. 然后将检索到的信息与输入的文本一起输入到生成模型中
  3. 最后生成模型根据输入的文本和检索到的信息生成输出
- RAG 由 3 个部分组成
  - Embedding：将输入的文本和检索到的信息映射为向量
  - Vector Store：存储向量化的信息
  - LLM：生成模型

## L12 - CNN

- 很经典的神经网络了 不再赘述
- 核心思想
  - 全连接神经网络处理图像分类具有以下问题
    - 果使用传统的向量作为输入 那么一旦物品产生小位移 就会导致向量大幅度变化
    - 对于图像输入来说参数太大 就算计算资源足够 过大的参数也往往会导致过拟合
    - 除此之外 全连接神经网络把图片作为一个向量输入 丢失了空间信息 并且看不到局部信息
  - 显然我们需要一个位移无关且局部相关的特征表示方式 这个方式就是卷积 使用卷积核提取特征就是 CNN 的核心思想
  - 题外话 卷积核模仿的是人类细胞的感受野 其会识别特定的特征
  - CNN 往往还有 stride、padding、pooling 等操作
    - stride：卷积核每次移动的步长
    - padding：在输入图像的边缘添加零值 使得卷积核可以覆盖到边缘的像素
    - pooling：下采样操作 用于减少特征图的大小 使得模型更具鲁棒性
- 一些经典 CNNs
  - LeNet：最早的卷积神经网络 由 Yann LeCun 提出 用于手写数字识别
  - AlexNet：2012 年的 ImageNet 比赛中获胜的卷积神经网络 由 Alex Krizhevsky 提出
  - VGGNet：2014 年的 ImageNet 比赛中获胜的卷积神经网络 由 Karen Simonyan 和 Andrew Zisserman 提出
  - ResNet：2015 年的 ImageNet 比赛中获胜的卷积神经网络 由 Kaiming He 提出 其使用了残差连接（Residual Connection）来解决深度网络训练困难的问题
- 计算公式
  - 卷积层输出大小：Hout = (Hin - K + 2P) / S + 1
    - Hin：输入大小
    - K：卷积核大小
    - P：padding 大小
    - S：stride 大小
  - 池化层输出大小：Hout = (Hin - K) / S + 1
    - Hin：输入大小
    - K：池化核大小
    - S：stride 大小
  - 卷积层参数量：K \* K \* Cin \* Cout + Cout \* has_bias
    - K：卷积核大小
    - Cin：输入通道数
    - Cout：输出通道数=卷积核数
    - has_bias：是否有偏置项
  - 全连接层参数量：(Cin + has_bias) \* Cout
    - Cin：输入通道数
    - Cout：输出通道数=卷积核数
    - has_bias：是否有偏置项

## L13 - CV

### 图片分割

- 图片分割是计算机视觉中的一个重要任务 其目的是将图像中的每个像素分配给一个类别
- 一个简单的做法是先用 CNN 提取特征 然后放大到原图大小 也即先通过池化、卷积进行下采样 再通过上采样放大回去
- 重新审视卷积 会发现假如展开原矩阵为一个向量 那么卷积操作等价于矩阵乘法 既然如此 上采样可以通过乘一个转置卷积矩阵来实现 等价于逆卷积操作
- 如上所述的类似于 En-De 结构的网络叫做 U-Net 是一种端到端模型

### 目标检测

- 目标检测是计算机视觉中的一个重要任务 其目的是在图像中找到目标的位置和类别
- 分类非常简单 只需要一个卷积神经网络就可以完成 位置检测则需要给出一个坐标和高宽来表示目标的位置
- 如果有多个目标 需要使用选择搜索算法来生成候选框（代表可能存在目标的位置）然后使用 CNN 来对每个候选框进行分类和回归
- RCNN（Region-based CNN）是一种用于目标检测的卷积神经网络 其使用了选择搜索算法来生成候选框 然后使用 CNN 来对每个候选框进行分类和回归 然而候选框数目太多会导致计算量过大
- Fast RCNN 是对 RCNN 的改进 其先对图片进行卷积 在输出的特征图上再进行 ROI（Region of Interest）操作 然后对每个候选框进行分类和回归
- Faster RCNN 是对 Fast RCNN 的改进 其使用了 RPN（Region Proposal Network）来生成候选框 RPN 是一个卷积神经网络 其可以在特征图上进行滑动窗口操作 来生成候选框
- YOLO（You Only Look Once）将目标检测问题视为回归问题 先产生多个候选框 然后对每个候选框预测(x, y, w, h, c) 其中 c 是 confidence score 最终根据置信度来组合候选框 形成最终的检测结果

### 其它任务

- 包括图像描述、图像生成等
- Image Captioning：本质上是翻译任务 因此可以使用 Encoder-Decoder 模型实现 即将图像编码为一个向量 然后将这个向量解码为描述 这其中仍然可以使用注意力机制来增强模型的能力 也可以把图片的特征图作为输入向量 构造自注意力机制
- Transformer 的编码器的作用是提取图像特征 而 CNN 也是提取图像特征的 有人想到或许可以省去 CNN 的步骤 直接用 Transformer 来提取图像特征 于是就有了 Vision Transformer（ViT）模型 其直接将图像划分为多个 patch 然后将每个 patch 展平作为输入向量 通过 Transformer 的编码器来提取特征 最后将特征输入到分类器中进行分类
- VLM（Vision Language Model）是将图像和文本结合起来的模型 即加入了图像的大模型
- CLIP（Contrastive Language-Image Pre-training）将图片和文本分别编码 嵌入到同一个向量空间 目标是让相似的图片和文本在这个空间中相似度更高 训练时 损失函数是图片文本对的 softmax 推理时只需要找到内积最大的文本图片对即可
- Adapted LLM 用 LLM 作为 decoder 将图片嵌入为可以输入给 LLM 的向量 和文本嵌入一起输入到 decoder 中 最终生成文本 LLaVA 就是这样的模型 现在大部分的大语言模型都是这样的做法

## L14 - K-Means

- K-Means 是一种无监督学习算法 其目的是将数据集划分为 K 个簇 使得每个簇的内部相似度最大 而不同簇之间的相似度最小
- 具体步骤
  1. 随机选择 K 个初始中心点
  2. 将每个样本分配到离其最近的中心点所在的簇
  3. 更新每个簇的中心点 为该簇中所有样本的均值（也就是质心）
  4. 重复步骤 2 和 3 直到中心点不再变化或达到最大迭代次数
- 复杂度
  - 时间复杂度：O(n \* k \* d \* i) 其中 n 是样本数 k 是簇数 d 是维度 i 是迭代次数
  - 空间复杂度：O(n + k \* d)
- 缺点
  - 局部最优 最终解是不确定的
  - 对于初始中心点的选择敏感
  - 假设簇是球形的
  - 需要数据可以求均值
- 肘部法则：通过计算不同 K 值下的 SSE（Sum of Squared Errors）来选择 K 值 当 K 增加时 SSE 会减小 但是当 K 达到一定值后 SSE 的减小幅度会减小 这个点就是最佳的 K 值
- 如何选择合适的初始中心点
  - K-Means++：在随机选择第一个中心点后 根据当前中心点的距离来选择下一个中心点 使得新选择的中心点距离当前中心点尽可能远
- 应用
  - 图片分割
  - 文本聚类
  - 图片压缩：将图片的颜色空间划分为 K 个簇 然后用每个簇的中心点来表示该簇中的所有像素

## L15 - Dimensionality Reduction

- 降维是将高维数据映射到低维空间的过程 其目的是减少数据的维度 使得数据更容易处理和可视化 本质上是减少特征之间的相关性 提取出独立的新特征
- 常见的降维方法包括 PCA 和 Auto-encoder

### PCA

- PCA（Principal Component Analysis）是一种线性降维方法 其通过线性变换将数据投影到一个新的坐标系中 使得新坐标系中的数据方差最大
- 比如一组数据 X=[x1,x2,x3,...,xn] 采用新坐标轴 u1 和 u2 那么 zi=uiT\*xi 就是新坐标系轴下的投影 Z=UT\*X 就是经过转换后的所有坐标 我们需要找到的就是 ui 使得 var(uiT\*X)=uT\*S\*u 最大 其中 S 是原始数据的协方差矩阵
- 形式化问题（一个二次规划问题）
  - 目标函数：max(uT\*S\*u)
  - 约束条件：||u||=1 即 u 是单位向量
  - 通过拉格朗日条件
    - L(u, λ) = uT\*S\*u - λ(||u||^2-1)
    - 对 u 求导数 令其为 0 得到 S\*u=λ\*u
    - 也就是对于协方差矩阵 S 求特征值和特征向量 求得的特征向量 u 就是解
    - 对于 TopK 的 PCA 只需要取出方差最大的前 K 个特征向量即可
- K 的选定：info loss=1-∑(λi/∑λi) 其中 λi 是特征值
- 具体步骤
  1. 对数据进行中心化处理 即为所有样本减去均值 使得数据均值为 0
  2. 计算协方差矩阵 S
  3. 计算协方差矩阵 S 的特征值和特征向量
  4. 选择前 K 个特征向量作为新的坐标系
  5. 将数据投影到新的坐标系中 得到降维后的数据
- 应用
  - 人脸识别：将人脸图像降维为多个特征向量 每一个新的人脸都可以表示为这些特征向量的线性组合
- PCA 本质上就是一个神经网络 提取出了原数据的主成分特征

### Auto-encoder

- Auto-encoder 是一种非线性降维方法 其通过神经网络来学习数据的低维表示
- 具体而言
  1. 编码器：将输入数据编码为一个低维的表示
  2. 解码器：将低维的表示解码为原始数据
  3. 损失函数：最小化输入数据和解码器输出数据之间的差异
- Deep Auto-encoder：将深度神经网络作为编码器和解码器 编码后的向量被称为隐编码（latent code）
- AE 和 PCA 的区别在于 PCA 是线性的 AE 是非线性的 但是两者可以看做殊途同归 都是在对数据进行降维
- 应用：图片搜索 先将数据降维之后再做相关性搜索 可以找到相似的图片
- Denoising Auto-encoder：在输入数据中添加噪声 然后训练模型去去噪声 使得模型可以从噪声中恢复出原始数据

## L16 - Generative Model

- 生成模型是指通过学习数据的真实分布来生成新的数据的模型 通过概率分布 模型可以生成符合某些特征的数据

### GAN

- GAN（Generative Adversarial Network）是一种生成模型 其通过对抗训练来学习数据的真实分布
- GAN 由两个部分组成
  - 生成器（Generator）：生成新的数据
  - 判别器（Discriminator）：判断数据是真实的还是生成的
- 具体流程
  1. 从真实分布中采样数据 x 从噪声分布中采样数据 z
  2. 输入数据到判别器 D 中 计算损失函数
  3. 根据损失函数更新判别器 D 的参数
  4. 循环 1-3 步 k 次
  5. 从噪声分布中采样数据 z
  6. 输入数据到生成器 G 中 计算损失函数
  7. 根据损失函数更新生成器 G 的参数
  8. 循环 1-7 步 直到收敛
- 具体可见深度学习笔记
- GAN 的变体
  - DCGAN：使用卷积神经网络作为生成器和判别器 其具有 latent vector arithmetic 的特性 类似于戴眼镜的男人-不戴眼镜的男人+不戴眼镜的女人=戴眼镜的女人
  - Conditional GAN：在生成器和判别器中加入条件变量 使得生成的数据具有特定的属性 相当于一个 image2image 的模型（图像翻译）
  - Cycle GAN：用于无监督的图像翻译 其通过循环一致性损失来训练模型 使得生成的数据可以转换回原始数据 即两个生成器 G 分别将 A 风格的图像转换为 B 风格的图像和 B 风格的图像转换为 A 风格的图像
- GAN 的缺陷是必须要借助判别器来指导生成器训练 如果需要不同特征的图片生成 就需要不同的判别器 VAE（Variational Auto-encoder）通过 AE 学习数据的特征 从而做到直接生成不同特征的图片

### VAE

- VAE（Variational Auto-encoder）是一种生成模型 其通过变分推断来学习数据的真实分布
- AE 本身可以做到将图片编码为特征向量 然后解码回原始图片 但是无法做到从这个特征向量生成相似图片（因为向量附近的其他向量生成的可能是无意义噪声）即向量空间中有效的区域是离散的 因此我们希望 AE 编码出的是一个分布 而不是一个向量
- 训练过程
  1. 编码器：输入一批图片数据 输出均值 μ 和方差 σ
  2. 重参数化：从均值和方差中采样一个向量 z=μ+σ\*ε 其中 ε 是从标准正态分布中采样的噪声 保证了 z 的分布是连续的 从而可以传播梯度
  3. 解码器：输入 z 输出重构的图片
  4. 损失函数：最小化重构损失和 KL 散度
     - 重构损失：最小化输入数据和解码器输出数据之间的差异
     - KL 散度：最小化编码器输出的分布和标准正态分布之间的差异
     - Loss = E[||x-x’||^2] + KL(q(z|x)||p(z))
  5. 梯度下降 反向传播
- VAE 本质上将特征向量的空间通过正则化变成了具有连续性和完整性的分布

### Diffusion Model

- Diffusion Model 是一种生成模型 其通过逐步去噪声来生成新的数据
- 具体而言
  1. 从真实数据中采样数据 x0
  2. 将数据逐步添加噪声 直到变成标准正态分布
  3. 学习一个去噪声的模型 D(x, t) 来预测每一步的噪声 一般是一个 U-Net 而损失是相同时间步的真实噪声和预测噪声之间的均方误差
  4. 从标准正态分布中采样数据 xt
  5. 使用去噪声模型 D(x, t) 来逐步去噪声 直到得到新的数据 x0

## L17 - RL

- 强化学习属于机器学习中与监督学习和无监督学习并列的分支 其核心思想是在缺少标签数据的情况下 通过与环境的交互来学习一个策略 使得模型可以在给定的状态下选择一个最优的动作（或策略）
- 马尔可夫决策过程（MDP）是强化学习的基础模型 核心观念是马尔可夫性 也就是当前状态只与前一个状态有关 与之前的状态无关
- RL 的基本要素
  - 状态空间 S：表示所有可能的状态
  - 动作空间 A：表示所有可能的动作
  - 策略 π：表示在给定状态下选择的动作 是一个条件概率分布
  - 奖励函数 R：表示在给定状态下选择的动作所获得的奖励
  - 回报函数 G：奖励随时间的积累 其中折扣因子 γ 用于控制未来奖励的权重
- RL 的基本流程
  1. 给定当前状态 s_t
  2. agent 根据策略 π 选择一个动作 a_t
  3. agent 执行动作 a_t 环境根据动作 a_t 转移到下一个状态 s_t+1 并返回奖励 r_t（需要注意 环境的转移可能是一个概率分布）
  4. agent 根据奖励 r_t 和下一个状态 s_t+1 更新策略 π
  5. 重复 1-4 直到达到终止状态
- RL 的目标是找到一个最优的策略 π 使得在这个策略下 回报的期望值最大化 这个期望值被称为价值函数 V_π(s) 即状态 s 下 之后持续执行 π 策略所获得的期望回报
- 除了状态价值 V 以外 还有状态动作价值 Q_π(s,a) 也就是在状态 s 下 执行动作 a 后 之后持续执行 π 策略所获得的期望回报
- Bellman 方程
  - 任何状态下的回报期望都可以拆解为当前状态的即时奖励和下一个状态开始累积的未来奖励 这样的递归关系被称为 Bellman 方程
  - V(s) = E[R_t + γV(s_t+1)|s_t=s]
  - Q(s,a) = E[R_t + γQ(s_t+1,a')|s_t=s,a_t=a]
- 寻找最优策略的方法
  - 基于价值函数的方法：学习出准确的价值函数 然后让 agent 始终选择最大价值的策略 比如 Q-learning SARSA 等
  - 基于策略的方法：直接学习策略本身 使得策略的长期回报最大化 比如 REINFORCE TRPO PPO 等
  - 如何选择行动：前者使用 ε-贪婪策略来选择动作（前期探索 后期利用） 后者则直接按照动作的概率分布来选择
  - 如何更新策略：前者基于贝尔曼方程更新价值函数 后者则会将对应策略的概率进行调整（比如获得正奖励就增加概率 负奖励就减少概率）
  - 其中价值方法中更新策略包括 MC 方法和 TD 方法
    - MC 方法：计算多个回合的平均值来估计价值函数 这使得模型方差较大 但是偏差较小
    - TD 方法：使用即时奖励和 2 个估计值来估计价值函数 这使得模型方差较小 但是偏差较大
- 优化算法的超参数
  - 更新频率：更新策略的频率 比如可以一回合更新一次 也可以每个时间步都更新
  - 传播深度：更新策略时 向后传播多远 比如可以只传播到当前状态 也可以传播到更远的状态
  - 优化公式：比如价值算法可以用 TD 也可以用 MC

#### Q-Learning

- Q-learning 是一种基于价值函数的方法 其主要思想是通过学习状态价值函数 Q(s,a) 来选择最优的动作
- 使用 ε-贪婪策略来选择动作
- 使用 Bellman 方程来更新 Q(s,a) 即
  - Q(s,a)' = Q(s,a) + α[R_t + γmaxQ(s_t+1,a') - Q(s,a)] 其中 α 是学习率 γ 是折扣因子
  - 可以看到 QL 假设下一步选定的是下一步状态中 Q 值最大的动作 并以此计算 Q 值的误差
  - 这样的策略也被称为 off-policy 策略 也就是实际执行的策略和用于学习的策略不一致

#### DQN

- DQN 是 Q-learning 的一种变种 其主要思想是使用深度神经网络来近似 Q(s,a) 的值 而非使用表格来存储 Q(s,a) 的值
- DQN 包含三个部分
  - Q 网络：输入状态 s 输出 Q(s,a) 向量 目的是学习实际 Q 值
  - 经验回放：一个表格 存储 agent 过去的经验（s,a,r,s'） 通过随机采样来打破数据之间的相关性
  - 目标网络：和 Q 网络结构相同 目的是学习下一个状态后的折扣 Q 值
- 具体流程
  1. 生成训练数据：
     - agent 根据 ε-贪婪策略在 Q 网络的输出中选择一个动作 a_t
     - 执行动作 a_t 观察到奖励 r_t 和下一个状态 s_t+1
     - 将 (s_t,a_t,r_t,s_t+1) 存入经验回放表格中
     - 重复一定次数 得到批量的训练数据
  2. 训练 Q 网络：
     - 随机从经验回放表格中采样一批数据(s,a,r,s')
     - 输入状态 s 计算 Q 网络的输出 选取对应动作的 Q 值 即 Q(s,a)
     - 输入状态 s' 计算目标网络的输出 选取最大的 Q 值 即 maxQ(s',a')
     - 计算损失 L = MSE(Q(s,a),r+γmaxQ(s',a')) = Σ(Q(s,a)-r-γmaxQ(s',a'))^2 / batch_size
     - 使用反向传播算法来更新 Q 网络的参数
  3. 更新目标网络：
     - 重复 1、2 一定次数后 再将 Q 网络的参数复制到目标网络中
- 可以看到 DQN 基本遵循了原始的 Q-learning 的思想 其中
  - 经验回放的作用是打破数据之间的相关性（可以满足独立同分布）并且神经网络需要批量数据来训练
  - 目标网络的作用是避免 Q 函数更新过快 导致模型不稳定

#### SARSA

- SARSA 和 QL 的唯一区别在于
  - SARSA 是 on-policy 策略 也就是实际执行的策略和用于学习的策略一致
  - 也即 会在真正选择了下一步的动作后 才会更新上一步的 Q 值

#### 策略梯度算法

- 策略梯度算法是基于策略的方法 其本质是梯度上升法
- 目标函数是：J(θ) = E[R|π_θ] 也就是在当前策略下的期望回报 在实际中 我们对这个策略进行多次采样得到批量的轨迹 τ 然后计算 J(θ)的梯度
- 策略梯度定理告诉我们可以用采样的轨迹来估计 J(θ)的梯度 具体推导比较复杂 公式如下：
  ```
  ∇J(θ) = E[∑∇logπ_θ(a|s)Q(s,a)] = E[∑∇logπ_θ(a|s)(r+γV(s'))]
  ```
- 计算出梯度后即可更新策略：
  - θ' = θ + α∇J(θ) 其中 α 是学习率

#### Baseline

- 我们注意到整个梯度的计算中包含两项
  - 第一项是 ∇logπ_θ(a|s) 也就是当前策略的梯度
  - 第二项是 Q(s,a) 也就是当前状态下的动作价值函数
- 如果我们可以缩减某一项的尺度大小 就相当于给整个乘积值做了一个缩放 自然就可以减少方差
- Baseline 方法的核心思想就是将第二项减去一个基准值 b(s) 使得整体的方差变小 这个基准值需要保证无偏性 一般可以选择 Q(s,a)的均值作为基准值 也就是 V(s)

#### REINFORCE

- REINFORCE 是一种基于策略的方法 其主要思想是通过采样的轨迹来估计 J(θ) 的梯度
- 核心思想是
- 具体流程
  1. agent 根据当前策略 π_θ 采样多条轨迹 τ = (s_0,a_0,r_0,s_1,a_1,r_1,...,s_T,a_T,r_T)
  2. 对于每条轨迹的每一个时间步 t 计算 G_t = ∑γ^k-t \* r_k (k=t+1,...,T)
  3. 更新策略 θ' = θ + α∇J(θ) = θ + α∇logπ_θ(a|s)G_t
  4. 重复 1-3 直到收敛
- 可以看到 REINFORCE 的核心思想是使用 Monte Carlo 方法来估计 J(θ)的梯度 也就是通过采样的轨迹来估计 Q(s,a) 也就是说 轨迹的实际回报值 G_t = ∑γ^k-t \* r_k (k=t+1,...,T) 代表着当前状态下的动作价值函数 Q(s,a)
- 由于 G_t 是一个随机变量 因此 REINFORCE 的方差较大 但是偏差较小 可以引入 baseline 方法来减少方差

#### MCTS

- MCTS（Monte Carlo Tree Search）是一种基于采样的搜索算法 其主要思想是通过随机采样来估计状态的价值
- MCTS 包含四个步骤
  1. 选择（Selection）：从根节点开始 根据 UCT（Upper Confidence Bound for Trees）选择一个子节点 直到到达叶子节点
  2. 扩展（Expansion）：在叶子节点上扩展一个新的子节点
  3. 模拟（Simulation）：从新扩展的子节点开始进行随机模拟 直到到达终止状态
  4. 回溯（Backpropagation）：将模拟结果回传到根节点 更新每个节点的访问次数和总奖励

#### Actor-Critic

- Actor-Critic 是一种结合了基于价值函数和基于策略的方法 其主要思想是使用一个价值函数来估计当前策略的价值 然后使用这个价值函数来更新策略
- baseline 方法提到过如何通过引入一个基准值来减少方差 而 A-C 算法中这个 baseline 就是状态价值函数 V 这使得梯度公式中的 Q(s,a)可以被替换为 Q(s,a)-V(s) 这个函数也被称为优势函数 A(s,a) 也就是在当前状态下 选择动作 a 的优势
- 我的理解：首先 V 是 Q 在不同动作下的期望值 因此 Q-V 不会改变整个期望值 然后 减去这个均值使得梯度中的后一项（Q(s,a)-V(s)）的绝对值变小 这也就使得整个梯度项的方差变小了
- Actor-Critic 包含两个部分
  - Actor：负责选择动作的策略网络 其输入状态 s 输出动作的概率分布 π_θ(a|s)
  - Critic：负责估计当前策略的价值函数 其输入状态 s 输出价值 V(s)
- 具体流程
  1. Agent 执行一个时间步 t 得到(s,a,r,s')
  2. Critic 根据当前状态 s 和动作 a 计算 V(s) 根据下一个状态 s' 和当前策略的输出计算 V(s')
  3. Critic 计算出 TD 误差 δ = r + γV(s') - V(s)
  4. Critic 使用损失函数 L = MSE(δ) 来更新神经网络的参数
  5. Actor 更新策略 θ' = θ + α∇J(θ) = θ + α∇logπ_θ(a|s)A(s,a) 其中 A(s,a) = Q(s,a)-V(s) = r + γV(s') - V(s) = δ (实际上是无偏估计)

#### TRPO

- TRPO 是一种基于策略的方法 其核心思想是对模型的更新进行约束 从而保证模型性能单调不减 防止出现学习率过大导致模型性能下降的情况
- 信任域即对于模型参数更新的约束 在 TPRO 算法中 这个约束是基于 KL 散度的 即更新前后的策略 π 和 π' 之间的 KL 散度不能超过阈值 δ 本质上也就限制了策略不能变化太多
- TRPO 将参数的优化转换为一个最优化问题 其约束就是上面提到的信任域 而目标函数则是 L(θ) = E[π'_θ(a|s) \* A(s,a) / π_θ(a|s)] 根据证明可以得到 只要在约束内 L(θ)有所提升 那么模型性能 J(θ)必定单调不减 因此只需要用这个优化的 θ' 来替换 θ 即可
- 实际应用中 上述最优化问题过于复杂 通常使用线性化的方法来简化 具体过程我也细看 在此不赘述

#### PPO

- PPO 是对于 TRPO 的一种改进 其主要思想是改进目标函数 使得计算复杂度降低
- 有两种类型的 PPO 它们都丢弃了 KL 散度的约束 对目标函数 L 进行了一定的修改
  - PPO-Clip：使用一个剪切的目标函数 使得模型的更新不会超过一个阈值
  - PPO-Penalty：使用一个惩罚项来约束模型的更新
- PPO-Clip 为目标函数中的 r*t(θ) = π'*θ(a|s) / π_θ(a|s) （也被称为重要性采样比率）添加了一个剪切项
  ```
  L_CLIP(θ) = E[min(r_t(θ)A(s,a),clip(r_t(θ),1-ε,1+ε)A(s,a))]
  ```
  这使得如果策略分布变化过大 clip 会限制目标函数 L 的增长 从而梯度不会过大 也就限制了 θ 的更新
- PPO-Penalty 则是直接在目标函数中添加一个 KL 散度的惩罚项
  ```
  L_PENALTY(θ) = E[r_t(θ)A(s,a)] - β * D_KL(π_θ(a|s),π'_θ(a|s))
  ```
  其中 β 是一个动态变化的超参数 当 D_KL 过大时 β 会增大 反之减小 也就限制了 θ 的更新
- 两者的本质都是在限制目标函数 L 不会增长过大 从而限制 θ 的更新幅度
- 其中 PPO-Clip 算法在实际应用中效果更好 实现更简单

## L18 - Optimization

### Challenge

- Local Minima：局部最优是指在某个区域内的最小值或最大值 也就是梯度为 0 且 Hessian 矩阵大于 0 的点
- Saddle Point：鞍点是指在某个方向上是最小值 在另一个方向上是最大值的点 也就是梯度为 0 但是 Hessian 矩阵不是正定的点
- Vanishing Gradients：平坦区域是指梯度接近于 0 的区域 也就是梯度的范数非常小的区域

### Learning Mode

- 在线学习：在线学习是指模型在每次接收到新的数据时都进行更新 而不是等到所有数据都准备好后再进行更新 这种方式可以减少内存的使用 但是需要注意模型的稳定性
- 离线学习：离线学习是指模型在所有数据都准备好后再进行更新 这种方式可以保证模型的稳定性 但是需要更多的内存
- 批量梯度下降：批量梯度下降是指在每次更新时使用所有数据来计算梯度 这种方式可以保证模型的稳定性 同时可以根据内存调整批量大小

### Optimization Algorithm

- SGDM（Stochastic Gradient Descent with Momentum）：类似于物理中的动量 学习率会随着 epoch 不断积累 从而使得模型在梯度下降时具有惯性 可以加速收敛并减少振荡
- Adaptive Learning Rate：自适应学习率是指根据梯度的变化来调整学习率 使得模型在训练过程中可以更快地收敛
  - AdaGrad（Adaptive Gradient）：根据每个参数的历史梯度来调整学习率 使得每个参数的学习率都不同 适用于稀疏数据
  - RMSProp（Root Mean Square Propagation）：类似于 AdaGrad 但是使用指数衰减平均来计算每个参数的学习率 使得学习率不会过小 适用于非平稳数据
  - Adam（Adaptive Moment Estimation）：结合了 SGDM 和 RMSProp 的优点 使用动量和自适应学习率来更新参数 适用于大多数场景

## Review
- 