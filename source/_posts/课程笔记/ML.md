---
title: 《机器学习》课程笔记
date: 2025-02-19 14:36:41
categories: 课程笔记
tags:
  - 机器学习
  - 人工智能
  - 深度学习
index_img:
banner_img:
excerpt: ""
---

## L1 - 机器学习概述

### AI 发展简史

- 萌芽期
- 兴奋期
- 爆发期
- 数据驱动
- 深度学习

### AI vs ML vs DL

- AI>ML>DL 机器学习是需要从样例中学习数据规律的技术 而深度学习则是其中自动学习特征的一种方法

### WHAT IS ML？

- 从数据（数据集）中学习规律（模型） 不断优化性能（优化算法）以实现目标（损失函数）的算法

### 分类

- 监督学习：每个输入样本都有一个标签 比如分类问题、回归问题
- 无监督学习：没有标签的数据集 比如聚类问题、降维问题
- 强化学习：通过与环境交互来学习 环境包括奖励和惩罚等机制 目标是最大化长期奖励

### 泛化性

- 泛化性是指模型对未知数据的适应能力 比如一个分类模型是否能处理未见过的数据
- 过拟合：模型在训练集上表现很好 但在测试集上表现很差 通常是因为模型把训练数据的噪声因素也一并学习 相当于只会背题的学生 泛化性差
  - 措施
    1. 数据划分：训练集、验证集、测试集
    2. 交叉验证：K 折交叉验证 即每份数据集轮流作为验证集 解决数据量不足的问题
    3. 早停：出现过拟合迹象时及时停止训练（即训练损失下降 但是验证损失上升）
    4. 正则化：对参数增加惩罚项 限制模型的复杂度
- 欠拟合：模型在训练集和测试集上表现都很差 通常是因为模型太简单 无法拟合数据的复杂性 相当于智商不够的学生 泛化性差

## L2 - 线性回归

- 线性回归是一种用于预测连续值的监督学习算法

### 模型结构

- 基本形式：y = wx + b 其中 w 是权重 x 是输入 b 是偏置
- 训练过程：根据误差来调整 w 和 b
- 预测过程：根据新输入的 x 来预测 y
- 损失函数：均方误差（MSE）是最常用的损失函数 即预测值与真实值的差的平方的均值
- 优化方法：梯度下降是最常用的优化方法 即根据损失函数的梯度来调整参数
- 解析解：w = (X^T X)^-1 X^T y 其中 X 是输入矩阵 y 是输出向量 存在矩阵不可逆的情况 可以通过正则化来解决

## L3 - 分类问题

- 分类问题是一种用于预测离散值的监督学习算法 包括决策树、随机森林等

### 决策树

- 基本思想：数据的分布可能是非线性的 决策树通过递归地划分数据空间来进行分类
- 基本结构：根节点、内部节点、叶节点 其中叶节点代表类别 非叶结点代表特征判断条件
- 训练过程（递归构造决策树）
  1. 构造根节点
  2. 选择一个最合适的特征进行划分
  3. 根据特征将数据集划分为若干子集
  4. 创建子节点 递归地对子集进行划分
  5. 递归进行 2-4 直到满足停止条件（包括叶子节点都属于同一类、叶子节点样本数量太小、没有更多特征可用）
- 训练算法（如何选择最合适的特征）
  - ID3：基于信息增益 即选择能够最大程度降低信息熵的特征 每一轮遍历所有特征 计算信息增益 注意特征分界值通常选择中位数或是平均值
  - CART：基于基尼指数
  - C4.5：基于信息增益比
- 优点
  - 简单直观 易于理解 可视化性强
  - 预处理要求低 可以处理缺失值
  - 可以处理非线性边界
  - 数据驱动 可以任意精度拟合数据集
- 缺点
  - 容易过拟合 会把噪声数据划分为一个叶子节点
  - 为了解决过拟合问题 下面介绍随机森林

### 随机森林

- 基本思想：将数据随机划分为若干子集 分别训练若干个决策树 最后通过投票来决定最终分类结果 由于噪声数据最多只会影响一个决策树 因此不会影响最终的分类结果
- 优点
  - 比决策树更准确
  - 训练并行化 速度快
  - 可以处理高维数据
  - 可以处理缺失值
  - 结果可解释性强
  - 可以检测特征之间的相关性
  - 对于不平衡数据集效果好 可以平衡误差

## L4 - 贝叶斯分类

- 基本思想：将分类预测表示为概率分布的形式 通过贝叶斯定理来计算后验概率 选取概率最大的类别作为预测结果
- 计算后验概率时 需要提取很多特征的联合概率分布 为了简化计算 引入了**朴素贝叶斯假设** 即假设特征之间相互独立 这样联合分布就可以分解为各个特征的边缘分布的乘积 而边缘分布概率只需要通过统计计算即可
- 一旦某一中特征的概率为 0 那么整个后验概率就为 0 为了解决这个问题 引入了**拉普拉斯平滑** 即为每个事件的 cnt+1 保证非零
- 算法过程：（以垃圾邮件分类为例）
  1. 计算每个分类的先验概率 比如 P(垃圾邮件) = 0.1 P(非垃圾邮件) = 0.9
  2. 根据训练集统计出每个特征的条件概率 比如 P(“免费”|垃圾邮件) = 0.8 P(“免费”|非垃圾邮件) = 0.1
  3. 对于一个新样本 根据贝叶斯定理计算后验概率 比如 P(垃圾邮件|“免费” “优惠”) = 0.2 P(非垃圾邮件|“免费” “优惠”) = 0.3 取概率最大的类别作为预测结果 即非垃圾邮件
- 优点
  - 非常快速 文本分类效果好
- 缺点
  - 假设过于简单 可能导致欠拟合

## L5 - KNN

- 基本思想：通过计算新样本与训练集中所有样本的距离 来找到与新样本最近的 k 个样本 然后根据这 k 个样本的类别来预测新样本的类别
- 距离计算：常用的距离计算方法有欧氏距离 曼哈顿距离 闵可夫斯基距离 余弦相似度等
- 算法过程
  1. 预处理数据 包括归一化、标准化等
  2. 计算新样本与训练集中所有样本的距离
  3. 找到与新样本最近的 k 个样本
  4. 根据这 k 个样本的类别来预测新样本的类别
- K 的选择
  - 维诺图：将区域划分为不同的类别区域 使得每个区域的点到其种子点的距离最小
  - 当 K 较小时 容易受到噪声的影响 会导致过拟合
  - 当 K 较大时 容易受到样本不均衡的影响 会导致欠拟合
  - K 合适时 分界线比较平滑
- 优点
  - 简单直观 易于理解
  - 无需训练
- 缺点
  - 数据量太大时计算量大
  - 数据维度太高时效果不好

## L6 - 逻辑回归

- 基本思想：逻辑回归是一种用于预测分类问题的监督学习算法 使用判别函数来判定样本的类别 可以认为逻辑回归拟合的是类别的边界

### 感知机

- 判别函数：g(x) = w1x1 + w2x2 + w0 大于和小于 0 分别代表两个类别
- 训练过程：对于每一个数据 如果分类正确 则不做任何操作 如果分类错误 则 w+=学习率\*样本
- 缺点：判别函数输出的是两个离散点 没法进行优化 换句话说 我们无法知道一个样本属于某一类的概率

### 逻辑回归

- 逻辑函数：log(y/(1-y)) 其中 y 是样本属于某一类的概率
  - 经推导 若样本呈正态分布 逻辑函数可以被表示为 wT \* x + w0 即 x 的线性函数
  - 再经反推得 y=1/(1+e^(-wTx+w0)) 即 sigmoid 函数 其具有概率性质
  - sigmoid 的导数：y=sigmoid(x) 则 dy/dx = y(1-y)
- 判别函数：g(x) = 1/(1+e^(-wTx+w0)) 即上面推导的逻辑函数 大于和小于 0.5 分别代表两个类别
- 损失函数：交叉熵损失函数 即真实值和预测值的交叉熵
- 训练过程：梯度下降法 不断调整 w 和 w0 使得损失函数最小化即可
- 优点
  - 判别函数具有概率性质 可以代表样本属于某一类的概率

### 多分类问题

- 逻辑回归是二分类问题的解决方案 对于多分类问题 可以使用 softmax 函数 输出一个概率向量